{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "raw_data = pd.read_csv(\"./TrafficTwoMonth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data[['Time', 'Day of the week', 'CarCount', 'BikeCount', 'BusCount', 'TruckCount', 'Total', 'Traffic Situation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>CarCount</th>\n",
       "      <th>BikeCount</th>\n",
       "      <th>BusCount</th>\n",
       "      <th>TruckCount</th>\n",
       "      <th>Total</th>\n",
       "      <th>Traffic Situation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12:00:00 AM</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12:15:00 AM</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>52</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12:30:00 AM</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12:45:00 AM</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1:00:00 AM</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>48</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>10:45:00 PM</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>56</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>11:00:00 PM</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>11:15:00 PM</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>11:30:00 PM</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>11:45:00 PM</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5952 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time Day of the week  CarCount  BikeCount  BusCount  TruckCount  \\\n",
       "0     12:00:00 AM         Tuesday        13          2         2          24   \n",
       "1     12:15:00 AM         Tuesday        14          1         1          36   \n",
       "2     12:30:00 AM         Tuesday        10          2         2          32   \n",
       "3     12:45:00 AM         Tuesday        10          2         2          36   \n",
       "4      1:00:00 AM         Tuesday        11          2         1          34   \n",
       "...           ...             ...       ...        ...       ...         ...   \n",
       "5947  10:45:00 PM        Thursday        16          3         1          36   \n",
       "5948  11:00:00 PM        Thursday        11          0         1          30   \n",
       "5949  11:15:00 PM        Thursday        15          4         1          25   \n",
       "5950  11:30:00 PM        Thursday        16          5         0          27   \n",
       "5951  11:45:00 PM        Thursday        14          3         1          15   \n",
       "\n",
       "      Total Traffic Situation  \n",
       "0        41            normal  \n",
       "1        52            normal  \n",
       "2        46            normal  \n",
       "3        50            normal  \n",
       "4        48            normal  \n",
       "...     ...               ...  \n",
       "5947     56            normal  \n",
       "5948     42            normal  \n",
       "5949     45            normal  \n",
       "5950     48            normal  \n",
       "5951     33               low  \n",
       "\n",
       "[5952 rows x 8 columns]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time                 0\n",
       "Day of the week      0\n",
       "CarCount             0\n",
       "BikeCount            0\n",
       "BusCount             0\n",
       "TruckCount           0\n",
       "Total                0\n",
       "Traffic Situation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_31276\\3206695382.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Time'] = pd.to_datetime(df['Time']).dt.hour\n"
     ]
    }
   ],
   "source": [
    "df['Time'] = pd.to_datetime(df['Time']).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuesday      960\n",
       "Wednesday    960\n",
       "Thursday     960\n",
       "Friday       768\n",
       "Saturday     768\n",
       "Sunday       768\n",
       "Monday       768\n",
       "Name: Day of the week, dtype: int64"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Day of the week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_31276\\3000454048.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Day of the week'] = df['Day of the week'].replace({\n"
     ]
    }
   ],
   "source": [
    "df['Day of the week'] = df['Day of the week'].replace({\n",
    "  'Monday':1,\n",
    "  'Tuesday':2,\n",
    "  'Wednesday':3,\n",
    "  'Thursday':4,\n",
    "  'Friday':5,\n",
    "  'Saturday':6,\n",
    "  'Sunday':7\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal    3610\n",
       "heavy     1137\n",
       "low        834\n",
       "high       371\n",
       "Name: Traffic Situation, dtype: int64"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Traffic Situation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_31276\\3943949687.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Traffic Situation'] = df['Traffic Situation'].replace({\n"
     ]
    }
   ],
   "source": [
    "df['Traffic Situation'] = df['Traffic Situation'].replace({\n",
    "  'low':0,\n",
    "  'normal':0,\n",
    "  'high':1,\n",
    "  'heavy':1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Time', 'Day of the week', 'CarCount', 'BikeCount', 'BusCount', 'TruckCount', 'Total']]\n",
    "y = df[['Traffic Situation']].values.reshape(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>CarCount</th>\n",
       "      <th>BikeCount</th>\n",
       "      <th>BusCount</th>\n",
       "      <th>TruckCount</th>\n",
       "      <th>Total</th>\n",
       "      <th>Traffic Situation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5952 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time  Day of the week  CarCount  BikeCount  BusCount  TruckCount  Total  \\\n",
       "0        0                2        13          2         2          24     41   \n",
       "1        0                2        14          1         1          36     52   \n",
       "2        0                2        10          2         2          32     46   \n",
       "3        0                2        10          2         2          36     50   \n",
       "4        1                2        11          2         1          34     48   \n",
       "...    ...              ...       ...        ...       ...         ...    ...   \n",
       "5947    22                4        16          3         1          36     56   \n",
       "5948    23                4        11          0         1          30     42   \n",
       "5949    23                4        15          4         1          25     45   \n",
       "5950    23                4        16          5         0          27     48   \n",
       "5951    23                4        14          3         1          15     33   \n",
       "\n",
       "      Traffic Situation  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "5947                  0  \n",
       "5948                  0  \n",
       "5949                  0  \n",
       "5950                  0  \n",
       "5951                  0  \n",
       "\n",
       "[5952 rows x 8 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/100\n",
    "x_test = x_test/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement algorithm Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  def __init__(self, iteration=1000, learning_rete=0.01, c=0, penalty='l1'):\n",
    "    self.iteration = iteration\n",
    "    self.learning_rete = learning_rete\n",
    "    self.c = c\n",
    "    self.penalty=penalty.lower()\n",
    "    self.w = None\n",
    "    self.b = None\n",
    "    self.loss = []\n",
    "    self.accuracy = []\n",
    "\n",
    "  def regularization(self):\n",
    "    # L1 regularization \n",
    "    if self.penalty == 'l1':\n",
    "      return self.c*np.sign(self.w)\n",
    "    # L2 regularization \n",
    "    if self.penalty == 'l2':\n",
    "      return self.c*self.w\n",
    "    return np.zeros(self.x.shape[1])\n",
    "  \n",
    "  def sigmoid(self, z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "  def fit(self, x, y, batch_size=1, random_seed = 4):\n",
    "    np.random.seed(random_seed)\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    num_samples, num_features = x.shape\n",
    "    self.loss = []\n",
    "    self.accuracy = []\n",
    "\n",
    "    if (self.w is None and self.b is None):\n",
    "      self.w = np.random.rand(num_features)\n",
    "      self.b = 0\n",
    "\n",
    "    # Gredient descent\n",
    "    for i in range(self.iteration):\n",
    "      \n",
    "      # Calculate loss\n",
    "      loss = self.cross_entropy(x, y)\n",
    "      self.loss.append(loss)\n",
    "      accuracy = self.score(x, y)\n",
    "      self.accuracy.append(accuracy)\n",
    "      print('iterations {} accuracy : {}  loss : {}'.format(i, accuracy, loss))\n",
    "\n",
    "      # Random training data\n",
    "      idx = np.random.choice(num_samples, int(batch_size*num_samples))\n",
    "      x_batch = x.iloc[idx]\n",
    "      y_batch = y[idx]\n",
    "\n",
    "      # Predict\n",
    "      z = np.dot(self.w, x_batch.T) + self.b\n",
    "      y_pred = self.sigmoid(z)\n",
    "\n",
    "      # Calculate gradient\n",
    "      gred_w = np.dot(x_batch.T, (y_pred - y_batch))/num_samples\n",
    "      gred_b = np.sum(y_pred - y_batch)/num_samples\n",
    "\n",
    "      # Regularization\n",
    "      gred_w = gred_w + self.regularization()\n",
    " \n",
    "      # Update parameter\n",
    "      self.w = self.w - (self.learning_rete*gred_w)\n",
    "      self.b = self.b - (self.learning_rete*gred_b)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, x):\n",
    "    z = np.dot(self.w, x.T) + self.b\n",
    "    probYgivenX = self.sigmoid(z)\n",
    "    return np.array([1 if p >= 0.5 else 0 for p in probYgivenX])\n",
    "\n",
    "  def score(self, x, y):\n",
    "    y_pred = self.predict(x)\n",
    "    return np.mean(y == y_pred)\n",
    "  \n",
    "  def regularization_cost(self):\n",
    "    # L1 regularization \n",
    "    if self.penalty == 'l1':\n",
    "      return self.c*np.sum(np.abs(self.w))\n",
    "    # L2 regularization \n",
    "    if self.penalty == 'l2':\n",
    "      return self.c*np.sum(np.square(self.w))/2\n",
    "    return 0\n",
    "  \n",
    "  def cross_entropy(self, x, y):\n",
    "    eps = 1e-15  # Small constant value to prevent division by zero\n",
    "    z = np.dot(self.w, x.T) + self.b\n",
    "    y_pred = self.sigmoid(z)\n",
    "    return -(np.dot(y.T,np.log(y_pred + eps)) + np.dot((1-y).T, np.log(1-y_pred + eps)))/x.shape[0] + self.regularization_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations 0 accuracy : 0.253098088636841  loss : 1.38141580449296\n",
      "iterations 1 accuracy : 0.253098088636841  loss : 1.375166851169794\n",
      "iterations 2 accuracy : 0.253098088636841  loss : 1.3690448209559565\n",
      "iterations 3 accuracy : 0.253098088636841  loss : 1.3630469378900512\n",
      "iterations 4 accuracy : 0.253098088636841  loss : 1.3569592902579473\n",
      "iterations 5 accuracy : 0.253098088636841  loss : 1.3508531418540544\n",
      "iterations 6 accuracy : 0.253098088636841  loss : 1.3448286662899884\n",
      "iterations 7 accuracy : 0.253098088636841  loss : 1.338853778083571\n",
      "iterations 8 accuracy : 0.253098088636841  loss : 1.3328910253139552\n",
      "iterations 9 accuracy : 0.253098088636841  loss : 1.327066190375843\n",
      "iterations 10 accuracy : 0.253098088636841  loss : 1.3212581624960889\n",
      "iterations 11 accuracy : 0.253098088636841  loss : 1.3154706752177463\n",
      "iterations 12 accuracy : 0.253098088636841  loss : 1.30959286499093\n",
      "iterations 13 accuracy : 0.253098088636841  loss : 1.3038710520662462\n",
      "iterations 14 accuracy : 0.253098088636841  loss : 1.298016636346818\n",
      "iterations 15 accuracy : 0.253098088636841  loss : 1.2922912211504258\n",
      "iterations 16 accuracy : 0.253098088636841  loss : 1.286568565222796\n",
      "iterations 17 accuracy : 0.253098088636841  loss : 1.2807879964781188\n",
      "iterations 18 accuracy : 0.253098088636841  loss : 1.2752767618328906\n",
      "iterations 19 accuracy : 0.253098088636841  loss : 1.269691547881435\n",
      "iterations 20 accuracy : 0.253098088636841  loss : 1.2639781398426129\n",
      "iterations 21 accuracy : 0.253098088636841  loss : 1.2583367828308565\n",
      "iterations 22 accuracy : 0.253098088636841  loss : 1.25280230165595\n",
      "iterations 23 accuracy : 0.253098088636841  loss : 1.247385768436875\n",
      "iterations 24 accuracy : 0.253098088636841  loss : 1.2418072809333611\n",
      "iterations 25 accuracy : 0.253098088636841  loss : 1.2362545729199597\n",
      "iterations 26 accuracy : 0.253098088636841  loss : 1.2309107234977141\n",
      "iterations 27 accuracy : 0.253098088636841  loss : 1.2254798980514547\n",
      "iterations 28 accuracy : 0.253098088636841  loss : 1.2201342684104357\n",
      "iterations 29 accuracy : 0.253098088636841  loss : 1.2148292839164394\n",
      "iterations 30 accuracy : 0.253098088636841  loss : 1.209426575636412\n",
      "iterations 31 accuracy : 0.253098088636841  loss : 1.2040115701767478\n",
      "iterations 32 accuracy : 0.253098088636841  loss : 1.1986541087824114\n",
      "iterations 33 accuracy : 0.253098088636841  loss : 1.193382942128685\n",
      "iterations 34 accuracy : 0.253098088636841  loss : 1.187979686727813\n",
      "iterations 35 accuracy : 0.253098088636841  loss : 1.1828357742619753\n",
      "iterations 36 accuracy : 0.253098088636841  loss : 1.177602528649193\n",
      "iterations 37 accuracy : 0.253098088636841  loss : 1.172432165580853\n",
      "iterations 38 accuracy : 0.253098088636841  loss : 1.1672970224331125\n",
      "iterations 39 accuracy : 0.253098088636841  loss : 1.162156722930117\n",
      "iterations 40 accuracy : 0.253098088636841  loss : 1.1570759505051806\n",
      "iterations 41 accuracy : 0.253098088636841  loss : 1.15194529114578\n",
      "iterations 42 accuracy : 0.253098088636841  loss : 1.1469187879014922\n",
      "iterations 43 accuracy : 0.253098088636841  loss : 1.1418567446797867\n",
      "iterations 44 accuracy : 0.253098088636841  loss : 1.1367545442238196\n",
      "iterations 45 accuracy : 0.253098088636841  loss : 1.1318714243446928\n",
      "iterations 46 accuracy : 0.253098088636841  loss : 1.1268783063062253\n",
      "iterations 47 accuracy : 0.253098088636841  loss : 1.1218672521119344\n",
      "iterations 48 accuracy : 0.253098088636841  loss : 1.1169607466536897\n",
      "iterations 49 accuracy : 0.253098088636841  loss : 1.1119981625771833\n",
      "iterations 50 accuracy : 0.253098088636841  loss : 1.107080459524401\n",
      "iterations 51 accuracy : 0.253098088636841  loss : 1.1022389936849637\n",
      "iterations 52 accuracy : 0.253098088636841  loss : 1.0973537837083944\n",
      "iterations 53 accuracy : 0.253098088636841  loss : 1.0925689976081865\n",
      "iterations 54 accuracy : 0.253098088636841  loss : 1.0878192985114754\n",
      "iterations 55 accuracy : 0.253098088636841  loss : 1.0831032627316226\n",
      "iterations 56 accuracy : 0.253098088636841  loss : 1.078359262249979\n",
      "iterations 57 accuracy : 0.253098088636841  loss : 1.0736037627623356\n",
      "iterations 58 accuracy : 0.253098088636841  loss : 1.069051904825367\n",
      "iterations 59 accuracy : 0.253098088636841  loss : 1.064402878443636\n",
      "iterations 60 accuracy : 0.253098088636841  loss : 1.059741506822515\n",
      "iterations 61 accuracy : 0.253098088636841  loss : 1.055071303841953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations 62 accuracy : 0.253098088636841  loss : 1.0507069076793023\n",
      "iterations 63 accuracy : 0.253098088636841  loss : 1.0462181252150848\n",
      "iterations 64 accuracy : 0.253098088636841  loss : 1.0416286633661498\n",
      "iterations 65 accuracy : 0.253098088636841  loss : 1.0371413899165147\n",
      "iterations 66 accuracy : 0.2533081285444234  loss : 1.0325664417899585\n",
      "iterations 67 accuracy : 0.25351816845200587  loss : 1.0280909922897772\n",
      "iterations 68 accuracy : 0.25351816845200587  loss : 1.0236752178495307\n",
      "iterations 69 accuracy : 0.25351816845200587  loss : 1.0193224498327864\n",
      "iterations 70 accuracy : 0.25351816845200587  loss : 1.0149545655197945\n",
      "iterations 71 accuracy : 0.2541482881747532  loss : 1.0106732573148614\n",
      "iterations 72 accuracy : 0.2547784078975005  loss : 1.0063877906567003\n",
      "iterations 73 accuracy : 0.2556185675278303  loss : 1.0021229082659555\n",
      "iterations 74 accuracy : 0.25645872715816004  loss : 0.9978094004560144\n",
      "iterations 75 accuracy : 0.2579290065112371  loss : 0.9934637863229004\n",
      "iterations 76 accuracy : 0.25855912623398447  loss : 0.9891777491580095\n",
      "iterations 77 accuracy : 0.25960932577189666  loss : 0.9849997903995529\n",
      "iterations 78 accuracy : 0.2612896450325562  loss : 0.9808684153525433\n",
      "iterations 79 accuracy : 0.263600084015963  loss : 0.976647822257066\n",
      "iterations 80 accuracy : 0.26528040327662256  loss : 0.9725284005364624\n",
      "iterations 81 accuracy : 0.267380802352447  loss : 0.9684533786905971\n",
      "iterations 82 accuracy : 0.2688510817055241  loss : 0.964290590884717\n",
      "iterations 83 accuracy : 0.27053140096618356  loss : 0.9602733756258717\n",
      "iterations 84 accuracy : 0.2730518798571729  loss : 0.9562564266441087\n",
      "iterations 85 accuracy : 0.2753623188405797  loss : 0.9522038500453542\n",
      "iterations 86 accuracy : 0.277882797731569  loss : 0.9483090805737155\n",
      "iterations 87 accuracy : 0.2806133165301407  loss : 0.9444184087216632\n",
      "iterations 88 accuracy : 0.28334383532871243  loss : 0.9404403326009364\n",
      "iterations 89 accuracy : 0.2862843940348666  loss : 0.9365958962720764\n",
      "iterations 90 accuracy : 0.2898550724637681  loss : 0.932748516480577\n",
      "iterations 91 accuracy : 0.2923755513547574  loss : 0.9288644480302383\n",
      "iterations 92 accuracy : 0.2953161100609116  loss : 0.925068645522119\n",
      "iterations 93 accuracy : 0.29867674858223064  loss : 0.9213316823386564\n",
      "iterations 94 accuracy : 0.3011972274732199  loss : 0.9176070499310189\n",
      "iterations 95 accuracy : 0.3037177063642092  loss : 0.9139193911612905\n",
      "iterations 96 accuracy : 0.30854862423860535  loss : 0.9101652867076963\n",
      "iterations 97 accuracy : 0.3116992228523419  loss : 0.9065077294164477\n",
      "iterations 98 accuracy : 0.31568998109640833  loss : 0.9029432116988482\n",
      "iterations 99 accuracy : 0.31989077924805714  loss : 0.8993557985443298\n",
      "iterations 100 accuracy : 0.32472169712245325  loss : 0.895782784385458\n",
      "iterations 101 accuracy : 0.32976265490443185  loss : 0.8922095085429635\n",
      "iterations 102 accuracy : 0.33396345305608066  loss : 0.8886828277589722\n",
      "iterations 103 accuracy : 0.34173492963663094  loss : 0.8851905497268698\n",
      "iterations 104 accuracy : 0.34782608695652173  loss : 0.8816829649764183\n",
      "iterations 105 accuracy : 0.35307708464608273  loss : 0.8782637371497302\n",
      "iterations 106 accuracy : 0.3600084015963033  loss : 0.874733878634677\n",
      "iterations 107 accuracy : 0.36504935937828187  loss : 0.8712927222569091\n",
      "iterations 108 accuracy : 0.3694601974375131  loss : 0.8679320772975547\n",
      "iterations 109 accuracy : 0.37303087586641465  loss : 0.8645334678116886\n",
      "iterations 110 accuracy : 0.3784919134635581  loss : 0.8611817796946112\n",
      "iterations 111 accuracy : 0.38311279143037175  loss : 0.8578762582628227\n",
      "iterations 112 accuracy : 0.38731358958202056  loss : 0.8546147063716348\n",
      "iterations 113 accuracy : 0.39025414828817473  loss : 0.8513483176722555\n",
      "iterations 114 accuracy : 0.39445494643982354  loss : 0.848170155916298\n",
      "iterations 115 accuracy : 0.39844570468388996  loss : 0.8450625048206574\n",
      "iterations 116 accuracy : 0.40138626339004413  loss : 0.8418444981054592\n",
      "iterations 117 accuracy : 0.4039067422810334  loss : 0.8387702987735768\n",
      "iterations 118 accuracy : 0.40789750052509977  loss : 0.8356864399620187\n",
      "iterations 119 accuracy : 0.412308338584331  loss : 0.8325930951427514\n",
      "iterations 120 accuracy : 0.4158790170132325  loss : 0.829541958898838\n",
      "iterations 121 accuracy : 0.4204998949800462  loss : 0.8264864968055412\n",
      "iterations 122 accuracy : 0.42302037387103547  loss : 0.8235674590829107\n",
      "iterations 123 accuracy : 0.4251207729468599  loss : 0.8204677300136138\n",
      "iterations 124 accuracy : 0.4289014912833438  loss : 0.817549634020443\n",
      "iterations 125 accuracy : 0.4326822096198278  loss : 0.8146244834030153\n",
      "iterations 126 accuracy : 0.4364629279563117  loss : 0.8116860386723947\n",
      "iterations 127 accuracy : 0.4391934467548834  loss : 0.8088132268253692\n",
      "iterations 128 accuracy : 0.44129384583070785  loss : 0.8059267210396232\n",
      "iterations 129 accuracy : 0.44654484352026885  loss : 0.8030114701852211\n",
      "iterations 130 accuracy : 0.449485402226423  loss : 0.8001315523520321\n",
      "iterations 131 accuracy : 0.4524259609325772  loss : 0.7972451521268611\n",
      "iterations 132 accuracy : 0.4562066792690611  loss : 0.7944312815009167\n",
      "iterations 133 accuracy : 0.4591472379752153  loss : 0.7916125999481747\n",
      "iterations 134 accuracy : 0.4608275572358748  loss : 0.7888463380707649\n",
      "iterations 135 accuracy : 0.4633480361268641  loss : 0.7860986087689738\n",
      "iterations 136 accuracy : 0.4660785549254358  loss : 0.7834115378184843\n",
      "iterations 137 accuracy : 0.4706994328922495  loss : 0.7807018572122713\n",
      "iterations 138 accuracy : 0.47279983196807396  loss : 0.7780835120342565\n",
      "iterations 139 accuracy : 0.47574039067422813  loss : 0.775348844552862\n",
      "iterations 140 accuracy : 0.47868094938038225  loss : 0.7727200937976945\n",
      "iterations 141 accuracy : 0.48267170762444866  loss : 0.7701928748157427\n",
      "iterations 142 accuracy : 0.48582230623818523  loss : 0.7676205875316998\n",
      "iterations 143 accuracy : 0.4887628649443394  loss : 0.765062250949879\n",
      "iterations 144 accuracy : 0.49128334383532873  loss : 0.7625365497338502\n",
      "iterations 145 accuracy : 0.49317370300357066  loss : 0.7599903653178102\n",
      "iterations 146 accuracy : 0.4952741020793951  loss : 0.7575239805829262\n",
      "iterations 147 accuracy : 0.4980046208779668  loss : 0.7550137560442336\n",
      "iterations 148 accuracy : 0.5007351396765385  loss : 0.752621160241238\n",
      "iterations 149 accuracy : 0.5036756983826927  loss : 0.7501669135163143\n",
      "iterations 150 accuracy : 0.506196177273682  loss : 0.7478001120757389\n",
      "iterations 151 accuracy : 0.5099768956101659  loss : 0.7452535653176537\n",
      "iterations 152 accuracy : 0.5118672547784079  loss : 0.7428706913902319\n",
      "iterations 153 accuracy : 0.5141776937618148  loss : 0.7403914664068894\n",
      "iterations 154 accuracy : 0.5160680529300568  loss : 0.7380881039935812\n",
      "iterations 155 accuracy : 0.5175383322831338  loss : 0.7358448172714366\n",
      "iterations 156 accuracy : 0.5192186515437933  loss : 0.7335804171953076\n",
      "iterations 157 accuracy : 0.5206889308968704  loss : 0.7313402033891175\n",
      "iterations 158 accuracy : 0.5232094097878597  loss : 0.7290980582349972\n",
      "iterations 159 accuracy : 0.5261499684940139  loss : 0.7268111064125853\n",
      "iterations 160 accuracy : 0.5265700483091788  loss : 0.724547863089523\n",
      "iterations 161 accuracy : 0.5282503675698382  loss : 0.7222690704845867\n",
      "iterations 162 accuracy : 0.5295106070153329  loss : 0.7201242615552\n",
      "iterations 163 accuracy : 0.5305608065532451  loss : 0.7179093846446569\n",
      "iterations 164 accuracy : 0.5320310859063222  loss : 0.7157397322918879\n",
      "iterations 165 accuracy : 0.5341314849821466  loss : 0.7136814346961737\n",
      "iterations 166 accuracy : 0.5349716446124764  loss : 0.7116059171523681\n",
      "iterations 167 accuracy : 0.5351816845200588  loss : 0.7095474623894005\n",
      "iterations 168 accuracy : 0.5362318840579711  loss : 0.7074461621987961\n",
      "iterations 169 accuracy : 0.5372820835958833  loss : 0.705369336070707\n",
      "iterations 170 accuracy : 0.5391724427641252  loss : 0.7034256007097165\n",
      "iterations 171 accuracy : 0.5408527620247847  loss : 0.7013767808860383\n",
      "iterations 172 accuracy : 0.5427431211930267  loss : 0.6994544035243365\n",
      "iterations 173 accuracy : 0.5454736399915984  loss : 0.697485620524163\n",
      "iterations 174 accuracy : 0.5454736399915984  loss : 0.6956010005965589\n",
      "iterations 175 accuracy : 0.547153959252258  loss : 0.6937222177202368\n",
      "iterations 176 accuracy : 0.5482041587901701  loss : 0.6918227378452163\n",
      "iterations 177 accuracy : 0.5494643982356647  loss : 0.6899195600938812\n",
      "iterations 178 accuracy : 0.5513547574039067  loss : 0.68799393761294\n",
      "iterations 179 accuracy : 0.5547153959252258  loss : 0.6861798349240298\n",
      "iterations 180 accuracy : 0.5559756353707205  loss : 0.6843627523946704\n",
      "iterations 181 accuracy : 0.5563957151858853  loss : 0.6825823478958252\n",
      "iterations 182 accuracy : 0.5578659945389624  loss : 0.6806754564037159\n",
      "iterations 183 accuracy : 0.5593362738920394  loss : 0.6789350931182491\n",
      "iterations 184 accuracy : 0.5622768325981936  loss : 0.6770846914632044\n",
      "iterations 185 accuracy : 0.5637471119512707  loss : 0.6753301086765939\n",
      "iterations 186 accuracy : 0.5650073513967654  loss : 0.673580236202151\n",
      "iterations 187 accuracy : 0.5656374711195127  loss : 0.6718643673658139\n",
      "iterations 188 accuracy : 0.5675278302877547  loss : 0.6701492043372074\n",
      "iterations 189 accuracy : 0.569628229363579  loss : 0.6685043178555689\n",
      "iterations 190 accuracy : 0.5713085486242386  loss : 0.6668086572681828\n",
      "iterations 191 accuracy : 0.5725687880697332  loss : 0.6651706247736104\n",
      "iterations 192 accuracy : 0.5748792270531401  loss : 0.6634654891800859\n",
      "iterations 193 accuracy : 0.5765595463137996  loss : 0.66182515118086\n",
      "iterations 194 accuracy : 0.5792900651123714  loss : 0.6601934865633616\n",
      "iterations 195 accuracy : 0.5816005040957782  loss : 0.6586087740315006\n",
      "iterations 196 accuracy : 0.5837009031716026  loss : 0.6570590193346355\n",
      "iterations 197 accuracy : 0.5866414618777568  loss : 0.6554963755125197\n",
      "iterations 198 accuracy : 0.589582020583911  loss : 0.6539201336641893\n",
      "iterations 199 accuracy : 0.5914723797521529  loss : 0.6523405639285427\n",
      "iterations 200 accuracy : 0.5923125393824826  loss : 0.6508198501908276\n",
      "iterations 201 accuracy : 0.5942028985507246  loss : 0.6492744094001731\n",
      "iterations 202 accuracy : 0.596303297626549  loss : 0.6478557055306229\n",
      "iterations 203 accuracy : 0.5998739760554506  loss : 0.6463892287326696\n",
      "iterations 204 accuracy : 0.6019743751312749  loss : 0.6449373092608074\n",
      "iterations 205 accuracy : 0.6044948540222642  loss : 0.6434832446249936\n",
      "iterations 206 accuracy : 0.6076454526360009  loss : 0.6420169963206128\n",
      "iterations 207 accuracy : 0.6112161310649024  loss : 0.6405899220393988\n",
      "iterations 208 accuracy : 0.6143667296786389  loss : 0.6391036388479268\n",
      "iterations 209 accuracy : 0.6175173282923756  loss : 0.637754640730615\n",
      "iterations 210 accuracy : 0.6208779668136946  loss : 0.6363622215704997\n",
      "iterations 211 accuracy : 0.6225582860743542  loss : 0.6349465271758514\n",
      "iterations 212 accuracy : 0.6259189245956732  loss : 0.6336089578570304\n",
      "iterations 213 accuracy : 0.6294896030245747  loss : 0.6322691574216359\n",
      "iterations 214 accuracy : 0.6315900021003991  loss : 0.6309419741276158\n",
      "iterations 215 accuracy : 0.633480361268641  loss : 0.6296610534902859\n",
      "iterations 216 accuracy : 0.635370720436883  loss : 0.6283717530825279\n",
      "iterations 217 accuracy : 0.6374711195127074  loss : 0.6270616290369726\n",
      "iterations 218 accuracy : 0.6404116782188616  loss : 0.6258431090927572\n",
      "iterations 219 accuracy : 0.6433522369250158  loss : 0.6246060073566233\n",
      "iterations 220 accuracy : 0.6462927956311699  loss : 0.6233501216724171\n",
      "iterations 221 accuracy : 0.6488132745221592  loss : 0.622095334631804\n",
      "iterations 222 accuracy : 0.6517538332283134  loss : 0.6208767744363523\n",
      "iterations 223 accuracy : 0.6549044318420499  loss : 0.6196988078364237\n",
      "iterations 224 accuracy : 0.6584751102709515  loss : 0.6185017467795729\n",
      "iterations 225 accuracy : 0.6612056290695232  loss : 0.6173302044649517\n",
      "iterations 226 accuracy : 0.6645662675908423  loss : 0.6162144611824845\n",
      "iterations 227 accuracy : 0.6675068262969964  loss : 0.6150231482179145\n",
      "iterations 228 accuracy : 0.6700273051879857  loss : 0.6138503331382026\n",
      "iterations 229 accuracy : 0.6731779038017224  loss : 0.612754784987528\n",
      "iterations 230 accuracy : 0.6748582230623819  loss : 0.6116106550567921\n",
      "iterations 231 accuracy : 0.6769586221382062  loss : 0.6104539908630887\n",
      "iterations 232 accuracy : 0.6822096198277673  loss : 0.6093600505416645\n",
      "iterations 233 accuracy : 0.6851501785339215  loss : 0.6082615842331264\n",
      "iterations 234 accuracy : 0.6887208569628229  loss : 0.607160227219846\n",
      "iterations 235 accuracy : 0.6914513757613947  loss : 0.6061278085118061\n",
      "iterations 236 accuracy : 0.6954421340054611  loss : 0.6050779436645026\n",
      "iterations 237 accuracy : 0.6969124133585382  loss : 0.6040285369206354\n",
      "iterations 238 accuracy : 0.6998529720646923  loss : 0.6030073053683501\n",
      "iterations 239 accuracy : 0.7038437303087587  loss : 0.6019744464419243\n",
      "iterations 240 accuracy : 0.7059441293845831  loss : 0.6009467283916325\n",
      "iterations 241 accuracy : 0.707834488552825  loss : 0.599944654764012\n",
      "iterations 242 accuracy : 0.7093047679059021  loss : 0.5989455062429516\n",
      "iterations 243 accuracy : 0.7114051669817265  loss : 0.5979931886066091\n",
      "iterations 244 accuracy : 0.7135055660575509  loss : 0.596997329138157\n",
      "iterations 245 accuracy : 0.7143457256878807  loss : 0.5960517590129222\n",
      "iterations 246 accuracy : 0.71686620457887  loss : 0.5950757466454067\n",
      "iterations 247 accuracy : 0.7189666036546944  loss : 0.5941127131030409\n",
      "iterations 248 accuracy : 0.7214870825456837  loss : 0.5931761255292\n",
      "iterations 249 accuracy : 0.7237975215290905  loss : 0.5922469040289758\n",
      "iterations 250 accuracy : 0.7267380802352447  loss : 0.5913613215773743\n",
      "iterations 251 accuracy : 0.7294685990338164  loss : 0.5904518777261774\n",
      "iterations 252 accuracy : 0.7328292375551355  loss : 0.5895571607999437\n",
      "iterations 253 accuracy : 0.7349296366309599  loss : 0.5886760339543076\n",
      "iterations 254 accuracy : 0.738290275152279  loss : 0.5878075828236092\n",
      "iterations 255 accuracy : 0.7412308338584331  loss : 0.5869123925006192\n",
      "iterations 256 accuracy : 0.7422810333963453  loss : 0.5860374379471815\n",
      "iterations 257 accuracy : 0.7441713925645873  loss : 0.5851384652452454\n",
      "iterations 258 accuracy : 0.7454316320100819  loss : 0.5842649792449632\n",
      "iterations 259 accuracy : 0.7490023104389834  loss : 0.5833930887449684\n",
      "iterations 260 accuracy : 0.7511027095148078  loss : 0.5825777350868575\n",
      "iterations 261 accuracy : 0.7546733879437093  loss : 0.5817826716496284\n",
      "iterations 262 accuracy : 0.7567737870195337  loss : 0.5809269066418487\n",
      "iterations 263 accuracy : 0.7595043058181055  loss : 0.5801006002569286\n",
      "iterations 264 accuracy : 0.762654904431842  loss : 0.5792599057188039\n",
      "iterations 265 accuracy : 0.7658055030455787  loss : 0.5784485300290315\n",
      "iterations 266 accuracy : 0.7689561016593153  loss : 0.5776568406048798\n",
      "iterations 267 accuracy : 0.77021634110481  loss : 0.5768700572485325\n",
      "iterations 268 accuracy : 0.7710565007351396  loss : 0.5761169938317653\n",
      "iterations 269 accuracy : 0.7746271791640411  loss : 0.5753627061851565\n",
      "iterations 270 accuracy : 0.7763074984247007  loss : 0.5745968492509653\n",
      "iterations 271 accuracy : 0.7777777777777778  loss : 0.5738916319494443\n",
      "iterations 272 accuracy : 0.77882797731569  loss : 0.5731864702590524\n",
      "iterations 273 accuracy : 0.7815584961142618  loss : 0.5724389837937368\n",
      "iterations 274 accuracy : 0.7823986557445914  loss : 0.5717287361995446\n",
      "iterations 275 accuracy : 0.7847090947279983  loss : 0.5710335981152445\n",
      "iterations 276 accuracy : 0.7863894139886578  loss : 0.5703134107482812\n",
      "iterations 277 accuracy : 0.7880697332493174  loss : 0.5696329740484346\n",
      "iterations 278 accuracy : 0.7884898130644823  loss : 0.5688923378459397\n",
      "iterations 279 accuracy : 0.791220331863054  loss : 0.5681819227951068\n",
      "iterations 280 accuracy : 0.793110691031296  loss : 0.5675035366583765\n",
      "iterations 281 accuracy : 0.7943709304767906  loss : 0.5668439189937456\n",
      "iterations 282 accuracy : 0.7958412098298677  loss : 0.5661799571402008\n",
      "iterations 283 accuracy : 0.7975215290905272  loss : 0.5654982821774972\n",
      "iterations 284 accuracy : 0.7981516488132745  loss : 0.5648574128293694\n",
      "iterations 285 accuracy : 0.8000420079815165  loss : 0.5641986521618537\n",
      "iterations 286 accuracy : 0.8004620877966814  loss : 0.5635501079536704\n",
      "iterations 287 accuracy : 0.8036126864104179  loss : 0.5628967817227051\n",
      "iterations 288 accuracy : 0.8044528460407477  loss : 0.5622679978677899\n",
      "iterations 289 accuracy : 0.806973324931737  loss : 0.5616080874981056\n",
      "iterations 290 accuracy : 0.8084436042848141  loss : 0.5609900308468387\n",
      "iterations 291 accuracy : 0.8109640831758034  loss : 0.5603681513954003\n",
      "iterations 292 accuracy : 0.8128544423440454  loss : 0.5597534020310785\n",
      "iterations 293 accuracy : 0.8143247216971224  loss : 0.5591423796561501\n",
      "iterations 294 accuracy : 0.814534761604705  loss : 0.5585177713373113\n",
      "iterations 295 accuracy : 0.8160050409577819  loss : 0.5579454534588885\n",
      "iterations 296 accuracy : 0.8181054400336064  loss : 0.5573618204639718\n",
      "iterations 297 accuracy : 0.8191556395715186  loss : 0.5567831306311073\n",
      "iterations 298 accuracy : 0.8204158790170132  loss : 0.5562187133253703\n",
      "iterations 299 accuracy : 0.8220961982776728  loss : 0.555651248133196\n",
      "iterations 300 accuracy : 0.8237765175383323  loss : 0.5551362304010805\n",
      "iterations 301 accuracy : 0.8254568367989918  loss : 0.554561616797462\n",
      "iterations 302 accuracy : 0.8271371560596513  loss : 0.5540167578989057\n",
      "iterations 303 accuracy : 0.8281873555975635  loss : 0.5534923241643604\n",
      "iterations 304 accuracy : 0.828397395505146  loss : 0.5529406088249311\n",
      "iterations 305 accuracy : 0.8300777147658055  loss : 0.552377595224428\n",
      "iterations 306 accuracy : 0.8315479941188826  loss : 0.5518269003702564\n",
      "iterations 307 accuracy : 0.8325981936567948  loss : 0.5512896944880467\n",
      "iterations 308 accuracy : 0.8338584331022895  loss : 0.5507862951384149\n",
      "iterations 309 accuracy : 0.835118672547784  loss : 0.5502671713607923\n",
      "iterations 310 accuracy : 0.8359588321781138  loss : 0.5497415981792353\n",
      "iterations 311 accuracy : 0.8374291115311909  loss : 0.5491950384053924\n",
      "iterations 312 accuracy : 0.8384793110691031  loss : 0.5486912561730959\n",
      "iterations 313 accuracy : 0.8401596303297627  loss : 0.5481990157930637\n",
      "iterations 314 accuracy : 0.8409997899600924  loss : 0.5477175053197807\n",
      "iterations 315 accuracy : 0.8414198697752573  loss : 0.5472306476012203\n",
      "iterations 316 accuracy : 0.8414198697752573  loss : 0.5467079373150923\n",
      "iterations 317 accuracy : 0.8424700693131695  loss : 0.5462469755022895\n",
      "iterations 318 accuracy : 0.8437303087586642  loss : 0.5457929338232804\n",
      "iterations 319 accuracy : 0.8468809073724007  loss : 0.5452980325085656\n",
      "iterations 320 accuracy : 0.847511027095148  loss : 0.5448478877545974\n",
      "iterations 321 accuracy : 0.8485612266330603  loss : 0.5443786919177517\n",
      "iterations 322 accuracy : 0.851291745431632  loss : 0.5439095592699615\n",
      "iterations 323 accuracy : 0.8523419449695442  loss : 0.5434628294323904\n",
      "iterations 324 accuracy : 0.8538122243226213  loss : 0.5430170181052453\n",
      "iterations 325 accuracy : 0.8559126233984458  loss : 0.542580090765798\n",
      "iterations 326 accuracy : 0.8578029825666876  loss : 0.5421405074310889\n",
      "iterations 327 accuracy : 0.8588531821045998  loss : 0.5417095428248502\n",
      "iterations 328 accuracy : 0.8592732619197647  loss : 0.5412534426898903\n",
      "iterations 329 accuracy : 0.8605335013652594  loss : 0.5407864884693855\n",
      "iterations 330 accuracy : 0.8607435412728418  loss : 0.5403638449494856\n",
      "iterations 331 accuracy : 0.8615837009031716  loss : 0.539939437191025\n",
      "iterations 332 accuracy : 0.8624238605335014  loss : 0.5395043010871116\n",
      "iterations 333 accuracy : 0.8638941398865785  loss : 0.5390825691969053\n",
      "iterations 334 accuracy : 0.8657844990548205  loss : 0.5386689318903082\n",
      "iterations 335 accuracy : 0.8672547784078976  loss : 0.5382583211166289\n",
      "iterations 336 accuracy : 0.8697752572988868  loss : 0.5378394053462826\n",
      "iterations 337 accuracy : 0.8706154169292165  loss : 0.5374620665742673\n",
      "iterations 338 accuracy : 0.8729258559126234  loss : 0.5370579134258167\n",
      "iterations 339 accuracy : 0.8739760554505356  loss : 0.5366468164588084\n",
      "iterations 340 accuracy : 0.8754463348036127  loss : 0.5362866664503091\n",
      "iterations 341 accuracy : 0.8767065742491074  loss : 0.5359077490498477\n",
      "iterations 342 accuracy : 0.8771266540642723  loss : 0.5355130960927507\n",
      "iterations 343 accuracy : 0.8788069733249317  loss : 0.5351267880080036\n",
      "iterations 344 accuracy : 0.8798571728628439  loss : 0.5347560797488785\n",
      "iterations 345 accuracy : 0.8811174123083386  loss : 0.5344048033848973\n",
      "iterations 346 accuracy : 0.8827977315689981  loss : 0.5340413667305713\n",
      "iterations 347 accuracy : 0.8840579710144928  loss : 0.5336751638096243\n",
      "iterations 348 accuracy : 0.8846880907372401  loss : 0.5333363578157725\n",
      "iterations 349 accuracy : 0.8872085696282294  loss : 0.5329919565310055\n",
      "iterations 350 accuracy : 0.8882587691661415  loss : 0.5326275259676674\n",
      "iterations 351 accuracy : 0.8895190086116362  loss : 0.5322741875930891\n",
      "iterations 352 accuracy : 0.8909892879647133  loss : 0.5319148458497684\n",
      "iterations 353 accuracy : 0.8920394875026255  loss : 0.5315714313172283\n",
      "iterations 354 accuracy : 0.8928796471329553  loss : 0.5312353487964545\n",
      "iterations 355 accuracy : 0.8939298466708675  loss : 0.5308925141536097\n",
      "iterations 356 accuracy : 0.8947700063011972  loss : 0.5305434956466955\n",
      "iterations 357 accuracy : 0.8945599663936148  loss : 0.5302067354834722\n",
      "iterations 358 accuracy : 0.8951900861163621  loss : 0.5298837931274196\n",
      "iterations 359 accuracy : 0.895610165931527  loss : 0.5295740476465849\n",
      "iterations 360 accuracy : 0.8962402856542743  loss : 0.529265823710569\n",
      "iterations 361 accuracy : 0.8966603654694392  loss : 0.5289174305798882\n",
      "iterations 362 accuracy : 0.898970804452846  loss : 0.5285902376418402\n",
      "iterations 363 accuracy : 0.9000210039907582  loss : 0.5282637994314217\n",
      "iterations 364 accuracy : 0.9019113631590002  loss : 0.5279489103876754\n",
      "iterations 365 accuracy : 0.9021214030665826  loss : 0.5276476785637713\n",
      "iterations 366 accuracy : 0.9029615626969124  loss : 0.5273705092408675\n",
      "iterations 367 accuracy : 0.9040117622348246  loss : 0.5270676116475562\n",
      "iterations 368 accuracy : 0.9046418819575719  loss : 0.5267670659145172\n",
      "iterations 369 accuracy : 0.9054820415879017  loss : 0.5264515803365746\n",
      "iterations 370 accuracy : 0.9073724007561437  loss : 0.5261266999321077\n",
      "iterations 371 accuracy : 0.909472799831968  loss : 0.5258210711897535\n",
      "iterations 372 accuracy : 0.9105229993698802  loss : 0.5255434467658505\n",
      "iterations 373 accuracy : 0.91136315900021  loss : 0.52524898597534\n",
      "iterations 374 accuracy : 0.9124133585381222  loss : 0.5249578137822231\n",
      "iterations 375 accuracy : 0.9128334383532871  loss : 0.5246577148987234\n",
      "iterations 376 accuracy : 0.913253518168452  loss : 0.5243664339169521\n",
      "iterations 377 accuracy : 0.913253518168452  loss : 0.5240639033503243\n",
      "iterations 378 accuracy : 0.913253518168452  loss : 0.5237866191619549\n",
      "iterations 379 accuracy : 0.9140936777987818  loss : 0.5234941985850762\n",
      "iterations 380 accuracy : 0.9157739970594413  loss : 0.5232115959641898\n",
      "iterations 381 accuracy : 0.9159840369670237  loss : 0.5229403481012879\n",
      "iterations 382 accuracy : 0.9168241965973535  loss : 0.5226478257302005\n",
      "iterations 383 accuracy : 0.917034236504936  loss : 0.5223558326208981\n",
      "iterations 384 accuracy : 0.9180844360428482  loss : 0.5220708733197782\n",
      "iterations 385 accuracy : 0.9187145557655955  loss : 0.5218312495354801\n",
      "iterations 386 accuracy : 0.9185045158580131  loss : 0.5215580062867337\n",
      "iterations 387 accuracy : 0.9195547153959253  loss : 0.5212776971807332\n",
      "iterations 388 accuracy : 0.9203948750262549  loss : 0.5210319855014767\n",
      "iterations 389 accuracy : 0.9206049149338374  loss : 0.5207662388629648\n",
      "iterations 390 accuracy : 0.9208149548414198  loss : 0.5205268898589995\n",
      "iterations 391 accuracy : 0.9210249947490023  loss : 0.5202774314055966\n",
      "iterations 392 accuracy : 0.9216551144717496  loss : 0.5200071888978876\n",
      "iterations 393 accuracy : 0.9227053140096618  loss : 0.5197651425178272\n",
      "iterations 394 accuracy : 0.9227053140096618  loss : 0.5195323199769669\n",
      "iterations 395 accuracy : 0.923755513547574  loss : 0.5192755752811573\n",
      "iterations 396 accuracy : 0.9243856332703214  loss : 0.519018811561411\n",
      "iterations 397 accuracy : 0.9250157529930687  loss : 0.5187561153455585\n",
      "iterations 398 accuracy : 0.9260659525309809  loss : 0.518503505439616\n",
      "iterations 399 accuracy : 0.9266960722537282  loss : 0.5182562838203952\n",
      "iterations 400 accuracy : 0.9277462717916404  loss : 0.5180117364865479\n",
      "iterations 401 accuracy : 0.9292165511447175  loss : 0.5177523458309282\n",
      "iterations 402 accuracy : 0.9294265910523  loss : 0.5175232399726487\n",
      "iterations 403 accuracy : 0.9306868304977945  loss : 0.5172810359905481\n",
      "iterations 404 accuracy : 0.931316950220542  loss : 0.5170576422372555\n",
      "iterations 405 accuracy : 0.9325771896660365  loss : 0.5168402395756876\n",
      "iterations 406 accuracy : 0.9332073093887838  loss : 0.5166069799596135\n",
      "iterations 407 accuracy : 0.9336273892039487  loss : 0.5163817539787066\n",
      "iterations 408 accuracy : 0.9332073093887838  loss : 0.5161565641392907\n",
      "iterations 409 accuracy : 0.9334173492963663  loss : 0.5159323963077354\n",
      "iterations 410 accuracy : 0.9325771896660365  loss : 0.5156896343430507\n",
      "iterations 411 accuracy : 0.9323671497584541  loss : 0.5154500714763975\n",
      "iterations 412 accuracy : 0.9321571098508716  loss : 0.5152388020113989\n",
      "iterations 413 accuracy : 0.9323671497584541  loss : 0.5150158133050954\n",
      "iterations 414 accuracy : 0.932787229573619  loss : 0.5147946284187748\n",
      "iterations 415 accuracy : 0.9332073093887838  loss : 0.5145890790530675\n",
      "iterations 416 accuracy : 0.9334173492963663  loss : 0.51438564004085\n",
      "iterations 417 accuracy : 0.932787229573619  loss : 0.5141650823390194\n",
      "iterations 418 accuracy : 0.9315269901281243  loss : 0.5139363730882266\n",
      "iterations 419 accuracy : 0.931316950220542  loss : 0.5137263302941453\n",
      "iterations 420 accuracy : 0.9298466708674649  loss : 0.5135066595937041\n",
      "iterations 421 accuracy : 0.9294265910523  loss : 0.513300587738607\n",
      "iterations 422 accuracy : 0.9294265910523  loss : 0.513084981644013\n",
      "iterations 423 accuracy : 0.9290065112371351  loss : 0.5128659893186353\n",
      "iterations 424 accuracy : 0.9279563116992229  loss : 0.5126679642122731\n",
      "iterations 425 accuracy : 0.927536231884058  loss : 0.5124835205697267\n",
      "iterations 426 accuracy : 0.9266960722537282  loss : 0.5122725124932963\n",
      "iterations 427 accuracy : 0.9258559126233984  loss : 0.5120723442097499\n",
      "iterations 428 accuracy : 0.9260659525309809  loss : 0.5118835048538234\n",
      "iterations 429 accuracy : 0.9254358328082336  loss : 0.5116738911711033\n",
      "iterations 430 accuracy : 0.9258559126233984  loss : 0.5114772373218107\n",
      "iterations 431 accuracy : 0.9252257929006511  loss : 0.5112938706913133\n",
      "iterations 432 accuracy : 0.9241755933627389  loss : 0.5111047167790028\n",
      "iterations 433 accuracy : 0.923755513547574  loss : 0.5108895225965414\n",
      "iterations 434 accuracy : 0.9233354337324091  loss : 0.5107035042033217\n",
      "iterations 435 accuracy : 0.9212350346565847  loss : 0.5105121697782413\n",
      "iterations 436 accuracy : 0.9206049149338374  loss : 0.5103473874161444\n",
      "iterations 437 accuracy : 0.9206049149338374  loss : 0.5101513342953171\n",
      "iterations 438 accuracy : 0.9197647553035077  loss : 0.5099720207811844\n",
      "iterations 439 accuracy : 0.9191346355807604  loss : 0.509796617678648\n",
      "iterations 440 accuracy : 0.9189245956731779  loss : 0.5096135297266623\n",
      "iterations 441 accuracy : 0.9180844360428482  loss : 0.509412444684641\n",
      "iterations 442 accuracy : 0.9178743961352657  loss : 0.5092328954078881\n",
      "iterations 443 accuracy : 0.917034236504936  loss : 0.5090536598459281\n",
      "iterations 444 accuracy : 0.9164041167821886  loss : 0.5088729267227775\n",
      "iterations 445 accuracy : 0.9153539172442764  loss : 0.5086920505010398\n",
      "iterations 446 accuracy : 0.9145137576139466  loss : 0.5085003630526529\n",
      "iterations 447 accuracy : 0.9136735979836169  loss : 0.5083240122690224\n",
      "iterations 448 accuracy : 0.9128334383532871  loss : 0.5081452230838339\n",
      "iterations 449 accuracy : 0.9126233984457047  loss : 0.5079634488464086\n",
      "iterations 450 accuracy : 0.91136315900021  loss : 0.507789935123668\n",
      "iterations 451 accuracy : 0.9103129594622978  loss : 0.5076074987645722\n",
      "iterations 452 accuracy : 0.9092627599243857  loss : 0.5074378641908772\n",
      "iterations 453 accuracy : 0.9092627599243857  loss : 0.5072572252433337\n",
      "iterations 454 accuracy : 0.9082125603864735  loss : 0.5070772681221642\n",
      "iterations 455 accuracy : 0.9073724007561437  loss : 0.5069078349314828\n",
      "iterations 456 accuracy : 0.9067422810333964  loss : 0.506723155214078\n",
      "iterations 457 accuracy : 0.9056920814954842  loss : 0.5065668032889887\n",
      "iterations 458 accuracy : 0.9044318420499895  loss : 0.5063829756269949\n",
      "iterations 459 accuracy : 0.9038017223272422  loss : 0.5062167934141505\n",
      "iterations 460 accuracy : 0.9023314429741651  loss : 0.5060499765638975\n",
      "iterations 461 accuracy : 0.9017013232514177  loss : 0.5058830503800901\n",
      "iterations 462 accuracy : 0.9012812434362529  loss : 0.5057029444221175\n",
      "iterations 463 accuracy : 0.900861163621088  loss : 0.5055379448377925\n",
      "iterations 464 accuracy : 0.9004410838059231  loss : 0.5053771159583206\n",
      "iterations 465 accuracy : 0.9004410838059231  loss : 0.5052071195474583\n",
      "iterations 466 accuracy : 0.9000210039907582  loss : 0.505058059965004\n",
      "iterations 467 accuracy : 0.8987607645452635  loss : 0.50489738026024\n",
      "iterations 468 accuracy : 0.8983406847300988  loss : 0.5047340166862092\n",
      "iterations 469 accuracy : 0.8979206049149339  loss : 0.5045787533916865\n",
      "iterations 470 accuracy : 0.8977105650073514  loss : 0.5044141974088563\n",
      "iterations 471 accuracy : 0.8962402856542743  loss : 0.5042504810949344\n",
      "iterations 472 accuracy : 0.8958202058391095  loss : 0.5040917533374063\n",
      "iterations 473 accuracy : 0.8949800462087797  loss : 0.5039325806007761\n",
      "iterations 474 accuracy : 0.8939298466708675  loss : 0.5037773414282155\n",
      "iterations 475 accuracy : 0.893719806763285  loss : 0.5036267680075122\n",
      "iterations 476 accuracy : 0.8935097668557026  loss : 0.50348326559173\n",
      "iterations 477 accuracy : 0.8930896870405377  loss : 0.503332745350989\n",
      "iterations 478 accuracy : 0.8920394875026255  loss : 0.5031779502620091\n",
      "iterations 479 accuracy : 0.8909892879647133  loss : 0.5030372920156968\n",
      "iterations 480 accuracy : 0.8909892879647133  loss : 0.5028817708988857\n",
      "iterations 481 accuracy : 0.8897290485192186  loss : 0.5027247574903669\n",
      "iterations 482 accuracy : 0.888468809073724  loss : 0.5025725131542382\n",
      "iterations 483 accuracy : 0.886998529720647  loss : 0.5024222771765998\n",
      "iterations 484 accuracy : 0.8865784499054821  loss : 0.5022815147555935\n",
      "iterations 485 accuracy : 0.8859483301827347  loss : 0.5021309344509116\n",
      "iterations 486 accuracy : 0.885108170552405  loss : 0.501987340820272\n",
      "iterations 487 accuracy : 0.8846880907372401  loss : 0.5018355995389272\n",
      "iterations 488 accuracy : 0.8836378911993279  loss : 0.5016870774854078\n",
      "iterations 489 accuracy : 0.883217811384163  loss : 0.5015358807708442\n",
      "iterations 490 accuracy : 0.8827977315689981  loss : 0.5013877576655834\n",
      "iterations 491 accuracy : 0.8817475320310859  loss : 0.5012504556932591\n",
      "iterations 492 accuracy : 0.881327452215921  loss : 0.5011111142532994\n",
      "iterations 493 accuracy : 0.8809073724007561  loss : 0.5009760334538935\n",
      "iterations 494 accuracy : 0.8800672127704264  loss : 0.5008312608256538\n",
      "iterations 495 accuracy : 0.8796471329552615  loss : 0.5006888211795693\n",
      "iterations 496 accuracy : 0.8790170132325141  loss : 0.5005384628472286\n",
      "iterations 497 accuracy : 0.8790170132325141  loss : 0.5004074256457228\n",
      "iterations 498 accuracy : 0.8779668136946019  loss : 0.500257062733301\n",
      "iterations 499 accuracy : 0.8777567737870196  loss : 0.5001116913047609\n",
      "iterations 500 accuracy : 0.877546733879437  loss : 0.499966265787487\n",
      "iterations 501 accuracy : 0.8771266540642723  loss : 0.49982426684134673\n",
      "iterations 502 accuracy : 0.87607645452636  loss : 0.49968271154084276\n",
      "iterations 503 accuracy : 0.8754463348036127  loss : 0.499534599531704\n",
      "iterations 504 accuracy : 0.8752362948960303  loss : 0.49940740211088275\n",
      "iterations 505 accuracy : 0.8750262549884478  loss : 0.49926135445390607\n",
      "iterations 506 accuracy : 0.8746061751732829  loss : 0.4991175637864144\n",
      "iterations 507 accuracy : 0.8739760554505356  loss : 0.4989760309912777\n",
      "iterations 508 accuracy : 0.8731358958202058  loss : 0.4988406460965269\n",
      "iterations 509 accuracy : 0.8722957361898761  loss : 0.4987055263820492\n",
      "iterations 510 accuracy : 0.8718756563747112  loss : 0.49858436772794157\n",
      "iterations 511 accuracy : 0.8710354967443814  loss : 0.49845114767935733\n",
      "iterations 512 accuracy : 0.8706154169292165  loss : 0.49831034953869385\n",
      "iterations 513 accuracy : 0.8701953371140516  loss : 0.49818248787210906\n",
      "iterations 514 accuracy : 0.8695652173913043  loss : 0.49805912341846315\n",
      "iterations 515 accuracy : 0.8687250577609746  loss : 0.49792738949807636\n",
      "iterations 516 accuracy : 0.8685150178533921  loss : 0.4978078900216979\n",
      "iterations 517 accuracy : 0.8683049779458097  loss : 0.4976822026176068\n",
      "iterations 518 accuracy : 0.867044738500315  loss : 0.4975584362824693\n",
      "iterations 519 accuracy : 0.8657844990548205  loss : 0.49742534352920237\n",
      "iterations 520 accuracy : 0.8653644192396556  loss : 0.4972909485274794\n",
      "iterations 521 accuracy : 0.8651543793320731  loss : 0.4971607663177656\n",
      "iterations 522 accuracy : 0.8645242596093258  loss : 0.497030318992141\n",
      "iterations 523 accuracy : 0.863684099978996  loss : 0.49689695437585035\n",
      "iterations 524 accuracy : 0.8632640201638311  loss : 0.49677859536929375\n",
      "iterations 525 accuracy : 0.8632640201638311  loss : 0.4966596597594782\n",
      "iterations 526 accuracy : 0.8624238605335014  loss : 0.496531684075861\n",
      "iterations 527 accuracy : 0.8622138206259189  loss : 0.4964073482300243\n",
      "iterations 528 accuracy : 0.8613736609955892  loss : 0.49628109525868774\n",
      "iterations 529 accuracy : 0.8603234614576769  loss : 0.4961519416490455\n",
      "iterations 530 accuracy : 0.8596933417349296  loss : 0.4960352862365736\n",
      "iterations 531 accuracy : 0.8594833018273472  loss : 0.49591979533634734\n",
      "iterations 532 accuracy : 0.8586431421970174  loss : 0.4957863234701022\n",
      "iterations 533 accuracy : 0.8582230623818525  loss : 0.49565814038696093\n",
      "iterations 534 accuracy : 0.8575929426591052  loss : 0.495527909747464\n",
      "iterations 535 accuracy : 0.8573829027515227  loss : 0.4954162819000773\n",
      "iterations 536 accuracy : 0.8571728628439403  loss : 0.4952961505507562\n",
      "iterations 537 accuracy : 0.8571728628439403  loss : 0.49517029281418723\n",
      "iterations 538 accuracy : 0.8567527830287754  loss : 0.49504147711425256\n",
      "iterations 539 accuracy : 0.8563327032136105  loss : 0.4949115557294301\n",
      "iterations 540 accuracy : 0.8563327032136105  loss : 0.49480083061712093\n",
      "iterations 541 accuracy : 0.8557025834908633  loss : 0.49468051398732626\n",
      "iterations 542 accuracy : 0.8554925435832809  loss : 0.4945579198566587\n",
      "iterations 543 accuracy : 0.8552825036756984  loss : 0.494441649107526\n",
      "iterations 544 accuracy : 0.8546523839529511  loss : 0.494322435846922\n",
      "iterations 545 accuracy : 0.8540222642302038  loss : 0.49419567556241006\n",
      "iterations 546 accuracy : 0.8529720646922916  loss : 0.49407023665752486\n",
      "iterations 547 accuracy : 0.8525519848771267  loss : 0.4939479391308865\n",
      "iterations 548 accuracy : 0.8523419449695442  loss : 0.49382974283864955\n",
      "iterations 549 accuracy : 0.8521319050619618  loss : 0.493713200201994\n",
      "iterations 550 accuracy : 0.8515017853392145  loss : 0.49360105082836775\n",
      "iterations 551 accuracy : 0.8506616257088847  loss : 0.49347731310794274\n",
      "iterations 552 accuracy : 0.8498214660785549  loss : 0.49335896840858545\n",
      "iterations 553 accuracy : 0.8496114261709725  loss : 0.49324152536368643\n",
      "iterations 554 accuracy : 0.84940138626339  loss : 0.4931267925156048\n",
      "iterations 555 accuracy : 0.8485612266330603  loss : 0.4930048365206203\n",
      "iterations 556 accuracy : 0.8481411468178954  loss : 0.49288725915731835\n",
      "iterations 557 accuracy : 0.847511027095148  loss : 0.49276996729341327\n",
      "iterations 558 accuracy : 0.8468809073724007  loss : 0.4926507521499015\n",
      "iterations 559 accuracy : 0.8458307078344885  loss : 0.49253496642197425\n",
      "iterations 560 accuracy : 0.8458307078344885  loss : 0.49242003413997\n",
      "iterations 561 accuracy : 0.8443604284814115  loss : 0.4922960016306157\n",
      "iterations 562 accuracy : 0.8441503885738291  loss : 0.49218091848571494\n",
      "iterations 563 accuracy : 0.8441503885738291  loss : 0.49207785120381703\n",
      "iterations 564 accuracy : 0.8437303087586642  loss : 0.49196689854053005\n",
      "iterations 565 accuracy : 0.8433102289434993  loss : 0.49185235858793025\n",
      "iterations 566 accuracy : 0.8428901491283344  loss : 0.49173930659611165\n",
      "iterations 567 accuracy : 0.842680109220752  loss : 0.49163246847558856\n",
      "iterations 568 accuracy : 0.842680109220752  loss : 0.49152481628462313\n",
      "iterations 569 accuracy : 0.8424700693131695  loss : 0.49140804242676894\n",
      "iterations 570 accuracy : 0.8422600294055871  loss : 0.4913010518686383\n",
      "iterations 571 accuracy : 0.8416299096828398  loss : 0.4911830104249261\n",
      "iterations 572 accuracy : 0.8412098298676749  loss : 0.49106999181350003\n",
      "iterations 573 accuracy : 0.8409997899600924  loss : 0.4909542170075242\n",
      "iterations 574 accuracy : 0.8405797101449275  loss : 0.4908445368595549\n",
      "iterations 575 accuracy : 0.8405797101449275  loss : 0.4907438882134967\n",
      "iterations 576 accuracy : 0.8405797101449275  loss : 0.49063607106217255\n",
      "iterations 577 accuracy : 0.8403696702373451  loss : 0.49052944218124267\n",
      "iterations 578 accuracy : 0.8403696702373451  loss : 0.49042614470501295\n",
      "iterations 579 accuracy : 0.8393194706994329  loss : 0.4903196560218914\n",
      "iterations 580 accuracy : 0.8391094307918504  loss : 0.49020643189224034\n",
      "iterations 581 accuracy : 0.8386893509766856  loss : 0.49010033702475164\n",
      "iterations 582 accuracy : 0.8384793110691031  loss : 0.48999234146805587\n",
      "iterations 583 accuracy : 0.8384793110691031  loss : 0.48988802653539504\n",
      "iterations 584 accuracy : 0.8382692711615207  loss : 0.4897804376511309\n",
      "iterations 585 accuracy : 0.8378491913463558  loss : 0.4896736890749971\n",
      "iterations 586 accuracy : 0.8374291115311909  loss : 0.48956499818666604\n",
      "iterations 587 accuracy : 0.8372190716236085  loss : 0.4894495916513987\n",
      "iterations 588 accuracy : 0.8365889519008611  loss : 0.48934088883310517\n",
      "iterations 589 accuracy : 0.8363789119932787  loss : 0.48923723890586085\n",
      "iterations 590 accuracy : 0.8359588321781138  loss : 0.4891259206797209\n",
      "iterations 591 accuracy : 0.8353287124553666  loss : 0.48901779410159985\n",
      "iterations 592 accuracy : 0.835118672547784  loss : 0.488907722466825\n",
      "iterations 593 accuracy : 0.835118672547784  loss : 0.4888056235704641\n",
      "iterations 594 accuracy : 0.8344885528250368  loss : 0.4887035623454864\n",
      "iterations 595 accuracy : 0.8344885528250368  loss : 0.4886061595365912\n",
      "iterations 596 accuracy : 0.8338584331022895  loss : 0.48850269593807816\n",
      "iterations 597 accuracy : 0.833648393194707  loss : 0.48840248422032406\n",
      "iterations 598 accuracy : 0.8334383532871246  loss : 0.4882980933724751\n",
      "iterations 599 accuracy : 0.8334383532871246  loss : 0.4881944833702782\n",
      "iterations 600 accuracy : 0.8332283133795421  loss : 0.4880929699586418\n",
      "iterations 601 accuracy : 0.8332283133795421  loss : 0.4879920603106183\n",
      "iterations 602 accuracy : 0.8330182734719597  loss : 0.4878813965650862\n",
      "iterations 603 accuracy : 0.8330182734719597  loss : 0.48777920349852255\n",
      "iterations 604 accuracy : 0.8325981936567948  loss : 0.48767633450670966\n",
      "iterations 605 accuracy : 0.8321781138416299  loss : 0.48756737536418754\n",
      "iterations 606 accuracy : 0.8321781138416299  loss : 0.4874648311836317\n",
      "iterations 607 accuracy : 0.8319680739340475  loss : 0.48736378801005753\n",
      "iterations 608 accuracy : 0.8311279143037177  loss : 0.48725912699969687\n",
      "iterations 609 accuracy : 0.8307078344885528  loss : 0.48716091791116806\n",
      "iterations 610 accuracy : 0.8307078344885528  loss : 0.48705761235009065\n",
      "iterations 611 accuracy : 0.8304977945809704  loss : 0.4869564782656637\n",
      "iterations 612 accuracy : 0.8300777147658055  loss : 0.48685057006266347\n",
      "iterations 613 accuracy : 0.8300777147658055  loss : 0.48675299774696495\n",
      "iterations 614 accuracy : 0.8292375551354757  loss : 0.4866522036888873\n",
      "iterations 615 accuracy : 0.8292375551354757  loss : 0.48655833926311964\n",
      "iterations 616 accuracy : 0.8290275152278933  loss : 0.48645610429544794\n",
      "iterations 617 accuracy : 0.8290275152278933  loss : 0.4863567084492448\n",
      "iterations 618 accuracy : 0.8290275152278933  loss : 0.48625657313012205\n",
      "iterations 619 accuracy : 0.8290275152278933  loss : 0.48615936181912983\n",
      "iterations 620 accuracy : 0.8290275152278933  loss : 0.4860595910696089\n",
      "iterations 621 accuracy : 0.8286074354127284  loss : 0.4859600837028771\n",
      "iterations 622 accuracy : 0.8286074354127284  loss : 0.4858649746452534\n",
      "iterations 623 accuracy : 0.8288174753203108  loss : 0.48577148456170577\n",
      "iterations 624 accuracy : 0.8286074354127284  loss : 0.485674613979536\n",
      "iterations 625 accuracy : 0.828397395505146  loss : 0.48557603979302144\n",
      "iterations 626 accuracy : 0.8281873555975635  loss : 0.485474908349022\n",
      "iterations 627 accuracy : 0.8279773156899811  loss : 0.4853744550200582\n",
      "iterations 628 accuracy : 0.8277672757823986  loss : 0.4852728826794658\n",
      "iterations 629 accuracy : 0.8277672757823986  loss : 0.48517836265128467\n",
      "iterations 630 accuracy : 0.8277672757823986  loss : 0.4850765443982251\n",
      "iterations 631 accuracy : 0.8275572358748162  loss : 0.4849750338797898\n",
      "iterations 632 accuracy : 0.826507036336904  loss : 0.4848751032453417\n",
      "iterations 633 accuracy : 0.826507036336904  loss : 0.4847819781142892\n",
      "iterations 634 accuracy : 0.826507036336904  loss : 0.4846852791735891\n",
      "iterations 635 accuracy : 0.8267170762444864  loss : 0.48459460092824524\n",
      "iterations 636 accuracy : 0.826507036336904  loss : 0.4844990110756569\n",
      "iterations 637 accuracy : 0.8260869565217391  loss : 0.4844016278217497\n",
      "iterations 638 accuracy : 0.8260869565217391  loss : 0.48430709444140013\n",
      "iterations 639 accuracy : 0.8260869565217391  loss : 0.4842167774923923\n",
      "iterations 640 accuracy : 0.8260869565217391  loss : 0.48412021639175373\n",
      "iterations 641 accuracy : 0.8260869565217391  loss : 0.48402737710345606\n",
      "iterations 642 accuracy : 0.8256668767065742  loss : 0.4839293116478005\n",
      "iterations 643 accuracy : 0.8258769166141567  loss : 0.4838344833595249\n",
      "iterations 644 accuracy : 0.8252467968914093  loss : 0.4837368492316863\n",
      "iterations 645 accuracy : 0.825036756983827  loss : 0.48363772041349296\n",
      "iterations 646 accuracy : 0.825036756983827  loss : 0.48354229031494844\n",
      "iterations 647 accuracy : 0.825036756983827  loss : 0.4834513132856518\n",
      "iterations 648 accuracy : 0.825036756983827  loss : 0.48335983866633125\n",
      "iterations 649 accuracy : 0.825036756983827  loss : 0.4832674072358451\n",
      "iterations 650 accuracy : 0.825036756983827  loss : 0.4831792955386148\n",
      "iterations 651 accuracy : 0.825036756983827  loss : 0.483084922180153\n",
      "iterations 652 accuracy : 0.825036756983827  loss : 0.4829875160586266\n",
      "iterations 653 accuracy : 0.8248267170762444  loss : 0.4828894281083004\n",
      "iterations 654 accuracy : 0.8248267170762444  loss : 0.48280052238401544\n",
      "iterations 655 accuracy : 0.8248267170762444  loss : 0.48270582615592306\n",
      "iterations 656 accuracy : 0.8244066372610797  loss : 0.4826112504169311\n",
      "iterations 657 accuracy : 0.8244066372610797  loss : 0.48251835106206525\n",
      "iterations 658 accuracy : 0.8244066372610797  loss : 0.48242618662014775\n",
      "iterations 659 accuracy : 0.8244066372610797  loss : 0.48233465601116365\n",
      "iterations 660 accuracy : 0.8241965973534972  loss : 0.48224053795854177\n",
      "iterations 661 accuracy : 0.8241965973534972  loss : 0.48214950170718834\n",
      "iterations 662 accuracy : 0.8239865574459148  loss : 0.482056638799295\n",
      "iterations 663 accuracy : 0.8241965973534972  loss : 0.48196634078788475\n",
      "iterations 664 accuracy : 0.8239865574459148  loss : 0.48187321494513\n",
      "iterations 665 accuracy : 0.8239865574459148  loss : 0.48178386344674734\n",
      "iterations 666 accuracy : 0.8239865574459148  loss : 0.48168970520793425\n",
      "iterations 667 accuracy : 0.8239865574459148  loss : 0.4816013027710913\n",
      "iterations 668 accuracy : 0.8237765175383323  loss : 0.48150349991634506\n",
      "iterations 669 accuracy : 0.8235664776307499  loss : 0.4814079166752544\n",
      "iterations 670 accuracy : 0.8235664776307499  loss : 0.4813178256492272\n",
      "iterations 671 accuracy : 0.8235664776307499  loss : 0.48122428769906783\n",
      "iterations 672 accuracy : 0.8235664776307499  loss : 0.4811335606367086\n",
      "iterations 673 accuracy : 0.8233564377231674  loss : 0.48104073547741705\n",
      "iterations 674 accuracy : 0.8233564377231674  loss : 0.4809508289448596\n",
      "iterations 675 accuracy : 0.8233564377231674  loss : 0.48085841346189256\n",
      "iterations 676 accuracy : 0.823146397815585  loss : 0.4807630006902704\n",
      "iterations 677 accuracy : 0.8233564377231674  loss : 0.48067494674099853\n",
      "iterations 678 accuracy : 0.8233564377231674  loss : 0.4805849559456508\n",
      "iterations 679 accuracy : 0.8233564377231674  loss : 0.4804924027455385\n",
      "iterations 680 accuracy : 0.823146397815585  loss : 0.4803994227573045\n",
      "iterations 681 accuracy : 0.823146397815585  loss : 0.4803109338912232\n",
      "iterations 682 accuracy : 0.823146397815585  loss : 0.48022128841838946\n",
      "iterations 683 accuracy : 0.8229363579080026  loss : 0.4801270554399137\n",
      "iterations 684 accuracy : 0.8227263180004201  loss : 0.48003470225908124\n",
      "iterations 685 accuracy : 0.8229363579080026  loss : 0.4799454635802649\n",
      "iterations 686 accuracy : 0.8227263180004201  loss : 0.4798539552067382\n",
      "iterations 687 accuracy : 0.8227263180004201  loss : 0.47976080734377996\n",
      "iterations 688 accuracy : 0.8225162780928377  loss : 0.47966724515592263\n",
      "iterations 689 accuracy : 0.8225162780928377  loss : 0.47957696586862825\n",
      "iterations 690 accuracy : 0.8225162780928377  loss : 0.47948642620276194\n",
      "iterations 691 accuracy : 0.8223062381852552  loss : 0.47939338026790795\n",
      "iterations 692 accuracy : 0.8220961982776728  loss : 0.4793001798495395\n",
      "iterations 693 accuracy : 0.8220961982776728  loss : 0.4792103538208496\n",
      "iterations 694 accuracy : 0.8220961982776728  loss : 0.47911802272862747\n",
      "iterations 695 accuracy : 0.8218861583700903  loss : 0.47902522033490436\n",
      "iterations 696 accuracy : 0.821256038647343  loss : 0.47893381038351635\n",
      "iterations 697 accuracy : 0.8208359588321781  loss : 0.4788428295616312\n",
      "iterations 698 accuracy : 0.8208359588321781  loss : 0.4787544358285824\n",
      "iterations 699 accuracy : 0.8206259189245957  loss : 0.4786662450838647\n",
      "iterations 700 accuracy : 0.8197857592942659  loss : 0.4785746506179893\n",
      "iterations 701 accuracy : 0.8206259189245957  loss : 0.4784875573030744\n",
      "iterations 702 accuracy : 0.8206259189245957  loss : 0.478400779682806\n",
      "iterations 703 accuracy : 0.8195757193866835  loss : 0.47830728598583616\n",
      "iterations 704 accuracy : 0.8195757193866835  loss : 0.4782193715452087\n",
      "iterations 705 accuracy : 0.819365679479101  loss : 0.47812969292270785\n",
      "iterations 706 accuracy : 0.8187355597563537  loss : 0.47804053881031217\n",
      "iterations 707 accuracy : 0.8187355597563537  loss : 0.4779535099429441\n",
      "iterations 708 accuracy : 0.8187355597563537  loss : 0.477866093270996\n",
      "iterations 709 accuracy : 0.8187355597563537  loss : 0.4777773533253619\n",
      "iterations 710 accuracy : 0.8187355597563537  loss : 0.4776898188850039\n",
      "iterations 711 accuracy : 0.8183154799411888  loss : 0.47760003165072945\n",
      "iterations 712 accuracy : 0.8183154799411888  loss : 0.47751369839875885\n",
      "iterations 713 accuracy : 0.8187355597563537  loss : 0.4774295361265555\n",
      "iterations 714 accuracy : 0.8189455996639361  loss : 0.47734289245523703\n",
      "iterations 715 accuracy : 0.8187355597563537  loss : 0.47725472045833156\n",
      "iterations 716 accuracy : 0.8185255198487713  loss : 0.47716305454606567\n",
      "iterations 717 accuracy : 0.8183154799411888  loss : 0.4770729083955733\n",
      "iterations 718 accuracy : 0.8181054400336064  loss : 0.47698545255252095\n",
      "iterations 719 accuracy : 0.8178954001260239  loss : 0.4768981074234008\n",
      "iterations 720 accuracy : 0.8181054400336064  loss : 0.4768128560744307\n",
      "iterations 721 accuracy : 0.8181054400336064  loss : 0.47672652457890274\n",
      "iterations 722 accuracy : 0.8181054400336064  loss : 0.47663917490486724\n",
      "iterations 723 accuracy : 0.8183154799411888  loss : 0.4765529791689562\n",
      "iterations 724 accuracy : 0.8183154799411888  loss : 0.47646850042562594\n",
      "iterations 725 accuracy : 0.8183154799411888  loss : 0.47638087269783647\n",
      "iterations 726 accuracy : 0.8183154799411888  loss : 0.476293357920111\n",
      "iterations 727 accuracy : 0.8183154799411888  loss : 0.4762080905130037\n",
      "iterations 728 accuracy : 0.8183154799411888  loss : 0.47612229874547124\n",
      "iterations 729 accuracy : 0.8183154799411888  loss : 0.4760337469839151\n",
      "iterations 730 accuracy : 0.8183154799411888  loss : 0.4759471630143422\n",
      "iterations 731 accuracy : 0.8187355597563537  loss : 0.47586383542630906\n",
      "iterations 732 accuracy : 0.8185255198487713  loss : 0.4757778390973537\n",
      "iterations 733 accuracy : 0.8183154799411888  loss : 0.47569142710883716\n",
      "iterations 734 accuracy : 0.8181054400336064  loss : 0.4756025293697858\n",
      "iterations 735 accuracy : 0.8181054400336064  loss : 0.47551749723198894\n",
      "iterations 736 accuracy : 0.8183154799411888  loss : 0.4754322873658522\n",
      "iterations 737 accuracy : 0.8183154799411888  loss : 0.4753479598506587\n",
      "iterations 738 accuracy : 0.8181054400336064  loss : 0.47526091435100587\n",
      "iterations 739 accuracy : 0.8181054400336064  loss : 0.4751755874413016\n",
      "iterations 740 accuracy : 0.8178954001260239  loss : 0.47508948384037997\n",
      "iterations 741 accuracy : 0.8181054400336064  loss : 0.4750052949028007\n",
      "iterations 742 accuracy : 0.8181054400336064  loss : 0.4749198875753566\n",
      "iterations 743 accuracy : 0.8181054400336064  loss : 0.47483571379959544\n",
      "iterations 744 accuracy : 0.8183154799411888  loss : 0.47475277367886154\n",
      "iterations 745 accuracy : 0.8183154799411888  loss : 0.4746683073523819\n",
      "iterations 746 accuracy : 0.8183154799411888  loss : 0.47458518758731966\n",
      "iterations 747 accuracy : 0.8183154799411888  loss : 0.47449964857352095\n",
      "iterations 748 accuracy : 0.8183154799411888  loss : 0.47441446481479793\n",
      "iterations 749 accuracy : 0.8185255198487713  loss : 0.47433158259324454\n",
      "iterations 750 accuracy : 0.8185255198487713  loss : 0.47424782127166487\n",
      "iterations 751 accuracy : 0.8185255198487713  loss : 0.474161999943609\n",
      "iterations 752 accuracy : 0.8185255198487713  loss : 0.4740766012780925\n",
      "iterations 753 accuracy : 0.8185255198487713  loss : 0.4739913481735196\n",
      "iterations 754 accuracy : 0.8185255198487713  loss : 0.47390599474770434\n",
      "iterations 755 accuracy : 0.8185255198487713  loss : 0.47382043390411727\n",
      "iterations 756 accuracy : 0.8185255198487713  loss : 0.4737360917615962\n",
      "iterations 757 accuracy : 0.8185255198487713  loss : 0.4736505027975519\n",
      "iterations 758 accuracy : 0.8187355597563537  loss : 0.4735690949856752\n",
      "iterations 759 accuracy : 0.8185255198487713  loss : 0.47348293959721777\n",
      "iterations 760 accuracy : 0.8185255198487713  loss : 0.4733984251030968\n",
      "iterations 761 accuracy : 0.8185255198487713  loss : 0.47331551004252587\n",
      "iterations 762 accuracy : 0.8185255198487713  loss : 0.4732317357976498\n",
      "iterations 763 accuracy : 0.8185255198487713  loss : 0.4731483574799206\n",
      "iterations 764 accuracy : 0.8185255198487713  loss : 0.4730659677902116\n",
      "iterations 765 accuracy : 0.8185255198487713  loss : 0.4729824990303948\n",
      "iterations 766 accuracy : 0.8185255198487713  loss : 0.4729004989218645\n",
      "iterations 767 accuracy : 0.8185255198487713  loss : 0.47281545362500893\n",
      "iterations 768 accuracy : 0.8185255198487713  loss : 0.4727316823153854\n",
      "iterations 769 accuracy : 0.8183154799411888  loss : 0.4726478721388499\n",
      "iterations 770 accuracy : 0.8185255198487713  loss : 0.47256605514432704\n",
      "iterations 771 accuracy : 0.8185255198487713  loss : 0.4724835724288549\n",
      "iterations 772 accuracy : 0.8185255198487713  loss : 0.4724005463023274\n",
      "iterations 773 accuracy : 0.8187355597563537  loss : 0.472317568576538\n",
      "iterations 774 accuracy : 0.8185255198487713  loss : 0.4722325271840133\n",
      "iterations 775 accuracy : 0.8187355597563537  loss : 0.4721519044017455\n",
      "iterations 776 accuracy : 0.8187355597563537  loss : 0.472069524102032\n",
      "iterations 777 accuracy : 0.8189455996639361  loss : 0.47198654151045505\n",
      "iterations 778 accuracy : 0.8189455996639361  loss : 0.4719036570284354\n",
      "iterations 779 accuracy : 0.8189455996639361  loss : 0.471822262342614\n",
      "iterations 780 accuracy : 0.8189455996639361  loss : 0.47173905508449476\n",
      "iterations 781 accuracy : 0.8189455996639361  loss : 0.47165644064553786\n",
      "iterations 782 accuracy : 0.8189455996639361  loss : 0.4715736634534984\n",
      "iterations 783 accuracy : 0.8189455996639361  loss : 0.4714913315125134\n",
      "iterations 784 accuracy : 0.8189455996639361  loss : 0.471407936267804\n",
      "iterations 785 accuracy : 0.8189455996639361  loss : 0.47132725889908306\n",
      "iterations 786 accuracy : 0.8189455996639361  loss : 0.4712453430562642\n",
      "iterations 787 accuracy : 0.8189455996639361  loss : 0.4711626627604097\n",
      "iterations 788 accuracy : 0.8189455996639361  loss : 0.47108058969079686\n",
      "iterations 789 accuracy : 0.8189455996639361  loss : 0.4709993691061452\n",
      "iterations 790 accuracy : 0.819365679479101  loss : 0.4709204980217342\n",
      "iterations 791 accuracy : 0.8189455996639361  loss : 0.4708380732706539\n",
      "iterations 792 accuracy : 0.819365679479101  loss : 0.47075587254479906\n",
      "iterations 793 accuracy : 0.8191556395715186  loss : 0.4706731884746739\n",
      "iterations 794 accuracy : 0.819365679479101  loss : 0.4705937051548486\n",
      "iterations 795 accuracy : 0.8202058391094308  loss : 0.4705147931727386\n",
      "iterations 796 accuracy : 0.8202058391094308  loss : 0.4704347895197878\n",
      "iterations 797 accuracy : 0.8202058391094308  loss : 0.47035352014356735\n",
      "iterations 798 accuracy : 0.8204158790170132  loss : 0.47027484954627663\n",
      "iterations 799 accuracy : 0.8204158790170132  loss : 0.4701930348179973\n",
      "iterations 800 accuracy : 0.8204158790170132  loss : 0.47011152947758506\n",
      "iterations 801 accuracy : 0.8204158790170132  loss : 0.4700296186033832\n",
      "iterations 802 accuracy : 0.8202058391094308  loss : 0.46994671226447043\n",
      "iterations 803 accuracy : 0.8195757193866835  loss : 0.469865746614567\n",
      "iterations 804 accuracy : 0.8195757193866835  loss : 0.4697870206384846\n",
      "iterations 805 accuracy : 0.819365679479101  loss : 0.4697063673954653\n",
      "iterations 806 accuracy : 0.819365679479101  loss : 0.4696230234254092\n",
      "iterations 807 accuracy : 0.8191556395715186  loss : 0.4695411604040987\n",
      "iterations 808 accuracy : 0.819365679479101  loss : 0.4694600680553665\n",
      "iterations 809 accuracy : 0.819365679479101  loss : 0.46938027666638144\n",
      "iterations 810 accuracy : 0.819365679479101  loss : 0.4693013491406525\n",
      "iterations 811 accuracy : 0.8199957992018484  loss : 0.46922146366544865\n",
      "iterations 812 accuracy : 0.8199957992018484  loss : 0.4691429932659429\n",
      "iterations 813 accuracy : 0.819365679479101  loss : 0.46906272920061454\n",
      "iterations 814 accuracy : 0.819365679479101  loss : 0.4689839811995378\n",
      "iterations 815 accuracy : 0.819365679479101  loss : 0.46890502249220717\n",
      "iterations 816 accuracy : 0.819365679479101  loss : 0.46882426689492535\n",
      "iterations 817 accuracy : 0.8199957992018484  loss : 0.4687456778465344\n",
      "iterations 818 accuracy : 0.8197857592942659  loss : 0.4686642210779921\n",
      "iterations 819 accuracy : 0.8202058391094308  loss : 0.4685841818450354\n",
      "iterations 820 accuracy : 0.8204158790170132  loss : 0.46850588078813155\n",
      "iterations 821 accuracy : 0.8204158790170132  loss : 0.4684268781468947\n",
      "iterations 822 accuracy : 0.8206259189245957  loss : 0.4683466549952867\n",
      "iterations 823 accuracy : 0.8204158790170132  loss : 0.4682653337096975\n",
      "iterations 824 accuracy : 0.8204158790170132  loss : 0.4681862347607025\n",
      "iterations 825 accuracy : 0.8204158790170132  loss : 0.4681060257482675\n",
      "iterations 826 accuracy : 0.8206259189245957  loss : 0.4680270888235767\n",
      "iterations 827 accuracy : 0.8206259189245957  loss : 0.46794916219274013\n",
      "iterations 828 accuracy : 0.8206259189245957  loss : 0.46787035588685966\n",
      "iterations 829 accuracy : 0.8206259189245957  loss : 0.4677912474326888\n",
      "iterations 830 accuracy : 0.8206259189245957  loss : 0.4677115981851631\n",
      "iterations 831 accuracy : 0.8206259189245957  loss : 0.46763261118928146\n",
      "iterations 832 accuracy : 0.8206259189245957  loss : 0.4675531304526015\n",
      "iterations 833 accuracy : 0.8206259189245957  loss : 0.46747404806727877\n",
      "iterations 834 accuracy : 0.8206259189245957  loss : 0.46739429687052597\n",
      "iterations 835 accuracy : 0.8206259189245957  loss : 0.4673170642547122\n",
      "iterations 836 accuracy : 0.8206259189245957  loss : 0.4672364845334845\n",
      "iterations 837 accuracy : 0.8206259189245957  loss : 0.46715649804448706\n",
      "iterations 838 accuracy : 0.8206259189245957  loss : 0.46707829957505614\n",
      "iterations 839 accuracy : 0.8206259189245957  loss : 0.4670003772635601\n",
      "iterations 840 accuracy : 0.8206259189245957  loss : 0.4669225958808638\n",
      "iterations 841 accuracy : 0.8206259189245957  loss : 0.4668433048877177\n",
      "iterations 842 accuracy : 0.8206259189245957  loss : 0.4667656153882074\n",
      "iterations 843 accuracy : 0.8206259189245957  loss : 0.46668892861159517\n",
      "iterations 844 accuracy : 0.8206259189245957  loss : 0.46661034613397223\n",
      "iterations 845 accuracy : 0.8206259189245957  loss : 0.4665330668564631\n",
      "iterations 846 accuracy : 0.8206259189245957  loss : 0.46645482602143723\n",
      "iterations 847 accuracy : 0.8206259189245957  loss : 0.4663760486716891\n",
      "iterations 848 accuracy : 0.8206259189245957  loss : 0.4663000650185247\n",
      "iterations 849 accuracy : 0.8206259189245957  loss : 0.4662213757194533\n",
      "iterations 850 accuracy : 0.8206259189245957  loss : 0.4661428729030427\n",
      "iterations 851 accuracy : 0.8206259189245957  loss : 0.4660647651688825\n",
      "iterations 852 accuracy : 0.8206259189245957  loss : 0.4659882874495222\n",
      "iterations 853 accuracy : 0.8210459987397606  loss : 0.46590967211742623\n",
      "iterations 854 accuracy : 0.8210459987397606  loss : 0.4658332865960785\n",
      "iterations 855 accuracy : 0.8210459987397606  loss : 0.4657551834590171\n",
      "iterations 856 accuracy : 0.8218861583700903  loss : 0.4656787124502249\n",
      "iterations 857 accuracy : 0.8218861583700903  loss : 0.465600975697384\n",
      "iterations 858 accuracy : 0.8218861583700903  loss : 0.4655225317180588\n",
      "iterations 859 accuracy : 0.8218861583700903  loss : 0.465444570334027\n",
      "iterations 860 accuracy : 0.8220961982776728  loss : 0.4653683805812461\n",
      "iterations 861 accuracy : 0.8220961982776728  loss : 0.46529170889496785\n",
      "iterations 862 accuracy : 0.8225162780928377  loss : 0.4652146660128317\n",
      "iterations 863 accuracy : 0.8229363579080026  loss : 0.46513691848968214\n",
      "iterations 864 accuracy : 0.8229363579080026  loss : 0.46505989326181607\n",
      "iterations 865 accuracy : 0.8233564377231674  loss : 0.46498414176612657\n",
      "iterations 866 accuracy : 0.8233564377231674  loss : 0.46490757151508494\n",
      "iterations 867 accuracy : 0.8233564377231674  loss : 0.46483014237876397\n",
      "iterations 868 accuracy : 0.8233564377231674  loss : 0.4647546634785572\n",
      "iterations 869 accuracy : 0.8235664776307499  loss : 0.46467989075946214\n",
      "iterations 870 accuracy : 0.8239865574459148  loss : 0.4646029510963902\n",
      "iterations 871 accuracy : 0.8237765175383323  loss : 0.46452703215218694\n",
      "iterations 872 accuracy : 0.8237765175383323  loss : 0.46444997294191676\n",
      "iterations 873 accuracy : 0.8237765175383323  loss : 0.46437340933309545\n",
      "iterations 874 accuracy : 0.8237765175383323  loss : 0.46429553161958415\n",
      "iterations 875 accuracy : 0.8237765175383323  loss : 0.4642175051296568\n",
      "iterations 876 accuracy : 0.8237765175383323  loss : 0.4641413372927654\n",
      "iterations 877 accuracy : 0.8237765175383323  loss : 0.4640639461609202\n",
      "iterations 878 accuracy : 0.8241965973534972  loss : 0.46398693726674894\n",
      "iterations 879 accuracy : 0.8244066372610797  loss : 0.4639114141906475\n",
      "iterations 880 accuracy : 0.824616677168662  loss : 0.463835817261318\n",
      "iterations 881 accuracy : 0.824616677168662  loss : 0.46375980220452345\n",
      "iterations 882 accuracy : 0.824616677168662  loss : 0.4636837239000645\n",
      "iterations 883 accuracy : 0.824616677168662  loss : 0.46360874576633504\n",
      "iterations 884 accuracy : 0.824616677168662  loss : 0.46353370923307635\n",
      "iterations 885 accuracy : 0.824616677168662  loss : 0.4634574170151638\n",
      "iterations 886 accuracy : 0.8244066372610797  loss : 0.46338112795578473\n",
      "iterations 887 accuracy : 0.8244066372610797  loss : 0.46330601912808567\n",
      "iterations 888 accuracy : 0.8241965973534972  loss : 0.46323026277942314\n",
      "iterations 889 accuracy : 0.824616677168662  loss : 0.46315775573228196\n",
      "iterations 890 accuracy : 0.825036756983827  loss : 0.46308236134700126\n",
      "iterations 891 accuracy : 0.8248267170762444  loss : 0.46300549672847474\n",
      "iterations 892 accuracy : 0.8248267170762444  loss : 0.46292927905346404\n",
      "iterations 893 accuracy : 0.8248267170762444  loss : 0.4628549309452414\n",
      "iterations 894 accuracy : 0.8248267170762444  loss : 0.46277872179748314\n",
      "iterations 895 accuracy : 0.8248267170762444  loss : 0.46270348063640065\n",
      "iterations 896 accuracy : 0.825036756983827  loss : 0.46262733513321724\n",
      "iterations 897 accuracy : 0.8252467968914093  loss : 0.4625517273014682\n",
      "iterations 898 accuracy : 0.8252467968914093  loss : 0.4624775893711089\n",
      "iterations 899 accuracy : 0.8252467968914093  loss : 0.462403322809691\n",
      "iterations 900 accuracy : 0.8252467968914093  loss : 0.46233021110189837\n",
      "iterations 901 accuracy : 0.8252467968914093  loss : 0.46225515369916476\n",
      "iterations 902 accuracy : 0.8252467968914093  loss : 0.46218120368285553\n",
      "iterations 903 accuracy : 0.8252467968914093  loss : 0.4621047417034275\n",
      "iterations 904 accuracy : 0.8252467968914093  loss : 0.46202989799055105\n",
      "iterations 905 accuracy : 0.8252467968914093  loss : 0.461956584341314\n",
      "iterations 906 accuracy : 0.8254568367989918  loss : 0.4618808871594526\n",
      "iterations 907 accuracy : 0.8254568367989918  loss : 0.4618051428342594\n",
      "iterations 908 accuracy : 0.8254568367989918  loss : 0.46173184293228553\n",
      "iterations 909 accuracy : 0.8254568367989918  loss : 0.46165728347156965\n",
      "iterations 910 accuracy : 0.8254568367989918  loss : 0.46158244042165736\n",
      "iterations 911 accuracy : 0.8254568367989918  loss : 0.4615077073283043\n",
      "iterations 912 accuracy : 0.8254568367989918  loss : 0.4614340264613066\n",
      "iterations 913 accuracy : 0.8254568367989918  loss : 0.4613624297277278\n",
      "iterations 914 accuracy : 0.8254568367989918  loss : 0.46128859590652654\n",
      "iterations 915 accuracy : 0.8258769166141567  loss : 0.46121335526506874\n",
      "iterations 916 accuracy : 0.8258769166141567  loss : 0.4611396381953723\n",
      "iterations 917 accuracy : 0.8258769166141567  loss : 0.46106566406408556\n",
      "iterations 918 accuracy : 0.8258769166141567  loss : 0.4609908048238665\n",
      "iterations 919 accuracy : 0.8258769166141567  loss : 0.46091850817832314\n",
      "iterations 920 accuracy : 0.8262969964293215  loss : 0.46084604789408035\n",
      "iterations 921 accuracy : 0.8262969964293215  loss : 0.46077314640557443\n",
      "iterations 922 accuracy : 0.8267170762444864  loss : 0.46069879548657067\n",
      "iterations 923 accuracy : 0.8262969964293215  loss : 0.46062510018226854\n",
      "iterations 924 accuracy : 0.8267170762444864  loss : 0.46055199109852624\n",
      "iterations 925 accuracy : 0.8269271161520689  loss : 0.46047920903586387\n",
      "iterations 926 accuracy : 0.8271371560596513  loss : 0.46040580017834004\n",
      "iterations 927 accuracy : 0.8271371560596513  loss : 0.4603337256180387\n",
      "iterations 928 accuracy : 0.8271371560596513  loss : 0.4602616323613751\n",
      "iterations 929 accuracy : 0.8271371560596513  loss : 0.46018736910726943\n",
      "iterations 930 accuracy : 0.8271371560596513  loss : 0.4601126954577261\n",
      "iterations 931 accuracy : 0.8273471959672338  loss : 0.46003985284792465\n",
      "iterations 932 accuracy : 0.8273471959672338  loss : 0.4599645423302196\n",
      "iterations 933 accuracy : 0.8275572358748162  loss : 0.4598911274467914\n",
      "iterations 934 accuracy : 0.8275572358748162  loss : 0.45981915426852027\n",
      "iterations 935 accuracy : 0.8273471959672338  loss : 0.4597459941529856\n",
      "iterations 936 accuracy : 0.8275572358748162  loss : 0.45967206491243234\n",
      "iterations 937 accuracy : 0.8275572358748162  loss : 0.4595991028345546\n",
      "iterations 938 accuracy : 0.8275572358748162  loss : 0.45952662721216175\n",
      "iterations 939 accuracy : 0.8277672757823986  loss : 0.45945473975886164\n",
      "iterations 940 accuracy : 0.8277672757823986  loss : 0.459381714637041\n",
      "iterations 941 accuracy : 0.8277672757823986  loss : 0.4593091297228849\n",
      "iterations 942 accuracy : 0.8277672757823986  loss : 0.4592367026753779\n",
      "iterations 943 accuracy : 0.8277672757823986  loss : 0.45916445204238343\n",
      "iterations 944 accuracy : 0.8277672757823986  loss : 0.4590903993547059\n",
      "iterations 945 accuracy : 0.8279773156899811  loss : 0.4590182254415066\n",
      "iterations 946 accuracy : 0.8279773156899811  loss : 0.4589458906937004\n",
      "iterations 947 accuracy : 0.8279773156899811  loss : 0.4588721091151785\n",
      "iterations 948 accuracy : 0.8279773156899811  loss : 0.45879836141385055\n",
      "iterations 949 accuracy : 0.8281873555975635  loss : 0.45872495923388207\n",
      "iterations 950 accuracy : 0.8286074354127284  loss : 0.45865186684448717\n",
      "iterations 951 accuracy : 0.8286074354127284  loss : 0.45858110857229006\n",
      "iterations 952 accuracy : 0.828397395505146  loss : 0.4585093721706734\n",
      "iterations 953 accuracy : 0.8288174753203108  loss : 0.4584365117922886\n",
      "iterations 954 accuracy : 0.8290275152278933  loss : 0.45836357711582143\n",
      "iterations 955 accuracy : 0.8290275152278933  loss : 0.4582894229710171\n",
      "iterations 956 accuracy : 0.8294475950430582  loss : 0.45821652670210344\n",
      "iterations 957 accuracy : 0.8292375551354757  loss : 0.45814480123463436\n",
      "iterations 958 accuracy : 0.8296576349506406  loss : 0.4580732564270452\n",
      "iterations 959 accuracy : 0.8296576349506406  loss : 0.4580016870275415\n",
      "iterations 960 accuracy : 0.8298676748582231  loss : 0.45792967536221596\n",
      "iterations 961 accuracy : 0.8298676748582231  loss : 0.4578567664771841\n",
      "iterations 962 accuracy : 0.8296576349506406  loss : 0.4577843730624469\n",
      "iterations 963 accuracy : 0.8296576349506406  loss : 0.4577119433748542\n",
      "iterations 964 accuracy : 0.8296576349506406  loss : 0.4576404874111561\n",
      "iterations 965 accuracy : 0.8298676748582231  loss : 0.45757014984280686\n",
      "iterations 966 accuracy : 0.8298676748582231  loss : 0.4574982511965423\n",
      "iterations 967 accuracy : 0.8300777147658055  loss : 0.457426573470957\n",
      "iterations 968 accuracy : 0.8300777147658055  loss : 0.4573538630565288\n",
      "iterations 969 accuracy : 0.8300777147658055  loss : 0.4572822668101066\n",
      "iterations 970 accuracy : 0.8300777147658055  loss : 0.457210260586936\n",
      "iterations 971 accuracy : 0.8300777147658055  loss : 0.4571393810359373\n",
      "iterations 972 accuracy : 0.8300777147658055  loss : 0.45706681879736477\n",
      "iterations 973 accuracy : 0.8300777147658055  loss : 0.4569954597146972\n",
      "iterations 974 accuracy : 0.8300777147658055  loss : 0.45692354662106044\n",
      "iterations 975 accuracy : 0.8300777147658055  loss : 0.45685272677049804\n",
      "iterations 976 accuracy : 0.8300777147658055  loss : 0.456781732861224\n",
      "iterations 977 accuracy : 0.8300777147658055  loss : 0.45671068271330856\n",
      "iterations 978 accuracy : 0.8300777147658055  loss : 0.45664094238088065\n",
      "iterations 979 accuracy : 0.8304977945809704  loss : 0.4565688661195409\n",
      "iterations 980 accuracy : 0.8304977945809704  loss : 0.45649773541222466\n",
      "iterations 981 accuracy : 0.8304977945809704  loss : 0.456426369999815\n",
      "iterations 982 accuracy : 0.8309178743961353  loss : 0.4563552387217457\n",
      "iterations 983 accuracy : 0.8313379542113002  loss : 0.45628423894300485\n",
      "iterations 984 accuracy : 0.8311279143037177  loss : 0.45621433485674007\n",
      "iterations 985 accuracy : 0.8315479941188826  loss : 0.4561432074501816\n",
      "iterations 986 accuracy : 0.831758034026465  loss : 0.4560727706065759\n",
      "iterations 987 accuracy : 0.8315479941188826  loss : 0.4560017854297\n",
      "iterations 988 accuracy : 0.831758034026465  loss : 0.45592954260740215\n",
      "iterations 989 accuracy : 0.831758034026465  loss : 0.4558590536708686\n",
      "iterations 990 accuracy : 0.831758034026465  loss : 0.45578824442949434\n",
      "iterations 991 accuracy : 0.831758034026465  loss : 0.45571871139212805\n",
      "iterations 992 accuracy : 0.831758034026465  loss : 0.45564994688897953\n",
      "iterations 993 accuracy : 0.831758034026465  loss : 0.4555803368377824\n",
      "iterations 994 accuracy : 0.831758034026465  loss : 0.4555110504627688\n",
      "iterations 995 accuracy : 0.831758034026465  loss : 0.45544142803586257\n",
      "iterations 996 accuracy : 0.831758034026465  loss : 0.45537208541005203\n",
      "iterations 997 accuracy : 0.8319680739340475  loss : 0.4553011865256765\n",
      "iterations 998 accuracy : 0.8319680739340475  loss : 0.45523080619232514\n",
      "iterations 999 accuracy : 0.8319680739340475  loss : 0.4551611843286499\n",
      "iterations 1000 accuracy : 0.8319680739340475  loss : 0.4550899247768449\n",
      "iterations 1001 accuracy : 0.8323881537492124  loss : 0.4550184195512208\n",
      "iterations 1002 accuracy : 0.8323881537492124  loss : 0.4549477382099128\n",
      "iterations 1003 accuracy : 0.8323881537492124  loss : 0.4548771203852139\n",
      "iterations 1004 accuracy : 0.8325981936567948  loss : 0.45480675827808337\n",
      "iterations 1005 accuracy : 0.8325981936567948  loss : 0.45473601813931785\n",
      "iterations 1006 accuracy : 0.8328082335643773  loss : 0.45466449835911565\n",
      "iterations 1007 accuracy : 0.8334383532871246  loss : 0.45459462995269967\n",
      "iterations 1008 accuracy : 0.8338584331022895  loss : 0.45452454241428664\n",
      "iterations 1009 accuracy : 0.8338584331022895  loss : 0.45445466336667056\n",
      "iterations 1010 accuracy : 0.8340684730098719  loss : 0.45438530542264643\n",
      "iterations 1011 accuracy : 0.8342785129174544  loss : 0.4543150169336263\n",
      "iterations 1012 accuracy : 0.8344885528250368  loss : 0.45424712434232056\n",
      "iterations 1013 accuracy : 0.8340684730098719  loss : 0.45417775220377443\n",
      "iterations 1014 accuracy : 0.8344885528250368  loss : 0.45410828161664896\n",
      "iterations 1015 accuracy : 0.8344885528250368  loss : 0.4540391655387445\n",
      "iterations 1016 accuracy : 0.8344885528250368  loss : 0.45396850844755654\n",
      "iterations 1017 accuracy : 0.8346985927326191  loss : 0.4538992883578582\n",
      "iterations 1018 accuracy : 0.8349086326402017  loss : 0.4538293726518349\n",
      "iterations 1019 accuracy : 0.8349086326402017  loss : 0.45375969271554306\n",
      "iterations 1020 accuracy : 0.8349086326402017  loss : 0.4536889618486069\n",
      "iterations 1021 accuracy : 0.8349086326402017  loss : 0.4536201714656376\n",
      "iterations 1022 accuracy : 0.8349086326402017  loss : 0.4535517285191675\n",
      "iterations 1023 accuracy : 0.8349086326402017  loss : 0.45348508112871755\n",
      "iterations 1024 accuracy : 0.8349086326402017  loss : 0.45341648115964567\n",
      "iterations 1025 accuracy : 0.8349086326402017  loss : 0.4533489152337579\n",
      "iterations 1026 accuracy : 0.835118672547784  loss : 0.4532787684228647\n",
      "iterations 1027 accuracy : 0.835118672547784  loss : 0.4532098543002763\n",
      "iterations 1028 accuracy : 0.835118672547784  loss : 0.453141151611647\n",
      "iterations 1029 accuracy : 0.835118672547784  loss : 0.4530729778206765\n",
      "iterations 1030 accuracy : 0.835118672547784  loss : 0.45300402512806426\n",
      "iterations 1031 accuracy : 0.8355387523629489  loss : 0.4529350309268839\n",
      "iterations 1032 accuracy : 0.8357487922705314  loss : 0.4528660146305213\n",
      "iterations 1033 accuracy : 0.8359588321781138  loss : 0.4527965273739569\n",
      "iterations 1034 accuracy : 0.8359588321781138  loss : 0.45272704803675684\n",
      "iterations 1035 accuracy : 0.8363789119932787  loss : 0.45265855412679357\n",
      "iterations 1036 accuracy : 0.8363789119932787  loss : 0.45259023002980037\n",
      "iterations 1037 accuracy : 0.8363789119932787  loss : 0.45252254880412135\n",
      "iterations 1038 accuracy : 0.8363789119932787  loss : 0.4524537615439849\n",
      "iterations 1039 accuracy : 0.8367989918084436  loss : 0.4523841462252943\n",
      "iterations 1040 accuracy : 0.8367989918084436  loss : 0.4523164621391254\n",
      "iterations 1041 accuracy : 0.8367989918084436  loss : 0.45224710363889337\n",
      "iterations 1042 accuracy : 0.8367989918084436  loss : 0.4521796484175962\n",
      "iterations 1043 accuracy : 0.8367989918084436  loss : 0.452112719931552\n",
      "iterations 1044 accuracy : 0.8367989918084436  loss : 0.45204319829686\n",
      "iterations 1045 accuracy : 0.837009031716026  loss : 0.45197320878101604\n",
      "iterations 1046 accuracy : 0.837009031716026  loss : 0.4519047589454424\n",
      "iterations 1047 accuracy : 0.8374291115311909  loss : 0.4518346070188553\n",
      "iterations 1048 accuracy : 0.8374291115311909  loss : 0.45176343365027655\n",
      "iterations 1049 accuracy : 0.8374291115311909  loss : 0.45169605350468733\n",
      "iterations 1050 accuracy : 0.8374291115311909  loss : 0.4516281537031105\n",
      "iterations 1051 accuracy : 0.8374291115311909  loss : 0.4515586840077401\n",
      "iterations 1052 accuracy : 0.8374291115311909  loss : 0.45149061072119206\n",
      "iterations 1053 accuracy : 0.8376391514387733  loss : 0.45142173476708713\n",
      "iterations 1054 accuracy : 0.8376391514387733  loss : 0.4513543974030076\n",
      "iterations 1055 accuracy : 0.8376391514387733  loss : 0.45128542963824647\n",
      "iterations 1056 accuracy : 0.8378491913463558  loss : 0.45121804335308136\n",
      "iterations 1057 accuracy : 0.8382692711615207  loss : 0.45114903079164476\n",
      "iterations 1058 accuracy : 0.8382692711615207  loss : 0.4510808915340427\n",
      "iterations 1059 accuracy : 0.8382692711615207  loss : 0.45101421842388506\n",
      "iterations 1060 accuracy : 0.8384793110691031  loss : 0.45094593806524685\n",
      "iterations 1061 accuracy : 0.8384793110691031  loss : 0.4508784571312455\n",
      "iterations 1062 accuracy : 0.8386893509766856  loss : 0.45081093803629885\n",
      "iterations 1063 accuracy : 0.838899390884268  loss : 0.4507427810198708\n",
      "iterations 1064 accuracy : 0.8391094307918504  loss : 0.45067529034996706\n",
      "iterations 1065 accuracy : 0.8391094307918504  loss : 0.4506090394553936\n",
      "iterations 1066 accuracy : 0.8391094307918504  loss : 0.4505415929321894\n",
      "iterations 1067 accuracy : 0.8393194706994329  loss : 0.45047509323778306\n",
      "iterations 1068 accuracy : 0.8393194706994329  loss : 0.4504091271433439\n",
      "iterations 1069 accuracy : 0.8393194706994329  loss : 0.4503427465997922\n",
      "iterations 1070 accuracy : 0.8393194706994329  loss : 0.45027696637026116\n",
      "iterations 1071 accuracy : 0.8393194706994329  loss : 0.4502127969181166\n",
      "iterations 1072 accuracy : 0.8393194706994329  loss : 0.45014344962148056\n",
      "iterations 1073 accuracy : 0.8393194706994329  loss : 0.45007627473348055\n",
      "iterations 1074 accuracy : 0.8395295106070153  loss : 0.450009387102565\n",
      "iterations 1075 accuracy : 0.8397395505145978  loss : 0.4499416410929069\n",
      "iterations 1076 accuracy : 0.8399495904221802  loss : 0.449873935021185\n",
      "iterations 1077 accuracy : 0.8399495904221802  loss : 0.44980745029729563\n",
      "iterations 1078 accuracy : 0.8403696702373451  loss : 0.4497396992331153\n",
      "iterations 1079 accuracy : 0.8403696702373451  loss : 0.44967200432886717\n",
      "iterations 1080 accuracy : 0.8403696702373451  loss : 0.4496046609566209\n",
      "iterations 1081 accuracy : 0.8405797101449275  loss : 0.4495374019947618\n",
      "iterations 1082 accuracy : 0.8405797101449275  loss : 0.4494718029824089\n",
      "iterations 1083 accuracy : 0.8405797101449275  loss : 0.4494054325153682\n",
      "iterations 1084 accuracy : 0.8405797101449275  loss : 0.44933848952985983\n",
      "iterations 1085 accuracy : 0.8405797101449275  loss : 0.4492708411353816\n",
      "iterations 1086 accuracy : 0.8405797101449275  loss : 0.4492046492090282\n",
      "iterations 1087 accuracy : 0.8405797101449275  loss : 0.4491375711242691\n",
      "iterations 1088 accuracy : 0.8409997899600924  loss : 0.449071123618158\n",
      "iterations 1089 accuracy : 0.8409997899600924  loss : 0.4490046114637323\n",
      "iterations 1090 accuracy : 0.8409997899600924  loss : 0.4489382934037679\n",
      "iterations 1091 accuracy : 0.8409997899600924  loss : 0.4488728836338748\n",
      "iterations 1092 accuracy : 0.8409997899600924  loss : 0.44880614573758965\n",
      "iterations 1093 accuracy : 0.8412098298676749  loss : 0.4487401748042254\n",
      "iterations 1094 accuracy : 0.8412098298676749  loss : 0.44867397751809374\n",
      "iterations 1095 accuracy : 0.8412098298676749  loss : 0.44860874597208167\n",
      "iterations 1096 accuracy : 0.8412098298676749  loss : 0.4485433362586029\n",
      "iterations 1097 accuracy : 0.8412098298676749  loss : 0.4484776039447032\n",
      "iterations 1098 accuracy : 0.8414198697752573  loss : 0.4484121326199054\n",
      "iterations 1099 accuracy : 0.8414198697752573  loss : 0.44834604097624947\n",
      "iterations 1100 accuracy : 0.8416299096828398  loss : 0.44828182474530087\n",
      "iterations 1101 accuracy : 0.8414198697752573  loss : 0.4482168712986622\n",
      "iterations 1102 accuracy : 0.8418399495904222  loss : 0.44815010841845376\n",
      "iterations 1103 accuracy : 0.8418399495904222  loss : 0.44808264126178793\n",
      "iterations 1104 accuracy : 0.8418399495904222  loss : 0.4480165447794867\n",
      "iterations 1105 accuracy : 0.8418399495904222  loss : 0.44795177024747673\n",
      "iterations 1106 accuracy : 0.8418399495904222  loss : 0.44788733758037924\n",
      "iterations 1107 accuracy : 0.8418399495904222  loss : 0.44782251234048753\n",
      "iterations 1108 accuracy : 0.8422600294055871  loss : 0.4477551827779036\n",
      "iterations 1109 accuracy : 0.8422600294055871  loss : 0.447688663168793\n",
      "iterations 1110 accuracy : 0.8422600294055871  loss : 0.44762287170442616\n",
      "iterations 1111 accuracy : 0.8422600294055871  loss : 0.447556881370733\n",
      "iterations 1112 accuracy : 0.8422600294055871  loss : 0.4474922935410294\n",
      "iterations 1113 accuracy : 0.8422600294055871  loss : 0.4474259548126859\n",
      "iterations 1114 accuracy : 0.8424700693131695  loss : 0.4473610128306361\n",
      "iterations 1115 accuracy : 0.8424700693131695  loss : 0.4472965774083074\n",
      "iterations 1116 accuracy : 0.842680109220752  loss : 0.44723132964228085\n",
      "iterations 1117 accuracy : 0.8428901491283344  loss : 0.44716449529979313\n",
      "iterations 1118 accuracy : 0.8428901491283344  loss : 0.4470999760941153\n",
      "iterations 1119 accuracy : 0.8431001890359168  loss : 0.4470342486317492\n",
      "iterations 1120 accuracy : 0.8433102289434993  loss : 0.44696937763712913\n",
      "iterations 1121 accuracy : 0.8435202688510817  loss : 0.4469023611968751\n",
      "iterations 1122 accuracy : 0.8437303087586642  loss : 0.44683901725699965\n",
      "iterations 1123 accuracy : 0.8439403486662466  loss : 0.4467737588940144\n",
      "iterations 1124 accuracy : 0.8441503885738291  loss : 0.4467083185243102\n",
      "iterations 1125 accuracy : 0.8443604284814115  loss : 0.44664062462245485\n",
      "iterations 1126 accuracy : 0.8443604284814115  loss : 0.4465764930133832\n",
      "iterations 1127 accuracy : 0.8443604284814115  loss : 0.44651130847541776\n",
      "iterations 1128 accuracy : 0.8443604284814115  loss : 0.44644613319750104\n",
      "iterations 1129 accuracy : 0.8443604284814115  loss : 0.4463825387774622\n",
      "iterations 1130 accuracy : 0.8443604284814115  loss : 0.4463184779713035\n",
      "iterations 1131 accuracy : 0.8443604284814115  loss : 0.44625434021700944\n",
      "iterations 1132 accuracy : 0.8447805082965764  loss : 0.44618851603389365\n",
      "iterations 1133 accuracy : 0.8449905482041588  loss : 0.44612453260365387\n",
      "iterations 1134 accuracy : 0.8449905482041588  loss : 0.44605889182019365\n",
      "iterations 1135 accuracy : 0.8452005881117413  loss : 0.4459935760103181\n",
      "iterations 1136 accuracy : 0.8452005881117413  loss : 0.4459297437726397\n",
      "iterations 1137 accuracy : 0.8452005881117413  loss : 0.4458646139821778\n",
      "iterations 1138 accuracy : 0.8454106280193237  loss : 0.4458005931975056\n",
      "iterations 1139 accuracy : 0.8454106280193237  loss : 0.4457369754383986\n",
      "iterations 1140 accuracy : 0.8464608275572358  loss : 0.44567088064143967\n",
      "iterations 1141 accuracy : 0.847511027095148  loss : 0.4456058124538117\n",
      "iterations 1142 accuracy : 0.847511027095148  loss : 0.4455402723301177\n",
      "iterations 1143 accuracy : 0.8477210670027305  loss : 0.4454770181654162\n",
      "iterations 1144 accuracy : 0.8477210670027305  loss : 0.44541340286780695\n",
      "iterations 1145 accuracy : 0.8477210670027305  loss : 0.4453504083819865\n",
      "iterations 1146 accuracy : 0.8477210670027305  loss : 0.4452850041042487\n",
      "iterations 1147 accuracy : 0.8477210670027305  loss : 0.44522159342874007\n",
      "iterations 1148 accuracy : 0.8477210670027305  loss : 0.44515704907086284\n",
      "iterations 1149 accuracy : 0.8477210670027305  loss : 0.4450914088604203\n",
      "iterations 1150 accuracy : 0.8477210670027305  loss : 0.44502893455016357\n",
      "iterations 1151 accuracy : 0.8477210670027305  loss : 0.44496380763009347\n",
      "iterations 1152 accuracy : 0.8477210670027305  loss : 0.44489935364302224\n",
      "iterations 1153 accuracy : 0.8477210670027305  loss : 0.4448345632032117\n",
      "iterations 1154 accuracy : 0.8477210670027305  loss : 0.44476930856907443\n",
      "iterations 1155 accuracy : 0.8477210670027305  loss : 0.4447044103447356\n",
      "iterations 1156 accuracy : 0.8479311069103129  loss : 0.4446411062353245\n",
      "iterations 1157 accuracy : 0.8479311069103129  loss : 0.4445784840506523\n",
      "iterations 1158 accuracy : 0.8481411468178954  loss : 0.4445139856316399\n",
      "iterations 1159 accuracy : 0.8483511867254778  loss : 0.44445021943344126\n",
      "iterations 1160 accuracy : 0.8483511867254778  loss : 0.4443869105262545\n",
      "iterations 1161 accuracy : 0.8483511867254778  loss : 0.44432319600366266\n",
      "iterations 1162 accuracy : 0.8483511867254778  loss : 0.4442604235586724\n",
      "iterations 1163 accuracy : 0.8485612266330603  loss : 0.4441966367971597\n",
      "iterations 1164 accuracy : 0.8487712665406427  loss : 0.44413341948841684\n",
      "iterations 1165 accuracy : 0.8487712665406427  loss : 0.4440704544396644\n",
      "iterations 1166 accuracy : 0.8489813064482251  loss : 0.44400758994462725\n",
      "iterations 1167 accuracy : 0.8491913463558076  loss : 0.44394578400574636\n",
      "iterations 1168 accuracy : 0.84940138626339  loss : 0.4438817115358906\n",
      "iterations 1169 accuracy : 0.8498214660785549  loss : 0.4438184447643067\n",
      "iterations 1170 accuracy : 0.8498214660785549  loss : 0.44375577660517956\n",
      "iterations 1171 accuracy : 0.8498214660785549  loss : 0.4436935303804518\n",
      "iterations 1172 accuracy : 0.8502415458937198  loss : 0.4436300297421451\n",
      "iterations 1173 accuracy : 0.8504515858013022  loss : 0.4435671052671722\n",
      "iterations 1174 accuracy : 0.8504515858013022  loss : 0.44350374815383103\n",
      "iterations 1175 accuracy : 0.8508716656164671  loss : 0.44344082410146185\n",
      "iterations 1176 accuracy : 0.8506616257088847  loss : 0.44337817161528664\n",
      "iterations 1177 accuracy : 0.8508716656164671  loss : 0.44331519961127064\n",
      "iterations 1178 accuracy : 0.8508716656164671  loss : 0.44325316844188434\n",
      "iterations 1179 accuracy : 0.8515017853392145  loss : 0.4431895410496648\n",
      "iterations 1180 accuracy : 0.8515017853392145  loss : 0.4431285068543095\n",
      "iterations 1181 accuracy : 0.8515017853392145  loss : 0.4430649474482572\n",
      "iterations 1182 accuracy : 0.8515017853392145  loss : 0.4430029182442449\n",
      "iterations 1183 accuracy : 0.8515017853392145  loss : 0.442939495035012\n",
      "iterations 1184 accuracy : 0.8515017853392145  loss : 0.44287794346195397\n",
      "iterations 1185 accuracy : 0.8515017853392145  loss : 0.44281569200062665\n",
      "iterations 1186 accuracy : 0.8515017853392145  loss : 0.44275281240195213\n",
      "iterations 1187 accuracy : 0.8515017853392145  loss : 0.4426900245938395\n",
      "iterations 1188 accuracy : 0.8515017853392145  loss : 0.44262902424848066\n",
      "iterations 1189 accuracy : 0.8515017853392145  loss : 0.44256528286951435\n",
      "iterations 1190 accuracy : 0.8515017853392145  loss : 0.44250212448636084\n",
      "iterations 1191 accuracy : 0.8515017853392145  loss : 0.4424378064031837\n",
      "iterations 1192 accuracy : 0.8515017853392145  loss : 0.442375672607458\n",
      "iterations 1193 accuracy : 0.8517118252467969  loss : 0.4423141850820303\n",
      "iterations 1194 accuracy : 0.8519218651543793  loss : 0.44225151507370464\n",
      "iterations 1195 accuracy : 0.8523419449695442  loss : 0.44218875988023765\n",
      "iterations 1196 accuracy : 0.8523419449695442  loss : 0.4421265784962418\n",
      "iterations 1197 accuracy : 0.8523419449695442  loss : 0.4420637850854539\n",
      "iterations 1198 accuracy : 0.8523419449695442  loss : 0.44200038027687144\n",
      "iterations 1199 accuracy : 0.8523419449695442  loss : 0.44193839987253264\n",
      "iterations 1200 accuracy : 0.8527620247847091  loss : 0.44187762356604227\n",
      "iterations 1201 accuracy : 0.8527620247847091  loss : 0.4418151589301592\n",
      "iterations 1202 accuracy : 0.8527620247847091  loss : 0.44175378546365257\n",
      "iterations 1203 accuracy : 0.8529720646922916  loss : 0.44169164655901927\n",
      "iterations 1204 accuracy : 0.853182104599874  loss : 0.44163049468096505\n",
      "iterations 1205 accuracy : 0.8533921445074564  loss : 0.4415700609128966\n",
      "iterations 1206 accuracy : 0.8533921445074564  loss : 0.4415094798881672\n",
      "iterations 1207 accuracy : 0.8533921445074564  loss : 0.44144620139420804\n",
      "iterations 1208 accuracy : 0.8533921445074564  loss : 0.44138458316823176\n",
      "iterations 1209 accuracy : 0.8538122243226213  loss : 0.4413229886389837\n",
      "iterations 1210 accuracy : 0.8540222642302038  loss : 0.4412615906661038\n",
      "iterations 1211 accuracy : 0.8540222642302038  loss : 0.44120166309989434\n",
      "iterations 1212 accuracy : 0.8540222642302038  loss : 0.44114002611959974\n",
      "iterations 1213 accuracy : 0.8540222642302038  loss : 0.44107880079969364\n",
      "iterations 1214 accuracy : 0.8540222642302038  loss : 0.44101768509178174\n",
      "iterations 1215 accuracy : 0.8540222642302038  loss : 0.4409573170277818\n",
      "iterations 1216 accuracy : 0.8540222642302038  loss : 0.44089689515953406\n",
      "iterations 1217 accuracy : 0.8540222642302038  loss : 0.44083631705295234\n",
      "iterations 1218 accuracy : 0.8540222642302038  loss : 0.4407761090797526\n",
      "iterations 1219 accuracy : 0.8540222642302038  loss : 0.44071609610128926\n",
      "iterations 1220 accuracy : 0.8544423440453687  loss : 0.4406545630482388\n",
      "iterations 1221 accuracy : 0.8548624238605335  loss : 0.44059262710384856\n",
      "iterations 1222 accuracy : 0.855072463768116  loss : 0.44053160304661904\n",
      "iterations 1223 accuracy : 0.8552825036756984  loss : 0.44047106713837403\n",
      "iterations 1224 accuracy : 0.8552825036756984  loss : 0.4404092691164393\n",
      "iterations 1225 accuracy : 0.8554925435832809  loss : 0.4403473659345019\n",
      "iterations 1226 accuracy : 0.8554925435832809  loss : 0.44028671982974815\n",
      "iterations 1227 accuracy : 0.855072463768116  loss : 0.4402266453318157\n",
      "iterations 1228 accuracy : 0.8552825036756984  loss : 0.44016642395188393\n",
      "iterations 1229 accuracy : 0.8554925435832809  loss : 0.4401051293215108\n",
      "iterations 1230 accuracy : 0.8554925435832809  loss : 0.44004512921998673\n",
      "iterations 1231 accuracy : 0.8554925435832809  loss : 0.43998502138537204\n",
      "iterations 1232 accuracy : 0.8554925435832809  loss : 0.4399239666682714\n",
      "iterations 1233 accuracy : 0.8554925435832809  loss : 0.43986279378102966\n",
      "iterations 1234 accuracy : 0.8554925435832809  loss : 0.4398037832475859\n",
      "iterations 1235 accuracy : 0.8554925435832809  loss : 0.4397440132885749\n",
      "iterations 1236 accuracy : 0.8557025834908633  loss : 0.43968260715840485\n",
      "iterations 1237 accuracy : 0.8557025834908633  loss : 0.43962065106647386\n",
      "iterations 1238 accuracy : 0.8561226633060282  loss : 0.4395606994681603\n",
      "iterations 1239 accuracy : 0.8561226633060282  loss : 0.4394986186137278\n",
      "iterations 1240 accuracy : 0.8561226633060282  loss : 0.43943784758205967\n",
      "iterations 1241 accuracy : 0.8561226633060282  loss : 0.43937702354300767\n",
      "iterations 1242 accuracy : 0.8561226633060282  loss : 0.4393168303316558\n",
      "iterations 1243 accuracy : 0.8561226633060282  loss : 0.4392566590684216\n",
      "iterations 1244 accuracy : 0.8563327032136105  loss : 0.43919633769069927\n",
      "iterations 1245 accuracy : 0.8563327032136105  loss : 0.439136618982983\n",
      "iterations 1246 accuracy : 0.856542743121193  loss : 0.4390757509781793\n",
      "iterations 1247 accuracy : 0.856962822936358  loss : 0.4390156237825372\n",
      "iterations 1248 accuracy : 0.856962822936358  loss : 0.43895731886699396\n",
      "iterations 1249 accuracy : 0.856962822936358  loss : 0.43889689736089\n",
      "iterations 1250 accuracy : 0.856962822936358  loss : 0.4388370065658626\n",
      "iterations 1251 accuracy : 0.8575929426591052  loss : 0.4387773313360713\n",
      "iterations 1252 accuracy : 0.8575929426591052  loss : 0.4387171992752087\n",
      "iterations 1253 accuracy : 0.8575929426591052  loss : 0.4386592003658879\n",
      "iterations 1254 accuracy : 0.8575929426591052  loss : 0.4386013312142448\n",
      "iterations 1255 accuracy : 0.8575929426591052  loss : 0.43854221960283263\n",
      "iterations 1256 accuracy : 0.8575929426591052  loss : 0.4384814861316906\n",
      "iterations 1257 accuracy : 0.8575929426591052  loss : 0.4384226098157184\n",
      "iterations 1258 accuracy : 0.8575929426591052  loss : 0.43836313211868866\n",
      "iterations 1259 accuracy : 0.8580130224742701  loss : 0.43830086227662474\n",
      "iterations 1260 accuracy : 0.8582230623818525  loss : 0.43824152948539236\n",
      "iterations 1261 accuracy : 0.8582230623818525  loss : 0.438181664066618\n",
      "iterations 1262 accuracy : 0.8582230623818525  loss : 0.4381214768154844\n",
      "iterations 1263 accuracy : 0.8582230623818525  loss : 0.4380613003664263\n",
      "iterations 1264 accuracy : 0.858433102289435  loss : 0.4380017935817836\n",
      "iterations 1265 accuracy : 0.8586431421970174  loss : 0.4379420082388354\n",
      "iterations 1266 accuracy : 0.8588531821045998  loss : 0.437881194381865\n",
      "iterations 1267 accuracy : 0.8590632220121823  loss : 0.4378212636030534\n",
      "iterations 1268 accuracy : 0.8596933417349296  loss : 0.4377627040906026\n",
      "iterations 1269 accuracy : 0.8590632220121823  loss : 0.4377048239060259\n",
      "iterations 1270 accuracy : 0.8590632220121823  loss : 0.43764597702508684\n",
      "iterations 1271 accuracy : 0.8596933417349296  loss : 0.43758723822555257\n",
      "iterations 1272 accuracy : 0.8596933417349296  loss : 0.4375282352552375\n",
      "iterations 1273 accuracy : 0.8596933417349296  loss : 0.4374685588158455\n",
      "iterations 1274 accuracy : 0.8596933417349296  loss : 0.4374095642642476\n",
      "iterations 1275 accuracy : 0.8596933417349296  loss : 0.43735090869525917\n",
      "iterations 1276 accuracy : 0.8596933417349296  loss : 0.4372907307275771\n",
      "iterations 1277 accuracy : 0.8599033816425121  loss : 0.4372303644589821\n",
      "iterations 1278 accuracy : 0.8599033816425121  loss : 0.437171758566465\n",
      "iterations 1279 accuracy : 0.8599033816425121  loss : 0.43711235570069695\n",
      "iterations 1280 accuracy : 0.8601134215500945  loss : 0.4370538638180973\n",
      "iterations 1281 accuracy : 0.8601134215500945  loss : 0.43699432866446436\n",
      "iterations 1282 accuracy : 0.8605335013652594  loss : 0.43693469433020116\n",
      "iterations 1283 accuracy : 0.8607435412728418  loss : 0.4368764065381739\n",
      "iterations 1284 accuracy : 0.8605335013652594  loss : 0.4368180853224273\n",
      "iterations 1285 accuracy : 0.8609535811804243  loss : 0.4367594855199292\n",
      "iterations 1286 accuracy : 0.8611636210880067  loss : 0.43670157350051353\n",
      "iterations 1287 accuracy : 0.8611636210880067  loss : 0.43664280912316694\n",
      "iterations 1288 accuracy : 0.8611636210880067  loss : 0.43658408612683597\n",
      "iterations 1289 accuracy : 0.8611636210880067  loss : 0.43652392637863824\n",
      "iterations 1290 accuracy : 0.8611636210880067  loss : 0.4364647038956201\n",
      "iterations 1291 accuracy : 0.8611636210880067  loss : 0.4364055424286409\n",
      "iterations 1292 accuracy : 0.8613736609955892  loss : 0.43634722750932875\n",
      "iterations 1293 accuracy : 0.8615837009031716  loss : 0.43628948941586815\n",
      "iterations 1294 accuracy : 0.861793740810754  loss : 0.43623073342705054\n",
      "iterations 1295 accuracy : 0.8620037807183365  loss : 0.43617169752357837\n",
      "iterations 1296 accuracy : 0.8620037807183365  loss : 0.43611309771344275\n",
      "iterations 1297 accuracy : 0.8620037807183365  loss : 0.43605422876328986\n",
      "iterations 1298 accuracy : 0.8622138206259189  loss : 0.4359957943141348\n",
      "iterations 1299 accuracy : 0.8622138206259189  loss : 0.4359374596418299\n",
      "iterations 1300 accuracy : 0.8622138206259189  loss : 0.4358781031649748\n",
      "iterations 1301 accuracy : 0.8622138206259189  loss : 0.43582134240126513\n",
      "iterations 1302 accuracy : 0.8622138206259189  loss : 0.4357629229091542\n",
      "iterations 1303 accuracy : 0.8624238605335014  loss : 0.4357032787686193\n",
      "iterations 1304 accuracy : 0.8624238605335014  loss : 0.4356442286385969\n",
      "iterations 1305 accuracy : 0.8624238605335014  loss : 0.4355864295736498\n",
      "iterations 1306 accuracy : 0.8624238605335014  loss : 0.43552962268975765\n",
      "iterations 1307 accuracy : 0.8624238605335014  loss : 0.43547142747906015\n",
      "iterations 1308 accuracy : 0.8624238605335014  loss : 0.43541298240634524\n",
      "iterations 1309 accuracy : 0.8628439403486663  loss : 0.43535418030014467\n",
      "iterations 1310 accuracy : 0.8628439403486663  loss : 0.4352951580514684\n",
      "iterations 1311 accuracy : 0.8628439403486663  loss : 0.4352363218735928\n",
      "iterations 1312 accuracy : 0.8630539802562487  loss : 0.43517806993765945\n",
      "iterations 1313 accuracy : 0.8630539802562487  loss : 0.43511957300050313\n",
      "iterations 1314 accuracy : 0.8630539802562487  loss : 0.43506206516144125\n",
      "iterations 1315 accuracy : 0.8630539802562487  loss : 0.4350047014227475\n",
      "iterations 1316 accuracy : 0.8630539802562487  loss : 0.43494676146244954\n",
      "iterations 1317 accuracy : 0.8630539802562487  loss : 0.43488939740577576\n",
      "iterations 1318 accuracy : 0.8632640201638311  loss : 0.4348328243094094\n",
      "iterations 1319 accuracy : 0.8632640201638311  loss : 0.4347755484478805\n",
      "iterations 1320 accuracy : 0.863684099978996  loss : 0.4347177743657726\n",
      "iterations 1321 accuracy : 0.863684099978996  loss : 0.4346603913475415\n",
      "iterations 1322 accuracy : 0.863684099978996  loss : 0.4346031557212865\n",
      "iterations 1323 accuracy : 0.8632640201638311  loss : 0.43454605938803337\n",
      "iterations 1324 accuracy : 0.8632640201638311  loss : 0.43448921595360884\n",
      "iterations 1325 accuracy : 0.8632640201638311  loss : 0.4344308550935229\n",
      "iterations 1326 accuracy : 0.863684099978996  loss : 0.4343717385078186\n",
      "iterations 1327 accuracy : 0.863684099978996  loss : 0.43431524414571643\n",
      "iterations 1328 accuracy : 0.863684099978996  loss : 0.43425876393724705\n",
      "iterations 1329 accuracy : 0.863684099978996  loss : 0.4342023467021036\n",
      "iterations 1330 accuracy : 0.8638941398865785  loss : 0.4341436646113046\n",
      "iterations 1331 accuracy : 0.8641041797941609  loss : 0.4340866719693904\n",
      "iterations 1332 accuracy : 0.8638941398865785  loss : 0.4340296456690429\n",
      "iterations 1333 accuracy : 0.8638941398865785  loss : 0.43397179389661344\n",
      "iterations 1334 accuracy : 0.8638941398865785  loss : 0.433915454810289\n",
      "iterations 1335 accuracy : 0.8641041797941609  loss : 0.4338583349016109\n",
      "iterations 1336 accuracy : 0.8643142197017434  loss : 0.4338011193288983\n",
      "iterations 1337 accuracy : 0.8643142197017434  loss : 0.43374406938777765\n",
      "iterations 1338 accuracy : 0.8643142197017434  loss : 0.4336863251145797\n",
      "iterations 1339 accuracy : 0.8645242596093258  loss : 0.43362769080255775\n",
      "iterations 1340 accuracy : 0.8645242596093258  loss : 0.433570973747924\n",
      "iterations 1341 accuracy : 0.8647342995169082  loss : 0.4335148432816065\n",
      "iterations 1342 accuracy : 0.8647342995169082  loss : 0.4334562866746656\n",
      "iterations 1343 accuracy : 0.8647342995169082  loss : 0.4333995213218112\n",
      "iterations 1344 accuracy : 0.8649443394244907  loss : 0.43334106064073313\n",
      "iterations 1345 accuracy : 0.8649443394244907  loss : 0.43328385940407965\n",
      "iterations 1346 accuracy : 0.8649443394244907  loss : 0.433227182944681\n",
      "iterations 1347 accuracy : 0.8649443394244907  loss : 0.4331706322706807\n",
      "iterations 1348 accuracy : 0.8651543793320731  loss : 0.43311433727852294\n",
      "iterations 1349 accuracy : 0.8651543793320731  loss : 0.43305940786141844\n",
      "iterations 1350 accuracy : 0.865574459147238  loss : 0.4330024854658789\n",
      "iterations 1351 accuracy : 0.865574459147238  loss : 0.43294782008497185\n",
      "iterations 1352 accuracy : 0.865574459147238  loss : 0.43289115788543175\n",
      "iterations 1353 accuracy : 0.8653644192396556  loss : 0.4328347298266821\n",
      "iterations 1354 accuracy : 0.865574459147238  loss : 0.4327779588361053\n",
      "iterations 1355 accuracy : 0.865574459147238  loss : 0.4327218367484837\n",
      "iterations 1356 accuracy : 0.865574459147238  loss : 0.4326656946707744\n",
      "iterations 1357 accuracy : 0.865574459147238  loss : 0.4326101572843536\n",
      "iterations 1358 accuracy : 0.8657844990548205  loss : 0.43255250088437197\n",
      "iterations 1359 accuracy : 0.8657844990548205  loss : 0.4324973300102877\n",
      "iterations 1360 accuracy : 0.865574459147238  loss : 0.4324415947172623\n",
      "iterations 1361 accuracy : 0.8657844990548205  loss : 0.432384742501788\n",
      "iterations 1362 accuracy : 0.8657844990548205  loss : 0.43232732924631845\n",
      "iterations 1363 accuracy : 0.8657844990548205  loss : 0.4322711177148686\n",
      "iterations 1364 accuracy : 0.8657844990548205  loss : 0.4322150259816398\n",
      "iterations 1365 accuracy : 0.8657844990548205  loss : 0.432160047344775\n",
      "iterations 1366 accuracy : 0.8657844990548205  loss : 0.43210486289404876\n",
      "iterations 1367 accuracy : 0.8662045788699853  loss : 0.4320475873020547\n",
      "iterations 1368 accuracy : 0.8668346985927327  loss : 0.43199147362417245\n",
      "iterations 1369 accuracy : 0.8668346985927327  loss : 0.4319352265824665\n",
      "iterations 1370 accuracy : 0.8668346985927327  loss : 0.43187961432152844\n",
      "iterations 1371 accuracy : 0.8668346985927327  loss : 0.43182354884184126\n",
      "iterations 1372 accuracy : 0.867044738500315  loss : 0.4317680429918519\n",
      "iterations 1373 accuracy : 0.867044738500315  loss : 0.43171208031865677\n",
      "iterations 1374 accuracy : 0.867044738500315  loss : 0.43165637697084747\n",
      "iterations 1375 accuracy : 0.867044738500315  loss : 0.43159960562873584\n",
      "iterations 1376 accuracy : 0.867044738500315  loss : 0.4315449275725788\n",
      "iterations 1377 accuracy : 0.867044738500315  loss : 0.4314895596347217\n",
      "iterations 1378 accuracy : 0.867044738500315  loss : 0.4314340515240108\n",
      "iterations 1379 accuracy : 0.8672547784078976  loss : 0.43137835730253604\n",
      "iterations 1380 accuracy : 0.86746481831548  loss : 0.4313208290696398\n",
      "iterations 1381 accuracy : 0.86746481831548  loss : 0.4312669525497363\n",
      "iterations 1382 accuracy : 0.8678848981306448  loss : 0.43121095696060957\n",
      "iterations 1383 accuracy : 0.8678848981306448  loss : 0.43115633095044165\n",
      "iterations 1384 accuracy : 0.8678848981306448  loss : 0.43110031993730424\n",
      "iterations 1385 accuracy : 0.8678848981306448  loss : 0.4310445775136328\n",
      "iterations 1386 accuracy : 0.8680949380382272  loss : 0.43098786684665824\n",
      "iterations 1387 accuracy : 0.8680949380382272  loss : 0.4309321466151327\n",
      "iterations 1388 accuracy : 0.8683049779458097  loss : 0.4308766765933744\n",
      "iterations 1389 accuracy : 0.8683049779458097  loss : 0.43082242526619713\n",
      "iterations 1390 accuracy : 0.8687250577609746  loss : 0.4307649266336232\n",
      "iterations 1391 accuracy : 0.8687250577609746  loss : 0.43070896718283136\n",
      "iterations 1392 accuracy : 0.868935097668557  loss : 0.4306533927137217\n",
      "iterations 1393 accuracy : 0.868935097668557  loss : 0.43059773379315097\n",
      "iterations 1394 accuracy : 0.8691451375761394  loss : 0.4305428804143936\n",
      "iterations 1395 accuracy : 0.8695652173913043  loss : 0.4304875355487853\n",
      "iterations 1396 accuracy : 0.8691451375761394  loss : 0.4304340706008703\n",
      "iterations 1397 accuracy : 0.8693551774837219  loss : 0.4303788125982444\n",
      "iterations 1398 accuracy : 0.8693551774837219  loss : 0.4303242854226756\n",
      "iterations 1399 accuracy : 0.8695652173913043  loss : 0.430268253739693\n",
      "iterations 1400 accuracy : 0.8695652173913043  loss : 0.43021279517096334\n",
      "iterations 1401 accuracy : 0.8697752572988868  loss : 0.4301584913005564\n",
      "iterations 1402 accuracy : 0.8697752572988868  loss : 0.4301030973837544\n",
      "iterations 1403 accuracy : 0.8697752572988868  loss : 0.4300469940546672\n",
      "iterations 1404 accuracy : 0.8697752572988868  loss : 0.4299913199588662\n",
      "iterations 1405 accuracy : 0.8699852972064692  loss : 0.42993518919504387\n",
      "iterations 1406 accuracy : 0.8699852972064692  loss : 0.4298805656963572\n",
      "iterations 1407 accuracy : 0.8699852972064692  loss : 0.4298256593186894\n",
      "iterations 1408 accuracy : 0.8699852972064692  loss : 0.4297703797664849\n",
      "iterations 1409 accuracy : 0.8699852972064692  loss : 0.4297151936733424\n",
      "iterations 1410 accuracy : 0.8699852972064692  loss : 0.4296608697955007\n",
      "iterations 1411 accuracy : 0.8699852972064692  loss : 0.42960559232011203\n",
      "iterations 1412 accuracy : 0.8699852972064692  loss : 0.42954989510509073\n",
      "iterations 1413 accuracy : 0.8699852972064692  loss : 0.4294955089592931\n",
      "iterations 1414 accuracy : 0.8699852972064692  loss : 0.4294426003904227\n",
      "iterations 1415 accuracy : 0.8699852972064692  loss : 0.4293874785768528\n",
      "iterations 1416 accuracy : 0.8699852972064692  loss : 0.42933382758774224\n",
      "iterations 1417 accuracy : 0.8704053770216341  loss : 0.42927887133268483\n",
      "iterations 1418 accuracy : 0.8704053770216341  loss : 0.4292241841235106\n",
      "iterations 1419 accuracy : 0.8704053770216341  loss : 0.4291709862979153\n",
      "iterations 1420 accuracy : 0.8704053770216341  loss : 0.4291162545513904\n",
      "iterations 1421 accuracy : 0.8704053770216341  loss : 0.4290623807132389\n",
      "iterations 1422 accuracy : 0.8704053770216341  loss : 0.42900833310576814\n",
      "iterations 1423 accuracy : 0.8704053770216341  loss : 0.4289534807125424\n",
      "iterations 1424 accuracy : 0.8706154169292165  loss : 0.42889904482493163\n",
      "iterations 1425 accuracy : 0.8704053770216341  loss : 0.42884554403614117\n",
      "iterations 1426 accuracy : 0.8704053770216341  loss : 0.4287914233962667\n",
      "iterations 1427 accuracy : 0.8704053770216341  loss : 0.42873622420450436\n",
      "iterations 1428 accuracy : 0.8704053770216341  loss : 0.42868196293422106\n",
      "iterations 1429 accuracy : 0.8706154169292165  loss : 0.4286264229048393\n",
      "iterations 1430 accuracy : 0.870825456836799  loss : 0.4285732021515376\n",
      "iterations 1431 accuracy : 0.870825456836799  loss : 0.4285189093882408\n",
      "iterations 1432 accuracy : 0.870825456836799  loss : 0.4284661947892157\n",
      "iterations 1433 accuracy : 0.8710354967443814  loss : 0.42841113083345594\n",
      "iterations 1434 accuracy : 0.8710354967443814  loss : 0.4283565730826325\n",
      "iterations 1435 accuracy : 0.8710354967443814  loss : 0.42830384458119997\n",
      "iterations 1436 accuracy : 0.8710354967443814  loss : 0.4282509217549612\n",
      "iterations 1437 accuracy : 0.8710354967443814  loss : 0.42819724560705624\n",
      "iterations 1438 accuracy : 0.8710354967443814  loss : 0.42814366269453197\n",
      "iterations 1439 accuracy : 0.8714555765595463  loss : 0.4280912155702272\n",
      "iterations 1440 accuracy : 0.8714555765595463  loss : 0.4280384691351755\n",
      "iterations 1441 accuracy : 0.8714555765595463  loss : 0.4279844467042817\n",
      "iterations 1442 accuracy : 0.8714555765595463  loss : 0.42792953737005807\n",
      "iterations 1443 accuracy : 0.8714555765595463  loss : 0.4278757891588538\n",
      "iterations 1444 accuracy : 0.8716656164671287  loss : 0.4278220880990034\n",
      "iterations 1445 accuracy : 0.8716656164671287  loss : 0.4277678888942807\n",
      "iterations 1446 accuracy : 0.8716656164671287  loss : 0.4277147482609687\n",
      "iterations 1447 accuracy : 0.8716656164671287  loss : 0.42766275898197575\n",
      "iterations 1448 accuracy : 0.8718756563747112  loss : 0.4276095792104793\n",
      "iterations 1449 accuracy : 0.8718756563747112  loss : 0.4275573868504553\n",
      "iterations 1450 accuracy : 0.8718756563747112  loss : 0.4275035679654627\n",
      "iterations 1451 accuracy : 0.8718756563747112  loss : 0.42745119544748383\n",
      "iterations 1452 accuracy : 0.8718756563747112  loss : 0.42739930366348006\n",
      "iterations 1453 accuracy : 0.8718756563747112  loss : 0.42734593302834945\n",
      "iterations 1454 accuracy : 0.8718756563747112  loss : 0.42729307761663066\n",
      "iterations 1455 accuracy : 0.8718756563747112  loss : 0.42723998813839004\n",
      "iterations 1456 accuracy : 0.8718756563747112  loss : 0.4271869212766212\n",
      "iterations 1457 accuracy : 0.8718756563747112  loss : 0.4271333734988953\n",
      "iterations 1458 accuracy : 0.8718756563747112  loss : 0.42708083204546277\n",
      "iterations 1459 accuracy : 0.8718756563747112  loss : 0.42702734731487374\n",
      "iterations 1460 accuracy : 0.8718756563747112  loss : 0.4269740442849249\n",
      "iterations 1461 accuracy : 0.8722957361898761  loss : 0.42691968493200316\n",
      "iterations 1462 accuracy : 0.8722957361898761  loss : 0.42686686326812584\n",
      "iterations 1463 accuracy : 0.8722957361898761  loss : 0.4268141948383807\n",
      "iterations 1464 accuracy : 0.8722957361898761  loss : 0.42676063959688176\n",
      "iterations 1465 accuracy : 0.8725057760974585  loss : 0.42670668881729273\n",
      "iterations 1466 accuracy : 0.872715816005041  loss : 0.42665394852094557\n",
      "iterations 1467 accuracy : 0.872715816005041  loss : 0.42660152271841173\n",
      "iterations 1468 accuracy : 0.8729258559126234  loss : 0.42654835198490315\n",
      "iterations 1469 accuracy : 0.8729258559126234  loss : 0.42649394462861634\n",
      "iterations 1470 accuracy : 0.8729258559126234  loss : 0.42644122684873903\n",
      "iterations 1471 accuracy : 0.8729258559126234  loss : 0.42638898257115315\n",
      "iterations 1472 accuracy : 0.8729258559126234  loss : 0.42633614314983276\n",
      "iterations 1473 accuracy : 0.8729258559126234  loss : 0.42628339320225184\n",
      "iterations 1474 accuracy : 0.8729258559126234  loss : 0.42623193946082566\n",
      "iterations 1475 accuracy : 0.8729258559126234  loss : 0.4261800736228031\n",
      "iterations 1476 accuracy : 0.8731358958202058  loss : 0.4261278612047675\n",
      "iterations 1477 accuracy : 0.8731358958202058  loss : 0.4260754371405704\n",
      "iterations 1478 accuracy : 0.8733459357277883  loss : 0.42602292301414774\n",
      "iterations 1479 accuracy : 0.8737660155429532  loss : 0.4259706450452744\n",
      "iterations 1480 accuracy : 0.8733459357277883  loss : 0.4259189363020551\n",
      "iterations 1481 accuracy : 0.8735559756353707  loss : 0.42586529884009\n",
      "iterations 1482 accuracy : 0.8735559756353707  loss : 0.42581388887423055\n",
      "iterations 1483 accuracy : 0.8735559756353707  loss : 0.42576257393659916\n",
      "iterations 1484 accuracy : 0.8735559756353707  loss : 0.42570958963116384\n",
      "iterations 1485 accuracy : 0.8739760554505356  loss : 0.42565785540107504\n",
      "iterations 1486 accuracy : 0.8746061751732829  loss : 0.42560521714708294\n",
      "iterations 1487 accuracy : 0.8746061751732829  loss : 0.42555204499575594\n",
      "iterations 1488 accuracy : 0.8746061751732829  loss : 0.4255004024359119\n",
      "iterations 1489 accuracy : 0.8746061751732829  loss : 0.4254468909756222\n",
      "iterations 1490 accuracy : 0.8746061751732829  loss : 0.4253945403246414\n",
      "iterations 1491 accuracy : 0.8746061751732829  loss : 0.42534310489190547\n",
      "iterations 1492 accuracy : 0.8746061751732829  loss : 0.4252910056623763\n",
      "iterations 1493 accuracy : 0.8746061751732829  loss : 0.4252400625769417\n",
      "iterations 1494 accuracy : 0.8746061751732829  loss : 0.42518829182179524\n",
      "iterations 1495 accuracy : 0.8746061751732829  loss : 0.4251369331660785\n",
      "iterations 1496 accuracy : 0.8746061751732829  loss : 0.4250846393942992\n",
      "iterations 1497 accuracy : 0.8750262549884478  loss : 0.42503209312460505\n",
      "iterations 1498 accuracy : 0.8750262549884478  loss : 0.42498033926813966\n",
      "iterations 1499 accuracy : 0.8750262549884478  loss : 0.4249286933574497\n",
      "iterations 1500 accuracy : 0.8752362948960303  loss : 0.4248754564413001\n",
      "iterations 1501 accuracy : 0.8752362948960303  loss : 0.42482338451892815\n",
      "iterations 1502 accuracy : 0.8752362948960303  loss : 0.42477187849977804\n",
      "iterations 1503 accuracy : 0.8754463348036127  loss : 0.4247209295146371\n",
      "iterations 1504 accuracy : 0.8756563747111952  loss : 0.42466807282610136\n",
      "iterations 1505 accuracy : 0.8756563747111952  loss : 0.42461610542854344\n",
      "iterations 1506 accuracy : 0.8756563747111952  loss : 0.4245647044549231\n",
      "iterations 1507 accuracy : 0.8758664146187776  loss : 0.42451243951927514\n",
      "iterations 1508 accuracy : 0.8762864944339425  loss : 0.4244592839224029\n",
      "iterations 1509 accuracy : 0.8762864944339425  loss : 0.4244083614665188\n",
      "iterations 1510 accuracy : 0.8762864944339425  loss : 0.42435603160910496\n",
      "iterations 1511 accuracy : 0.8764965343415249  loss : 0.4243047629469169\n",
      "iterations 1512 accuracy : 0.8764965343415249  loss : 0.42425221443994565\n",
      "iterations 1513 accuracy : 0.8764965343415249  loss : 0.4241992663247543\n",
      "iterations 1514 accuracy : 0.8764965343415249  loss : 0.4241488160532933\n",
      "iterations 1515 accuracy : 0.8764965343415249  loss : 0.424097314081244\n",
      "iterations 1516 accuracy : 0.8764965343415249  loss : 0.42404587749850875\n",
      "iterations 1517 accuracy : 0.8764965343415249  loss : 0.4239946485706002\n",
      "iterations 1518 accuracy : 0.8767065742491074  loss : 0.423942712605016\n",
      "iterations 1519 accuracy : 0.8767065742491074  loss : 0.42389087326262503\n",
      "iterations 1520 accuracy : 0.8767065742491074  loss : 0.4238395050641218\n",
      "iterations 1521 accuracy : 0.8767065742491074  loss : 0.42378691326634393\n",
      "iterations 1522 accuracy : 0.8767065742491074  loss : 0.4237352029121576\n",
      "iterations 1523 accuracy : 0.8767065742491074  loss : 0.4236839071466718\n",
      "iterations 1524 accuracy : 0.8767065742491074  loss : 0.4236327457150718\n",
      "iterations 1525 accuracy : 0.8769166141566898  loss : 0.4235811368882097\n",
      "iterations 1526 accuracy : 0.877546733879437  loss : 0.4235292716085424\n",
      "iterations 1527 accuracy : 0.8777567737870196  loss : 0.42347884019324755\n",
      "iterations 1528 accuracy : 0.8777567737870196  loss : 0.42342759580391554\n",
      "iterations 1529 accuracy : 0.8779668136946019  loss : 0.42337530565409787\n",
      "iterations 1530 accuracy : 0.8781768536021844  loss : 0.42332488415159225\n",
      "iterations 1531 accuracy : 0.8781768536021844  loss : 0.4232737555003577\n",
      "iterations 1532 accuracy : 0.8781768536021844  loss : 0.42322306764190554\n",
      "iterations 1533 accuracy : 0.8783868935097668  loss : 0.4231720039257907\n",
      "iterations 1534 accuracy : 0.8785969334173493  loss : 0.42312144280903347\n",
      "iterations 1535 accuracy : 0.8785969334173493  loss : 0.4230698903701887\n",
      "iterations 1536 accuracy : 0.8785969334173493  loss : 0.4230209874246089\n",
      "iterations 1537 accuracy : 0.8785969334173493  loss : 0.42296999931609686\n",
      "iterations 1538 accuracy : 0.8785969334173493  loss : 0.4229195196958332\n",
      "iterations 1539 accuracy : 0.8790170132325141  loss : 0.4228681849913912\n",
      "iterations 1540 accuracy : 0.8792270531400966  loss : 0.42281727063296953\n",
      "iterations 1541 accuracy : 0.8792270531400966  loss : 0.4227671704395\n",
      "iterations 1542 accuracy : 0.879437093047679  loss : 0.42271545312340264\n",
      "iterations 1543 accuracy : 0.879437093047679  loss : 0.42266534753427776\n",
      "iterations 1544 accuracy : 0.879437093047679  loss : 0.42261545338402773\n",
      "iterations 1545 accuracy : 0.879437093047679  loss : 0.4225664110617233\n",
      "iterations 1546 accuracy : 0.879437093047679  loss : 0.4225152792574634\n",
      "iterations 1547 accuracy : 0.879437093047679  loss : 0.42246456063271404\n",
      "iterations 1548 accuracy : 0.8798571728628439  loss : 0.4224123018320763\n",
      "iterations 1549 accuracy : 0.8798571728628439  loss : 0.42236302483536053\n",
      "iterations 1550 accuracy : 0.8798571728628439  loss : 0.42231172300746056\n",
      "iterations 1551 accuracy : 0.8798571728628439  loss : 0.4222624738585054\n",
      "iterations 1552 accuracy : 0.8798571728628439  loss : 0.4222111497589862\n",
      "iterations 1553 accuracy : 0.8798571728628439  loss : 0.42216121386753086\n",
      "iterations 1554 accuracy : 0.8798571728628439  loss : 0.42211081433688113\n",
      "iterations 1555 accuracy : 0.8802772526780088  loss : 0.4220604903446215\n",
      "iterations 1556 accuracy : 0.8802772526780088  loss : 0.4220096752109469\n",
      "iterations 1557 accuracy : 0.8802772526780088  loss : 0.4219605817112689\n",
      "iterations 1558 accuracy : 0.8802772526780088  loss : 0.42190934541121\n",
      "iterations 1559 accuracy : 0.8802772526780088  loss : 0.4218585244199453\n",
      "iterations 1560 accuracy : 0.8802772526780088  loss : 0.42180755122425356\n",
      "iterations 1561 accuracy : 0.8802772526780088  loss : 0.4217580781912489\n",
      "iterations 1562 accuracy : 0.8802772526780088  loss : 0.42170774199568034\n",
      "iterations 1563 accuracy : 0.8802772526780088  loss : 0.4216574811832283\n",
      "iterations 1564 accuracy : 0.8802772526780088  loss : 0.4216087390982481\n",
      "iterations 1565 accuracy : 0.8802772526780088  loss : 0.42155994856654466\n",
      "iterations 1566 accuracy : 0.8802772526780088  loss : 0.42151153456883855\n",
      "iterations 1567 accuracy : 0.8802772526780088  loss : 0.4214611009520692\n",
      "iterations 1568 accuracy : 0.8802772526780088  loss : 0.4214125905414967\n",
      "iterations 1569 accuracy : 0.8802772526780088  loss : 0.4213623883105981\n",
      "iterations 1570 accuracy : 0.8802772526780088  loss : 0.42131221203748237\n",
      "iterations 1571 accuracy : 0.8802772526780088  loss : 0.4212631224991653\n",
      "iterations 1572 accuracy : 0.8802772526780088  loss : 0.4212122709311166\n",
      "iterations 1573 accuracy : 0.8804872925855912  loss : 0.42116188553375006\n",
      "iterations 1574 accuracy : 0.8804872925855912  loss : 0.42111232638301255\n",
      "iterations 1575 accuracy : 0.8806973324931737  loss : 0.4210618478728012\n",
      "iterations 1576 accuracy : 0.8809073724007561  loss : 0.4210108778041057\n",
      "iterations 1577 accuracy : 0.8809073724007561  loss : 0.42096056469457793\n",
      "iterations 1578 accuracy : 0.8809073724007561  loss : 0.42091071030083976\n",
      "iterations 1579 accuracy : 0.8809073724007561  loss : 0.4208621851318713\n",
      "iterations 1580 accuracy : 0.8809073724007561  loss : 0.42081357793716656\n",
      "iterations 1581 accuracy : 0.8809073724007561  loss : 0.4207639284930317\n",
      "iterations 1582 accuracy : 0.8809073724007561  loss : 0.4207137755072978\n",
      "iterations 1583 accuracy : 0.8809073724007561  loss : 0.4206630410569452\n",
      "iterations 1584 accuracy : 0.8809073724007561  loss : 0.4206128168760435\n",
      "iterations 1585 accuracy : 0.8809073724007561  loss : 0.4205629161228685\n",
      "iterations 1586 accuracy : 0.8811174123083386  loss : 0.42051356006094925\n",
      "iterations 1587 accuracy : 0.881327452215921  loss : 0.4204638332284353\n",
      "iterations 1588 accuracy : 0.881327452215921  loss : 0.42041469207024523\n",
      "iterations 1589 accuracy : 0.8815374921235035  loss : 0.42036399294692417\n",
      "iterations 1590 accuracy : 0.8815374921235035  loss : 0.42031532586297454\n",
      "iterations 1591 accuracy : 0.8815374921235035  loss : 0.42026614599233414\n",
      "iterations 1592 accuracy : 0.8815374921235035  loss : 0.42021671923998505\n",
      "iterations 1593 accuracy : 0.8815374921235035  loss : 0.4201689664392532\n",
      "iterations 1594 accuracy : 0.8815374921235035  loss : 0.4201202668485678\n",
      "iterations 1595 accuracy : 0.8819575719386683  loss : 0.42007078417307064\n",
      "iterations 1596 accuracy : 0.8819575719386683  loss : 0.42002212061409494\n",
      "iterations 1597 accuracy : 0.8819575719386683  loss : 0.41997258893989725\n",
      "iterations 1598 accuracy : 0.8819575719386683  loss : 0.41992452767768546\n",
      "iterations 1599 accuracy : 0.8821676118462508  loss : 0.41987496559385207\n",
      "iterations 1600 accuracy : 0.8825876916614157  loss : 0.41982566495219403\n",
      "iterations 1601 accuracy : 0.8821676118462508  loss : 0.4197772160439074\n",
      "iterations 1602 accuracy : 0.8825876916614157  loss : 0.4197287053033095\n",
      "iterations 1603 accuracy : 0.8825876916614157  loss : 0.4196795943198012\n",
      "iterations 1604 accuracy : 0.8825876916614157  loss : 0.4196303265309408\n",
      "iterations 1605 accuracy : 0.8825876916614157  loss : 0.4195819966700348\n",
      "iterations 1606 accuracy : 0.8827977315689981  loss : 0.4195327067582404\n",
      "iterations 1607 accuracy : 0.8827977315689981  loss : 0.41948428177120023\n",
      "iterations 1608 accuracy : 0.8827977315689981  loss : 0.41943599508404467\n",
      "iterations 1609 accuracy : 0.8827977315689981  loss : 0.41938905208529986\n",
      "iterations 1610 accuracy : 0.8827977315689981  loss : 0.4193394541685094\n",
      "iterations 1611 accuracy : 0.8827977315689981  loss : 0.4192896938971119\n",
      "iterations 1612 accuracy : 0.8827977315689981  loss : 0.4192413327521538\n",
      "iterations 1613 accuracy : 0.8827977315689981  loss : 0.4191923548480335\n",
      "iterations 1614 accuracy : 0.8827977315689981  loss : 0.41914361067923356\n",
      "iterations 1615 accuracy : 0.8827977315689981  loss : 0.41909505222053245\n",
      "iterations 1616 accuracy : 0.8827977315689981  loss : 0.41904592710872024\n",
      "iterations 1617 accuracy : 0.8830077714765806  loss : 0.41899650375753\n",
      "iterations 1618 accuracy : 0.8830077714765806  loss : 0.4189485770450848\n",
      "iterations 1619 accuracy : 0.8830077714765806  loss : 0.4188992481228622\n",
      "iterations 1620 accuracy : 0.883217811384163  loss : 0.4188507712481776\n",
      "iterations 1621 accuracy : 0.8836378911993279  loss : 0.4188014306522574\n",
      "iterations 1622 accuracy : 0.8836378911993279  loss : 0.4187520381929471\n",
      "iterations 1623 accuracy : 0.8838479311069103  loss : 0.41870222857484674\n",
      "iterations 1624 accuracy : 0.8842680109220752  loss : 0.4186545757259889\n",
      "iterations 1625 accuracy : 0.8842680109220752  loss : 0.41860498112946537\n",
      "iterations 1626 accuracy : 0.8842680109220752  loss : 0.418557435708281\n",
      "iterations 1627 accuracy : 0.8842680109220752  loss : 0.41851009158201175\n",
      "iterations 1628 accuracy : 0.8842680109220752  loss : 0.4184623353398611\n",
      "iterations 1629 accuracy : 0.8842680109220752  loss : 0.4184144374151058\n",
      "iterations 1630 accuracy : 0.8844780508296576  loss : 0.4183663728050633\n",
      "iterations 1631 accuracy : 0.8844780508296576  loss : 0.4183197889862442\n",
      "iterations 1632 accuracy : 0.8844780508296576  loss : 0.4182719597059623\n",
      "iterations 1633 accuracy : 0.8844780508296576  loss : 0.41822237463336553\n",
      "iterations 1634 accuracy : 0.8844780508296576  loss : 0.4181753676834021\n",
      "iterations 1635 accuracy : 0.8844780508296576  loss : 0.4181277851279041\n",
      "iterations 1636 accuracy : 0.8844780508296576  loss : 0.4180791259311807\n",
      "iterations 1637 accuracy : 0.8846880907372401  loss : 0.4180311945588555\n",
      "iterations 1638 accuracy : 0.8846880907372401  loss : 0.4179833292647985\n",
      "iterations 1639 accuracy : 0.8846880907372401  loss : 0.4179353251113826\n",
      "iterations 1640 accuracy : 0.8846880907372401  loss : 0.41788759103062656\n",
      "iterations 1641 accuracy : 0.8846880907372401  loss : 0.41784043750733135\n",
      "iterations 1642 accuracy : 0.8846880907372401  loss : 0.4177917741213887\n",
      "iterations 1643 accuracy : 0.8846880907372401  loss : 0.4177446460408051\n",
      "iterations 1644 accuracy : 0.8846880907372401  loss : 0.41769717872615164\n",
      "iterations 1645 accuracy : 0.8846880907372401  loss : 0.41764929643797444\n",
      "iterations 1646 accuracy : 0.8846880907372401  loss : 0.41760131407837914\n",
      "iterations 1647 accuracy : 0.8846880907372401  loss : 0.41755317901389366\n",
      "iterations 1648 accuracy : 0.8846880907372401  loss : 0.41750639804508843\n",
      "iterations 1649 accuracy : 0.8846880907372401  loss : 0.4174593931950312\n",
      "iterations 1650 accuracy : 0.8846880907372401  loss : 0.4174119657322578\n",
      "iterations 1651 accuracy : 0.8846880907372401  loss : 0.4173632223648544\n",
      "iterations 1652 accuracy : 0.8846880907372401  loss : 0.4173163502687425\n",
      "iterations 1653 accuracy : 0.8846880907372401  loss : 0.4172704467028373\n",
      "iterations 1654 accuracy : 0.8846880907372401  loss : 0.41722273924507114\n",
      "iterations 1655 accuracy : 0.8846880907372401  loss : 0.4171755540121691\n",
      "iterations 1656 accuracy : 0.8846880907372401  loss : 0.4171292041229073\n",
      "iterations 1657 accuracy : 0.8846880907372401  loss : 0.41708178740651075\n",
      "iterations 1658 accuracy : 0.885108170552405  loss : 0.4170342482404403\n",
      "iterations 1659 accuracy : 0.8846880907372401  loss : 0.41698761153804004\n",
      "iterations 1660 accuracy : 0.8846880907372401  loss : 0.4169404204038589\n",
      "iterations 1661 accuracy : 0.885108170552405  loss : 0.4168917854540446\n",
      "iterations 1662 accuracy : 0.885108170552405  loss : 0.41684425297205285\n",
      "iterations 1663 accuracy : 0.885108170552405  loss : 0.4167965997399817\n",
      "iterations 1664 accuracy : 0.885108170552405  loss : 0.416748541875985\n",
      "iterations 1665 accuracy : 0.8853182104599874  loss : 0.41670169018452247\n",
      "iterations 1666 accuracy : 0.8853182104599874  loss : 0.41665513611370725\n",
      "iterations 1667 accuracy : 0.8853182104599874  loss : 0.4166091891956442\n",
      "iterations 1668 accuracy : 0.885108170552405  loss : 0.4165623425009706\n",
      "iterations 1669 accuracy : 0.8853182104599874  loss : 0.4165150848699311\n",
      "iterations 1670 accuracy : 0.8853182104599874  loss : 0.4164688027736962\n",
      "iterations 1671 accuracy : 0.8853182104599874  loss : 0.41642165080938015\n",
      "iterations 1672 accuracy : 0.8853182104599874  loss : 0.41637507011555835\n",
      "iterations 1673 accuracy : 0.8853182104599874  loss : 0.4163289427045714\n",
      "iterations 1674 accuracy : 0.8853182104599874  loss : 0.41628192564984884\n",
      "iterations 1675 accuracy : 0.8853182104599874  loss : 0.4162350032620182\n",
      "iterations 1676 accuracy : 0.8853182104599874  loss : 0.41618945365093357\n",
      "iterations 1677 accuracy : 0.8853182104599874  loss : 0.41614191971936215\n",
      "iterations 1678 accuracy : 0.8853182104599874  loss : 0.41609680196049303\n",
      "iterations 1679 accuracy : 0.8853182104599874  loss : 0.41604908624192577\n",
      "iterations 1680 accuracy : 0.8859483301827347  loss : 0.41600170694437\n",
      "iterations 1681 accuracy : 0.8861583700903172  loss : 0.4159548969460387\n",
      "iterations 1682 accuracy : 0.8859483301827347  loss : 0.4159085988645261\n",
      "iterations 1683 accuracy : 0.8863684099978996  loss : 0.41586127649871935\n",
      "iterations 1684 accuracy : 0.8863684099978996  loss : 0.4158151232581868\n",
      "iterations 1685 accuracy : 0.8863684099978996  loss : 0.4157678105684203\n",
      "iterations 1686 accuracy : 0.8865784499054821  loss : 0.4157220197477293\n",
      "iterations 1687 accuracy : 0.8865784499054821  loss : 0.41567558775034996\n",
      "iterations 1688 accuracy : 0.8865784499054821  loss : 0.41562869697795657\n",
      "iterations 1689 accuracy : 0.8863684099978996  loss : 0.4155835503582481\n",
      "iterations 1690 accuracy : 0.8863684099978996  loss : 0.4155361339563784\n",
      "iterations 1691 accuracy : 0.8865784499054821  loss : 0.4154890944581007\n",
      "iterations 1692 accuracy : 0.886998529720647  loss : 0.4154411441555947\n",
      "iterations 1693 accuracy : 0.886998529720647  loss : 0.41539506791361525\n",
      "iterations 1694 accuracy : 0.8874186095358118  loss : 0.4153473408351751\n",
      "iterations 1695 accuracy : 0.8874186095358118  loss : 0.41530031090844577\n",
      "iterations 1696 accuracy : 0.8874186095358118  loss : 0.41525245250551884\n",
      "iterations 1697 accuracy : 0.8874186095358118  loss : 0.4152070470688274\n",
      "iterations 1698 accuracy : 0.8874186095358118  loss : 0.41516019910926993\n",
      "iterations 1699 accuracy : 0.8878386893509767  loss : 0.4151122701447326\n",
      "iterations 1700 accuracy : 0.8878386893509767  loss : 0.41506497634875067\n",
      "iterations 1701 accuracy : 0.8880487292585592  loss : 0.41501827018332527\n",
      "iterations 1702 accuracy : 0.8878386893509767  loss : 0.4149731740913607\n",
      "iterations 1703 accuracy : 0.8878386893509767  loss : 0.41492750058658\n",
      "iterations 1704 accuracy : 0.8882587691661415  loss : 0.4148813114533579\n",
      "iterations 1705 accuracy : 0.8882587691661415  loss : 0.41483503574010716\n",
      "iterations 1706 accuracy : 0.8882587691661415  loss : 0.41478925753817086\n",
      "iterations 1707 accuracy : 0.8882587691661415  loss : 0.4147430843916512\n",
      "iterations 1708 accuracy : 0.8882587691661415  loss : 0.4146969867644709\n",
      "iterations 1709 accuracy : 0.888468809073724  loss : 0.41465179973205935\n",
      "iterations 1710 accuracy : 0.8886788489813064  loss : 0.41460520130710293\n",
      "iterations 1711 accuracy : 0.8886788489813064  loss : 0.41455944697933333\n",
      "iterations 1712 accuracy : 0.8886788489813064  loss : 0.4145130866591898\n",
      "iterations 1713 accuracy : 0.8886788489813064  loss : 0.4144676802761961\n",
      "iterations 1714 accuracy : 0.8886788489813064  loss : 0.4144223040961671\n",
      "iterations 1715 accuracy : 0.8886788489813064  loss : 0.4143771863916518\n",
      "iterations 1716 accuracy : 0.8886788489813064  loss : 0.4143316861631899\n",
      "iterations 1717 accuracy : 0.8886788489813064  loss : 0.4142861663231386\n",
      "iterations 1718 accuracy : 0.8886788489813064  loss : 0.41424017075746494\n",
      "iterations 1719 accuracy : 0.8888888888888888  loss : 0.41419485949660956\n",
      "iterations 1720 accuracy : 0.8890989287964713  loss : 0.41415084313642386\n",
      "iterations 1721 accuracy : 0.8890989287964713  loss : 0.41410527557243687\n",
      "iterations 1722 accuracy : 0.8893089687040537  loss : 0.41405904468623034\n",
      "iterations 1723 accuracy : 0.8893089687040537  loss : 0.4140151056124202\n",
      "iterations 1724 accuracy : 0.8893089687040537  loss : 0.4139687949259916\n",
      "iterations 1725 accuracy : 0.8895190086116362  loss : 0.4139223918786154\n",
      "iterations 1726 accuracy : 0.8897290485192186  loss : 0.4138754244735912\n",
      "iterations 1727 accuracy : 0.8897290485192186  loss : 0.41382909355325304\n",
      "iterations 1728 accuracy : 0.8897290485192186  loss : 0.413783509400067\n",
      "iterations 1729 accuracy : 0.8897290485192186  loss : 0.41373775282510816\n",
      "iterations 1730 accuracy : 0.8897290485192186  loss : 0.4136904283647762\n",
      "iterations 1731 accuracy : 0.8897290485192186  loss : 0.4136457324572026\n",
      "iterations 1732 accuracy : 0.8897290485192186  loss : 0.4136002298815\n",
      "iterations 1733 accuracy : 0.8897290485192186  loss : 0.41355551945220453\n",
      "iterations 1734 accuracy : 0.8897290485192186  loss : 0.4135099086781379\n",
      "iterations 1735 accuracy : 0.8897290485192186  loss : 0.4134652272011098\n",
      "iterations 1736 accuracy : 0.8897290485192186  loss : 0.4134203634302765\n",
      "iterations 1737 accuracy : 0.8897290485192186  loss : 0.4133751902936761\n",
      "iterations 1738 accuracy : 0.8901491283343835  loss : 0.4133286453268453\n",
      "iterations 1739 accuracy : 0.8899390884268011  loss : 0.4132831721935779\n",
      "iterations 1740 accuracy : 0.8901491283343835  loss : 0.4132380199418988\n",
      "iterations 1741 accuracy : 0.8901491283343835  loss : 0.4131939198492692\n",
      "iterations 1742 accuracy : 0.8901491283343835  loss : 0.41314867880524125\n",
      "iterations 1743 accuracy : 0.8899390884268011  loss : 0.4131040306638912\n",
      "iterations 1744 accuracy : 0.8899390884268011  loss : 0.41305924094763297\n",
      "iterations 1745 accuracy : 0.8899390884268011  loss : 0.41301487916836593\n",
      "iterations 1746 accuracy : 0.8899390884268011  loss : 0.41296937891835067\n",
      "iterations 1747 accuracy : 0.8897290485192186  loss : 0.4129251761070037\n",
      "iterations 1748 accuracy : 0.8897290485192186  loss : 0.41288048618541956\n",
      "iterations 1749 accuracy : 0.8899390884268011  loss : 0.4128367061830978\n",
      "iterations 1750 accuracy : 0.8901491283343835  loss : 0.41279219481004664\n",
      "iterations 1751 accuracy : 0.8901491283343835  loss : 0.41274842564388764\n",
      "iterations 1752 accuracy : 0.8901491283343835  loss : 0.4127025652617636\n",
      "iterations 1753 accuracy : 0.8901491283343835  loss : 0.41265704109330437\n",
      "iterations 1754 accuracy : 0.8901491283343835  loss : 0.4126121775082002\n",
      "iterations 1755 accuracy : 0.8901491283343835  loss : 0.4125677710971479\n",
      "iterations 1756 accuracy : 0.8901491283343835  loss : 0.41252273379848803\n",
      "iterations 1757 accuracy : 0.8901491283343835  loss : 0.4124783799888383\n",
      "iterations 1758 accuracy : 0.8903591682419659  loss : 0.4124326973007627\n",
      "iterations 1759 accuracy : 0.8903591682419659  loss : 0.41238878494772624\n",
      "iterations 1760 accuracy : 0.8903591682419659  loss : 0.41234466983341217\n",
      "iterations 1761 accuracy : 0.8907792480571308  loss : 0.412299451333448\n",
      "iterations 1762 accuracy : 0.8907792480571308  loss : 0.4122548593874115\n",
      "iterations 1763 accuracy : 0.8907792480571308  loss : 0.41220957514345996\n",
      "iterations 1764 accuracy : 0.8907792480571308  loss : 0.4121641388503045\n",
      "iterations 1765 accuracy : 0.8907792480571308  loss : 0.4121183918485727\n",
      "iterations 1766 accuracy : 0.8907792480571308  loss : 0.4120737697009525\n",
      "iterations 1767 accuracy : 0.8907792480571308  loss : 0.4120297160986876\n",
      "iterations 1768 accuracy : 0.8907792480571308  loss : 0.41198496545405483\n",
      "iterations 1769 accuracy : 0.8907792480571308  loss : 0.4119397996066458\n",
      "iterations 1770 accuracy : 0.8907792480571308  loss : 0.4118950523212191\n",
      "iterations 1771 accuracy : 0.8907792480571308  loss : 0.41185080245507416\n",
      "iterations 1772 accuracy : 0.8911993278722957  loss : 0.41180625533648624\n",
      "iterations 1773 accuracy : 0.8909892879647133  loss : 0.41176077203273775\n",
      "iterations 1774 accuracy : 0.8911993278722957  loss : 0.41171595232558955\n",
      "iterations 1775 accuracy : 0.8911993278722957  loss : 0.4116708978389453\n",
      "iterations 1776 accuracy : 0.8914093677798782  loss : 0.4116256306439909\n",
      "iterations 1777 accuracy : 0.8914093677798782  loss : 0.411581284476261\n",
      "iterations 1778 accuracy : 0.8914093677798782  loss : 0.41153681357148736\n",
      "iterations 1779 accuracy : 0.8916194076874606  loss : 0.4114927117225055\n",
      "iterations 1780 accuracy : 0.8916194076874606  loss : 0.4114483444638586\n",
      "iterations 1781 accuracy : 0.8916194076874606  loss : 0.41140397743348905\n",
      "iterations 1782 accuracy : 0.8916194076874606  loss : 0.4113607636760209\n",
      "iterations 1783 accuracy : 0.8916194076874606  loss : 0.4113160618823107\n",
      "iterations 1784 accuracy : 0.8914093677798782  loss : 0.4112732718403775\n",
      "iterations 1785 accuracy : 0.8914093677798782  loss : 0.4112300869495429\n",
      "iterations 1786 accuracy : 0.8911993278722957  loss : 0.4111871687335912\n",
      "iterations 1787 accuracy : 0.8914093677798782  loss : 0.41114335172136074\n",
      "iterations 1788 accuracy : 0.8914093677798782  loss : 0.41109928831257575\n",
      "iterations 1789 accuracy : 0.8914093677798782  loss : 0.4110561863535733\n",
      "iterations 1790 accuracy : 0.8914093677798782  loss : 0.41101155379129284\n",
      "iterations 1791 accuracy : 0.8916194076874606  loss : 0.410967446791882\n",
      "iterations 1792 accuracy : 0.8916194076874606  loss : 0.4109244247176341\n",
      "iterations 1793 accuracy : 0.8916194076874606  loss : 0.4108819278136196\n",
      "iterations 1794 accuracy : 0.8916194076874606  loss : 0.41083859670164136\n",
      "iterations 1795 accuracy : 0.891829447595043  loss : 0.4107949187523627\n",
      "iterations 1796 accuracy : 0.8920394875026255  loss : 0.4107515198021418\n",
      "iterations 1797 accuracy : 0.8922495274102079  loss : 0.41070651807237907\n",
      "iterations 1798 accuracy : 0.8922495274102079  loss : 0.41066418356239015\n",
      "iterations 1799 accuracy : 0.8922495274102079  loss : 0.41062073476903566\n",
      "iterations 1800 accuracy : 0.8920394875026255  loss : 0.41057658963699534\n",
      "iterations 1801 accuracy : 0.891829447595043  loss : 0.4105338178128481\n",
      "iterations 1802 accuracy : 0.8920394875026255  loss : 0.41049051611435483\n",
      "iterations 1803 accuracy : 0.8920394875026255  loss : 0.41044659586473\n",
      "iterations 1804 accuracy : 0.891829447595043  loss : 0.410403061444339\n",
      "iterations 1805 accuracy : 0.8922495274102079  loss : 0.41035881098510774\n",
      "iterations 1806 accuracy : 0.8920394875026255  loss : 0.41031593014721324\n",
      "iterations 1807 accuracy : 0.8916194076874606  loss : 0.4102727620029499\n",
      "iterations 1808 accuracy : 0.8916194076874606  loss : 0.410229451264856\n",
      "iterations 1809 accuracy : 0.891829447595043  loss : 0.4101859751480682\n",
      "iterations 1810 accuracy : 0.8922495274102079  loss : 0.4101415589854234\n",
      "iterations 1811 accuracy : 0.8922495274102079  loss : 0.4100954092809249\n",
      "iterations 1812 accuracy : 0.8924595673177904  loss : 0.4100517103668418\n",
      "iterations 1813 accuracy : 0.8924595673177904  loss : 0.41000915365933377\n",
      "iterations 1814 accuracy : 0.8924595673177904  loss : 0.4099661006110368\n",
      "iterations 1815 accuracy : 0.8924595673177904  loss : 0.40992222493871067\n",
      "iterations 1816 accuracy : 0.8926696072253728  loss : 0.409878778447395\n",
      "iterations 1817 accuracy : 0.8926696072253728  loss : 0.4098355489944696\n",
      "iterations 1818 accuracy : 0.8926696072253728  loss : 0.40979236508545536\n",
      "iterations 1819 accuracy : 0.8926696072253728  loss : 0.4097486488033679\n",
      "iterations 1820 accuracy : 0.8926696072253728  loss : 0.4097052316293836\n",
      "iterations 1821 accuracy : 0.8926696072253728  loss : 0.40966172356405123\n",
      "iterations 1822 accuracy : 0.8926696072253728  loss : 0.40961872729626125\n",
      "iterations 1823 accuracy : 0.8926696072253728  loss : 0.40957583425608385\n",
      "iterations 1824 accuracy : 0.8926696072253728  loss : 0.4095332657773648\n",
      "iterations 1825 accuracy : 0.8930896870405377  loss : 0.40948922911882113\n",
      "iterations 1826 accuracy : 0.8930896870405377  loss : 0.4094466929027206\n",
      "iterations 1827 accuracy : 0.8930896870405377  loss : 0.40940313644910137\n",
      "iterations 1828 accuracy : 0.8930896870405377  loss : 0.4093595127920914\n",
      "iterations 1829 accuracy : 0.8930896870405377  loss : 0.4093167557460236\n",
      "iterations 1830 accuracy : 0.8930896870405377  loss : 0.40927386851865555\n",
      "iterations 1831 accuracy : 0.8930896870405377  loss : 0.40923025678198804\n",
      "iterations 1832 accuracy : 0.8932997269481201  loss : 0.4091867394670315\n",
      "iterations 1833 accuracy : 0.8932997269481201  loss : 0.4091429596964061\n",
      "iterations 1834 accuracy : 0.8932997269481201  loss : 0.40909886772372295\n",
      "iterations 1835 accuracy : 0.8932997269481201  loss : 0.4090558100080471\n",
      "iterations 1836 accuracy : 0.8932997269481201  loss : 0.409013390011941\n",
      "iterations 1837 accuracy : 0.8935097668557026  loss : 0.40897073518542326\n",
      "iterations 1838 accuracy : 0.8935097668557026  loss : 0.40892777590900925\n",
      "iterations 1839 accuracy : 0.8935097668557026  loss : 0.4088844056545111\n",
      "iterations 1840 accuracy : 0.8935097668557026  loss : 0.40884181571748757\n",
      "iterations 1841 accuracy : 0.8935097668557026  loss : 0.4087975921631896\n",
      "iterations 1842 accuracy : 0.8935097668557026  loss : 0.40875437137135867\n",
      "iterations 1843 accuracy : 0.8935097668557026  loss : 0.40871090535859245\n",
      "iterations 1844 accuracy : 0.8935097668557026  loss : 0.40866747471787823\n",
      "iterations 1845 accuracy : 0.8935097668557026  loss : 0.40862338608204274\n",
      "iterations 1846 accuracy : 0.893719806763285  loss : 0.4085790857178567\n",
      "iterations 1847 accuracy : 0.893719806763285  loss : 0.4085369677229441\n",
      "iterations 1848 accuracy : 0.893719806763285  loss : 0.40849424504505955\n",
      "iterations 1849 accuracy : 0.893719806763285  loss : 0.4084510929298826\n",
      "iterations 1850 accuracy : 0.893719806763285  loss : 0.40840892652231603\n",
      "iterations 1851 accuracy : 0.8939298466708675  loss : 0.4083651451272059\n",
      "iterations 1852 accuracy : 0.8939298466708675  loss : 0.4083220722934447\n",
      "iterations 1853 accuracy : 0.8939298466708675  loss : 0.4082780356095295\n",
      "iterations 1854 accuracy : 0.8939298466708675  loss : 0.4082354318552335\n",
      "iterations 1855 accuracy : 0.8941398865784499  loss : 0.40819199222879726\n",
      "iterations 1856 accuracy : 0.8939298466708675  loss : 0.40814978663717777\n",
      "iterations 1857 accuracy : 0.8939298466708675  loss : 0.40810817123906235\n",
      "iterations 1858 accuracy : 0.8941398865784499  loss : 0.4080659761157815\n",
      "iterations 1859 accuracy : 0.8941398865784499  loss : 0.4080233546862647\n",
      "iterations 1860 accuracy : 0.8941398865784499  loss : 0.4079822882034646\n",
      "iterations 1861 accuracy : 0.8941398865784499  loss : 0.40793955120876746\n",
      "iterations 1862 accuracy : 0.8941398865784499  loss : 0.40789791268064785\n",
      "iterations 1863 accuracy : 0.8943499264860324  loss : 0.4078544294046724\n",
      "iterations 1864 accuracy : 0.8943499264860324  loss : 0.4078119575596481\n",
      "iterations 1865 accuracy : 0.8943499264860324  loss : 0.40776952505686714\n",
      "iterations 1866 accuracy : 0.8943499264860324  loss : 0.40772769780465234\n",
      "iterations 1867 accuracy : 0.8945599663936148  loss : 0.40768480628551834\n",
      "iterations 1868 accuracy : 0.8945599663936148  loss : 0.407643027035875\n",
      "iterations 1869 accuracy : 0.8945599663936148  loss : 0.40760030736216896\n",
      "iterations 1870 accuracy : 0.8945599663936148  loss : 0.40755746269696785\n",
      "iterations 1871 accuracy : 0.8945599663936148  loss : 0.4075136972576006\n",
      "iterations 1872 accuracy : 0.8954001260239446  loss : 0.407472404377406\n",
      "iterations 1873 accuracy : 0.8954001260239446  loss : 0.4074304642615579\n",
      "iterations 1874 accuracy : 0.8954001260239446  loss : 0.40738881156066736\n",
      "iterations 1875 accuracy : 0.8949800462087797  loss : 0.4073472634173304\n",
      "iterations 1876 accuracy : 0.8954001260239446  loss : 0.407305666367218\n",
      "iterations 1877 accuracy : 0.8958202058391095  loss : 0.40726259508996193\n",
      "iterations 1878 accuracy : 0.8958202058391095  loss : 0.40722046370853443\n",
      "iterations 1879 accuracy : 0.8960302457466919  loss : 0.40717835763445376\n",
      "iterations 1880 accuracy : 0.8960302457466919  loss : 0.4071356500273094\n",
      "iterations 1881 accuracy : 0.8960302457466919  loss : 0.40709436229647067\n",
      "iterations 1882 accuracy : 0.8960302457466919  loss : 0.4070519661102912\n",
      "iterations 1883 accuracy : 0.8960302457466919  loss : 0.4070095508386455\n",
      "iterations 1884 accuracy : 0.8962402856542743  loss : 0.4069673265174673\n",
      "iterations 1885 accuracy : 0.8960302457466919  loss : 0.40692549857176114\n",
      "iterations 1886 accuracy : 0.8960302457466919  loss : 0.4068830101665824\n",
      "iterations 1887 accuracy : 0.8960302457466919  loss : 0.4068425304270012\n",
      "iterations 1888 accuracy : 0.8960302457466919  loss : 0.4068005624855806\n",
      "iterations 1889 accuracy : 0.8964503255618568  loss : 0.40675886176511417\n",
      "iterations 1890 accuracy : 0.8962402856542743  loss : 0.40671724162035555\n",
      "iterations 1891 accuracy : 0.8966603654694392  loss : 0.40667498304027483\n",
      "iterations 1892 accuracy : 0.8966603654694392  loss : 0.4066320900357986\n",
      "iterations 1893 accuracy : 0.8966603654694392  loss : 0.40659051428115334\n",
      "iterations 1894 accuracy : 0.8962402856542743  loss : 0.4065487715731399\n",
      "iterations 1895 accuracy : 0.8966603654694392  loss : 0.4065067554247491\n",
      "iterations 1896 accuracy : 0.8966603654694392  loss : 0.4064644617270779\n",
      "iterations 1897 accuracy : 0.8966603654694392  loss : 0.4064230131655113\n",
      "iterations 1898 accuracy : 0.8966603654694392  loss : 0.4063819673727173\n",
      "iterations 1899 accuracy : 0.8966603654694392  loss : 0.40634042597415837\n",
      "iterations 1900 accuracy : 0.8966603654694392  loss : 0.40630018469913387\n",
      "iterations 1901 accuracy : 0.8966603654694392  loss : 0.4062599767136775\n",
      "iterations 1902 accuracy : 0.8966603654694392  loss : 0.4062178593083213\n",
      "iterations 1903 accuracy : 0.8966603654694392  loss : 0.40617565267073574\n",
      "iterations 1904 accuracy : 0.8966603654694392  loss : 0.40613387958269787\n",
      "iterations 1905 accuracy : 0.8966603654694392  loss : 0.4060927445857492\n",
      "iterations 1906 accuracy : 0.8968704053770217  loss : 0.406050983820119\n",
      "iterations 1907 accuracy : 0.8970804452846041  loss : 0.40600878129279444\n",
      "iterations 1908 accuracy : 0.8970804452846041  loss : 0.40596743261845636\n",
      "iterations 1909 accuracy : 0.8970804452846041  loss : 0.40592587637827054\n",
      "iterations 1910 accuracy : 0.8970804452846041  loss : 0.40588513632140477\n",
      "iterations 1911 accuracy : 0.8970804452846041  loss : 0.40584342729307316\n",
      "iterations 1912 accuracy : 0.8970804452846041  loss : 0.40580345974551896\n",
      "iterations 1913 accuracy : 0.8970804452846041  loss : 0.40576249490356303\n",
      "iterations 1914 accuracy : 0.8970804452846041  loss : 0.4057201290471488\n",
      "iterations 1915 accuracy : 0.8970804452846041  loss : 0.40567844501736694\n",
      "iterations 1916 accuracy : 0.8970804452846041  loss : 0.405637941538572\n",
      "iterations 1917 accuracy : 0.8970804452846041  loss : 0.40559640500685934\n",
      "iterations 1918 accuracy : 0.8970804452846041  loss : 0.4055551217881063\n",
      "iterations 1919 accuracy : 0.8970804452846041  loss : 0.40551436897454657\n",
      "iterations 1920 accuracy : 0.8970804452846041  loss : 0.40547466236093294\n",
      "iterations 1921 accuracy : 0.8970804452846041  loss : 0.405433337425037\n",
      "iterations 1922 accuracy : 0.8972904851921865  loss : 0.4053917632456862\n",
      "iterations 1923 accuracy : 0.8972904851921865  loss : 0.4053507183986396\n",
      "iterations 1924 accuracy : 0.8972904851921865  loss : 0.40531008153900755\n",
      "iterations 1925 accuracy : 0.897500525099769  loss : 0.40526755510157264\n",
      "iterations 1926 accuracy : 0.8977105650073514  loss : 0.405225896587626\n",
      "iterations 1927 accuracy : 0.8977105650073514  loss : 0.4051843077735316\n",
      "iterations 1928 accuracy : 0.8977105650073514  loss : 0.40514296970114155\n",
      "iterations 1929 accuracy : 0.8977105650073514  loss : 0.40510146156301385\n",
      "iterations 1930 accuracy : 0.8979206049149339  loss : 0.40505973762765196\n",
      "iterations 1931 accuracy : 0.8977105650073514  loss : 0.40501942337687513\n",
      "iterations 1932 accuracy : 0.8977105650073514  loss : 0.40497868871167253\n",
      "iterations 1933 accuracy : 0.8977105650073514  loss : 0.40493799848905765\n",
      "iterations 1934 accuracy : 0.8981306448225163  loss : 0.4048967401432222\n",
      "iterations 1935 accuracy : 0.8981306448225163  loss : 0.40485630886115437\n",
      "iterations 1936 accuracy : 0.8981306448225163  loss : 0.40481460013843473\n",
      "iterations 1937 accuracy : 0.8979206049149339  loss : 0.40477426297005553\n",
      "iterations 1938 accuracy : 0.8979206049149339  loss : 0.4047349978169183\n",
      "iterations 1939 accuracy : 0.8981306448225163  loss : 0.4046928259084031\n",
      "iterations 1940 accuracy : 0.8983406847300988  loss : 0.4046514243752409\n",
      "iterations 1941 accuracy : 0.8983406847300988  loss : 0.4046119672311253\n",
      "iterations 1942 accuracy : 0.8983406847300988  loss : 0.4045714116225448\n",
      "iterations 1943 accuracy : 0.8983406847300988  loss : 0.40453063666602723\n",
      "iterations 1944 accuracy : 0.8983406847300988  loss : 0.4044891018735183\n",
      "iterations 1945 accuracy : 0.8983406847300988  loss : 0.40444893108039415\n",
      "iterations 1946 accuracy : 0.8983406847300988  loss : 0.4044077797249224\n",
      "iterations 1947 accuracy : 0.8983406847300988  loss : 0.40436703320738115\n",
      "iterations 1948 accuracy : 0.8983406847300988  loss : 0.4043266121277535\n",
      "iterations 1949 accuracy : 0.8983406847300988  loss : 0.4042865657434661\n",
      "iterations 1950 accuracy : 0.8987607645452635  loss : 0.40424529612430693\n",
      "iterations 1951 accuracy : 0.8987607645452635  loss : 0.4042056762403933\n",
      "iterations 1952 accuracy : 0.8983406847300988  loss : 0.40416691364907936\n",
      "iterations 1953 accuracy : 0.8985507246376812  loss : 0.4041262861032829\n",
      "iterations 1954 accuracy : 0.8987607645452635  loss : 0.4040863892873142\n",
      "iterations 1955 accuracy : 0.898970804452846  loss : 0.4040463584893519\n",
      "iterations 1956 accuracy : 0.898970804452846  loss : 0.4040060578706544\n",
      "iterations 1957 accuracy : 0.898970804452846  loss : 0.40396608992015365\n",
      "iterations 1958 accuracy : 0.898970804452846  loss : 0.4039268517644306\n",
      "iterations 1959 accuracy : 0.898970804452846  loss : 0.40388592787871597\n",
      "iterations 1960 accuracy : 0.898970804452846  loss : 0.4038454476098762\n",
      "iterations 1961 accuracy : 0.898970804452846  loss : 0.40380620239849346\n",
      "iterations 1962 accuracy : 0.898970804452846  loss : 0.40376545380560425\n",
      "iterations 1963 accuracy : 0.898970804452846  loss : 0.40372471979085134\n",
      "iterations 1964 accuracy : 0.8991808443604284  loss : 0.4036842417361291\n",
      "iterations 1965 accuracy : 0.8991808443604284  loss : 0.4036436125696182\n",
      "iterations 1966 accuracy : 0.899390884268011  loss : 0.4036017630009037\n",
      "iterations 1967 accuracy : 0.899390884268011  loss : 0.4035608431514826\n",
      "iterations 1968 accuracy : 0.8996009241755933  loss : 0.40351969670741317\n",
      "iterations 1969 accuracy : 0.8996009241755933  loss : 0.4034804303608822\n",
      "iterations 1970 accuracy : 0.8996009241755933  loss : 0.40343916480615094\n",
      "iterations 1971 accuracy : 0.8996009241755933  loss : 0.403399431145165\n",
      "iterations 1972 accuracy : 0.8996009241755933  loss : 0.4033601888327166\n",
      "iterations 1973 accuracy : 0.8996009241755933  loss : 0.4033201152464691\n",
      "iterations 1974 accuracy : 0.9000210039907582  loss : 0.4032797857659729\n",
      "iterations 1975 accuracy : 0.9000210039907582  loss : 0.40323998742957223\n",
      "iterations 1976 accuracy : 0.9000210039907582  loss : 0.4032005385432887\n",
      "iterations 1977 accuracy : 0.9000210039907582  loss : 0.40316020744737247\n",
      "iterations 1978 accuracy : 0.9002310438983406  loss : 0.4031201851173672\n",
      "iterations 1979 accuracy : 0.9002310438983406  loss : 0.40308124460695216\n",
      "iterations 1980 accuracy : 0.9004410838059231  loss : 0.4030412231581163\n",
      "iterations 1981 accuracy : 0.9004410838059231  loss : 0.40300222983811446\n",
      "iterations 1982 accuracy : 0.9004410838059231  loss : 0.4029628457059884\n",
      "iterations 1983 accuracy : 0.9004410838059231  loss : 0.4029236377253374\n",
      "iterations 1984 accuracy : 0.9006511237135055  loss : 0.4028846303680212\n",
      "iterations 1985 accuracy : 0.9006511237135055  loss : 0.4028451690524127\n",
      "iterations 1986 accuracy : 0.9006511237135055  loss : 0.40280699346491117\n",
      "iterations 1987 accuracy : 0.9006511237135055  loss : 0.40276713883287274\n",
      "iterations 1988 accuracy : 0.9006511237135055  loss : 0.4027274244289682\n",
      "iterations 1989 accuracy : 0.9006511237135055  loss : 0.4026885173210839\n",
      "iterations 1990 accuracy : 0.9006511237135055  loss : 0.4026497949532087\n",
      "iterations 1991 accuracy : 0.900861163621088  loss : 0.40261019102160556\n",
      "iterations 1992 accuracy : 0.900861163621088  loss : 0.40256958674525856\n",
      "iterations 1993 accuracy : 0.900861163621088  loss : 0.4025304575624324\n",
      "iterations 1994 accuracy : 0.900861163621088  loss : 0.40249150114894333\n",
      "iterations 1995 accuracy : 0.900861163621088  loss : 0.4024511554253309\n",
      "iterations 1996 accuracy : 0.900861163621088  loss : 0.40241188554323115\n",
      "iterations 1997 accuracy : 0.900861163621088  loss : 0.40237197554076837\n",
      "iterations 1998 accuracy : 0.900861163621088  loss : 0.40233325030413725\n",
      "iterations 1999 accuracy : 0.9010712035286704  loss : 0.402293757095515\n",
      "iterations 2000 accuracy : 0.9010712035286704  loss : 0.40225470265194757\n",
      "iterations 2001 accuracy : 0.900861163621088  loss : 0.40221518289233765\n",
      "iterations 2002 accuracy : 0.9010712035286704  loss : 0.40217578257233977\n",
      "iterations 2003 accuracy : 0.900861163621088  loss : 0.4021381769629596\n",
      "iterations 2004 accuracy : 0.900861163621088  loss : 0.4020981027564607\n",
      "iterations 2005 accuracy : 0.9012812434362529  loss : 0.4020584891021942\n",
      "iterations 2006 accuracy : 0.9010712035286704  loss : 0.40202021859350073\n",
      "iterations 2007 accuracy : 0.900861163621088  loss : 0.40198176358168686\n",
      "iterations 2008 accuracy : 0.9014912833438353  loss : 0.4019407572224158\n",
      "iterations 2009 accuracy : 0.9014912833438353  loss : 0.40190226389856204\n",
      "iterations 2010 accuracy : 0.9014912833438353  loss : 0.40186400111189946\n",
      "iterations 2011 accuracy : 0.9014912833438353  loss : 0.40182460839082423\n",
      "iterations 2012 accuracy : 0.9014912833438353  loss : 0.40178604045078103\n",
      "iterations 2013 accuracy : 0.9014912833438353  loss : 0.40174623966489825\n",
      "iterations 2014 accuracy : 0.9014912833438353  loss : 0.40170730362436236\n",
      "iterations 2015 accuracy : 0.9014912833438353  loss : 0.40166791173651045\n",
      "iterations 2016 accuracy : 0.9014912833438353  loss : 0.4016282153964671\n",
      "iterations 2017 accuracy : 0.9014912833438353  loss : 0.4015893788311538\n",
      "iterations 2018 accuracy : 0.9014912833438353  loss : 0.40155081817847305\n",
      "iterations 2019 accuracy : 0.9014912833438353  loss : 0.40151180686183274\n",
      "iterations 2020 accuracy : 0.9014912833438353  loss : 0.4014725483173482\n",
      "iterations 2021 accuracy : 0.9014912833438353  loss : 0.401434065907044\n",
      "iterations 2022 accuracy : 0.9014912833438353  loss : 0.4013958338951981\n",
      "iterations 2023 accuracy : 0.9017013232514177  loss : 0.4013567637602189\n",
      "iterations 2024 accuracy : 0.9017013232514177  loss : 0.4013179193029577\n",
      "iterations 2025 accuracy : 0.9019113631590002  loss : 0.4012789796741486\n",
      "iterations 2026 accuracy : 0.9023314429741651  loss : 0.4012396873524725\n",
      "iterations 2027 accuracy : 0.90275152278933  loss : 0.40120050194492235\n",
      "iterations 2028 accuracy : 0.90275152278933  loss : 0.40116178147285547\n",
      "iterations 2029 accuracy : 0.90275152278933  loss : 0.40112212321454455\n",
      "iterations 2030 accuracy : 0.90275152278933  loss : 0.40108301466701873\n",
      "iterations 2031 accuracy : 0.90275152278933  loss : 0.4010444924545075\n",
      "iterations 2032 accuracy : 0.9023314429741651  loss : 0.4010058373211217\n",
      "iterations 2033 accuracy : 0.90275152278933  loss : 0.4009681608588145\n",
      "iterations 2034 accuracy : 0.90275152278933  loss : 0.4009307620563271\n",
      "iterations 2035 accuracy : 0.90275152278933  loss : 0.40089190753296106\n",
      "iterations 2036 accuracy : 0.90275152278933  loss : 0.40085344016248886\n",
      "iterations 2037 accuracy : 0.90275152278933  loss : 0.40081439048116363\n",
      "iterations 2038 accuracy : 0.90275152278933  loss : 0.4007766503294494\n",
      "iterations 2039 accuracy : 0.90275152278933  loss : 0.40073888062447915\n",
      "iterations 2040 accuracy : 0.9029615626969124  loss : 0.40070011453698773\n",
      "iterations 2041 accuracy : 0.9029615626969124  loss : 0.40066162749023365\n",
      "iterations 2042 accuracy : 0.9029615626969124  loss : 0.4006226678536576\n",
      "iterations 2043 accuracy : 0.9031716026044948  loss : 0.40058427491203824\n",
      "iterations 2044 accuracy : 0.9031716026044948  loss : 0.40054525470931374\n",
      "iterations 2045 accuracy : 0.9031716026044948  loss : 0.400506042327202\n",
      "iterations 2046 accuracy : 0.9031716026044948  loss : 0.40046871616822227\n",
      "iterations 2047 accuracy : 0.9029615626969124  loss : 0.40042957863078754\n",
      "iterations 2048 accuracy : 0.9033816425120773  loss : 0.40038993794271943\n",
      "iterations 2049 accuracy : 0.9031716026044948  loss : 0.4003521602945089\n",
      "iterations 2050 accuracy : 0.9038017223272422  loss : 0.40031400122295124\n",
      "iterations 2051 accuracy : 0.9038017223272422  loss : 0.4002758747515936\n",
      "iterations 2052 accuracy : 0.9038017223272422  loss : 0.40023736337971527\n",
      "iterations 2053 accuracy : 0.9038017223272422  loss : 0.40019982910163326\n",
      "iterations 2054 accuracy : 0.9042218021424071  loss : 0.40016057953546647\n",
      "iterations 2055 accuracy : 0.9042218021424071  loss : 0.40012200229301\n",
      "iterations 2056 accuracy : 0.9044318420499895  loss : 0.40008343708542915\n",
      "iterations 2057 accuracy : 0.9046418819575719  loss : 0.4000446652008254\n",
      "iterations 2058 accuracy : 0.9046418819575719  loss : 0.4000053448323852\n",
      "iterations 2059 accuracy : 0.9046418819575719  loss : 0.39996702617810265\n",
      "iterations 2060 accuracy : 0.9046418819575719  loss : 0.39992883798259937\n",
      "iterations 2061 accuracy : 0.9046418819575719  loss : 0.3998918448536935\n",
      "iterations 2062 accuracy : 0.9044318420499895  loss : 0.399853808225288\n",
      "iterations 2063 accuracy : 0.9046418819575719  loss : 0.3998160432658485\n",
      "iterations 2064 accuracy : 0.9046418819575719  loss : 0.39977877338764733\n",
      "iterations 2065 accuracy : 0.9046418819575719  loss : 0.3997422815064183\n",
      "iterations 2066 accuracy : 0.9046418819575719  loss : 0.39970459143068127\n",
      "iterations 2067 accuracy : 0.9046418819575719  loss : 0.39966645928360445\n",
      "iterations 2068 accuracy : 0.9046418819575719  loss : 0.39962923549265134\n",
      "iterations 2069 accuracy : 0.9046418819575719  loss : 0.3995915723896462\n",
      "iterations 2070 accuracy : 0.9048519218651544  loss : 0.3995526930222191\n",
      "iterations 2071 accuracy : 0.9048519218651544  loss : 0.39951486288807087\n",
      "iterations 2072 accuracy : 0.9050619617727368  loss : 0.39947576094446824\n",
      "iterations 2073 accuracy : 0.9050619617727368  loss : 0.3994385631685801\n",
      "iterations 2074 accuracy : 0.9052720016803193  loss : 0.39940038160011426\n",
      "iterations 2075 accuracy : 0.9054820415879017  loss : 0.39936215705732214\n",
      "iterations 2076 accuracy : 0.9054820415879017  loss : 0.3993246284733165\n",
      "iterations 2077 accuracy : 0.9056920814954842  loss : 0.399285826538606\n",
      "iterations 2078 accuracy : 0.9054820415879017  loss : 0.3992487320135149\n",
      "iterations 2079 accuracy : 0.9056920814954842  loss : 0.399210376413985\n",
      "iterations 2080 accuracy : 0.9056920814954842  loss : 0.399172979856783\n",
      "iterations 2081 accuracy : 0.9059021214030666  loss : 0.39913542736082397\n",
      "iterations 2082 accuracy : 0.9056920814954842  loss : 0.3990979229416606\n",
      "iterations 2083 accuracy : 0.9059021214030666  loss : 0.3990603378934916\n",
      "iterations 2084 accuracy : 0.9056920814954842  loss : 0.39902344504801895\n",
      "iterations 2085 accuracy : 0.9059021214030666  loss : 0.3989845113543024\n",
      "iterations 2086 accuracy : 0.906112161310649  loss : 0.39894492585400076\n",
      "iterations 2087 accuracy : 0.906112161310649  loss : 0.39890669306523946\n",
      "iterations 2088 accuracy : 0.906112161310649  loss : 0.398869279309431\n",
      "iterations 2089 accuracy : 0.906112161310649  loss : 0.3988314953051002\n",
      "iterations 2090 accuracy : 0.906112161310649  loss : 0.3987941839904953\n",
      "iterations 2091 accuracy : 0.906112161310649  loss : 0.39875565110259126\n",
      "iterations 2092 accuracy : 0.906112161310649  loss : 0.39871748776339355\n",
      "iterations 2093 accuracy : 0.906112161310649  loss : 0.398680566301728\n",
      "iterations 2094 accuracy : 0.906112161310649  loss : 0.3986434149894713\n",
      "iterations 2095 accuracy : 0.9063222012182315  loss : 0.39860536392828055\n",
      "iterations 2096 accuracy : 0.9063222012182315  loss : 0.39856757275526183\n",
      "iterations 2097 accuracy : 0.9065322411258139  loss : 0.39852992957289707\n",
      "iterations 2098 accuracy : 0.9067422810333964  loss : 0.3984911493440961\n",
      "iterations 2099 accuracy : 0.9067422810333964  loss : 0.3984534287858281\n",
      "iterations 2100 accuracy : 0.9067422810333964  loss : 0.3984165815681599\n",
      "iterations 2101 accuracy : 0.9069523209409788  loss : 0.39837920466233995\n",
      "iterations 2102 accuracy : 0.9069523209409788  loss : 0.39834210453430663\n",
      "iterations 2103 accuracy : 0.9069523209409788  loss : 0.39830470686892777\n",
      "iterations 2104 accuracy : 0.9069523209409788  loss : 0.39826814725660264\n",
      "iterations 2105 accuracy : 0.9069523209409788  loss : 0.39823027217991214\n",
      "iterations 2106 accuracy : 0.9071623608485613  loss : 0.39819243034521895\n",
      "iterations 2107 accuracy : 0.9071623608485613  loss : 0.39815531035730833\n",
      "iterations 2108 accuracy : 0.9071623608485613  loss : 0.3981186630888912\n",
      "iterations 2109 accuracy : 0.9071623608485613  loss : 0.39808127917110764\n",
      "iterations 2110 accuracy : 0.9071623608485613  loss : 0.3980442538454318\n",
      "iterations 2111 accuracy : 0.9071623608485613  loss : 0.398006777376094\n",
      "iterations 2112 accuracy : 0.9071623608485613  loss : 0.397969252656381\n",
      "iterations 2113 accuracy : 0.9071623608485613  loss : 0.3979310750493478\n",
      "iterations 2114 accuracy : 0.9071623608485613  loss : 0.3978942465253471\n",
      "iterations 2115 accuracy : 0.9071623608485613  loss : 0.39785598386593163\n",
      "iterations 2116 accuracy : 0.9073724007561437  loss : 0.39781918794832777\n",
      "iterations 2117 accuracy : 0.9073724007561437  loss : 0.3977820360387861\n",
      "iterations 2118 accuracy : 0.9075824406637261  loss : 0.3977450893357279\n",
      "iterations 2119 accuracy : 0.9075824406637261  loss : 0.39770827834809486\n",
      "iterations 2120 accuracy : 0.9075824406637261  loss : 0.39767138032186305\n",
      "iterations 2121 accuracy : 0.9075824406637261  loss : 0.3976351809439398\n",
      "iterations 2122 accuracy : 0.9077924805713086  loss : 0.39759742898824774\n",
      "iterations 2123 accuracy : 0.9077924805713086  loss : 0.39756145515271485\n",
      "iterations 2124 accuracy : 0.9075824406637261  loss : 0.3975248771062078\n",
      "iterations 2125 accuracy : 0.9075824406637261  loss : 0.39748834317456544\n",
      "iterations 2126 accuracy : 0.9075824406637261  loss : 0.3974513589257325\n",
      "iterations 2127 accuracy : 0.9077924805713086  loss : 0.39741375228145076\n",
      "iterations 2128 accuracy : 0.908002520478891  loss : 0.3973764001503658\n",
      "iterations 2129 accuracy : 0.9082125603864735  loss : 0.397339325844639\n",
      "iterations 2130 accuracy : 0.9084226002940559  loss : 0.39730143304257637\n",
      "iterations 2131 accuracy : 0.9084226002940559  loss : 0.39726508549020156\n",
      "iterations 2132 accuracy : 0.9082125603864735  loss : 0.3972288738095203\n",
      "iterations 2133 accuracy : 0.9086326402016384  loss : 0.3971907144881016\n",
      "iterations 2134 accuracy : 0.9086326402016384  loss : 0.39715353653402213\n",
      "iterations 2135 accuracy : 0.9086326402016384  loss : 0.3971167200905919\n",
      "iterations 2136 accuracy : 0.9088426801092208  loss : 0.397079190023545\n",
      "iterations 2137 accuracy : 0.9088426801092208  loss : 0.3970428738646007\n",
      "iterations 2138 accuracy : 0.9088426801092208  loss : 0.39700543362674706\n",
      "iterations 2139 accuracy : 0.9088426801092208  loss : 0.3969683346937273\n",
      "iterations 2140 accuracy : 0.9088426801092208  loss : 0.3969317860759329\n",
      "iterations 2141 accuracy : 0.9088426801092208  loss : 0.39689654021629517\n",
      "iterations 2142 accuracy : 0.9088426801092208  loss : 0.39686022533129517\n",
      "iterations 2143 accuracy : 0.9090527200168032  loss : 0.39682273284250535\n",
      "iterations 2144 accuracy : 0.9092627599243857  loss : 0.39678579792719293\n",
      "iterations 2145 accuracy : 0.9092627599243857  loss : 0.3967480062100592\n",
      "iterations 2146 accuracy : 0.9092627599243857  loss : 0.3967112033421811\n",
      "iterations 2147 accuracy : 0.909472799831968  loss : 0.39667432487791643\n",
      "iterations 2148 accuracy : 0.909472799831968  loss : 0.3966389255625546\n",
      "iterations 2149 accuracy : 0.909472799831968  loss : 0.39660239056389346\n",
      "iterations 2150 accuracy : 0.909472799831968  loss : 0.39656566804005\n",
      "iterations 2151 accuracy : 0.909472799831968  loss : 0.39652971832360606\n",
      "iterations 2152 accuracy : 0.909472799831968  loss : 0.3964936726940019\n",
      "iterations 2153 accuracy : 0.909472799831968  loss : 0.39645701460344296\n",
      "iterations 2154 accuracy : 0.909472799831968  loss : 0.39641977270848483\n",
      "iterations 2155 accuracy : 0.909472799831968  loss : 0.39638354481876975\n",
      "iterations 2156 accuracy : 0.909472799831968  loss : 0.3963478388710647\n",
      "iterations 2157 accuracy : 0.909472799831968  loss : 0.396312556398839\n",
      "iterations 2158 accuracy : 0.909472799831968  loss : 0.3962762373939722\n",
      "iterations 2159 accuracy : 0.909472799831968  loss : 0.3962402245210953\n",
      "iterations 2160 accuracy : 0.909472799831968  loss : 0.39620464474799083\n",
      "iterations 2161 accuracy : 0.909472799831968  loss : 0.39616705456546486\n",
      "iterations 2162 accuracy : 0.909472799831968  loss : 0.39613051287164686\n",
      "iterations 2163 accuracy : 0.909472799831968  loss : 0.3960942693980175\n",
      "iterations 2164 accuracy : 0.9096828397395506  loss : 0.3960572376573281\n",
      "iterations 2165 accuracy : 0.909472799831968  loss : 0.3960210964269274\n",
      "iterations 2166 accuracy : 0.9096828397395506  loss : 0.3959857363935781\n",
      "iterations 2167 accuracy : 0.909472799831968  loss : 0.3959500113440588\n",
      "iterations 2168 accuracy : 0.9096828397395506  loss : 0.39591355377834264\n",
      "iterations 2169 accuracy : 0.9096828397395506  loss : 0.39587659741929443\n",
      "iterations 2170 accuracy : 0.9096828397395506  loss : 0.39583943868879806\n",
      "iterations 2171 accuracy : 0.9096828397395506  loss : 0.3958035225476616\n",
      "iterations 2172 accuracy : 0.9096828397395506  loss : 0.39576735896027454\n",
      "iterations 2173 accuracy : 0.9096828397395506  loss : 0.39573184430440994\n",
      "iterations 2174 accuracy : 0.909892879647133  loss : 0.39569592123338876\n",
      "iterations 2175 accuracy : 0.9103129594622978  loss : 0.3956588558663454\n",
      "iterations 2176 accuracy : 0.9105229993698802  loss : 0.39562302097269464\n",
      "iterations 2177 accuracy : 0.9107330392774627  loss : 0.39558692580149957\n",
      "iterations 2178 accuracy : 0.9109430791850451  loss : 0.3955509295164784\n",
      "iterations 2179 accuracy : 0.9109430791850451  loss : 0.39551556391306236\n",
      "iterations 2180 accuracy : 0.9109430791850451  loss : 0.39547922071861785\n",
      "iterations 2181 accuracy : 0.9109430791850451  loss : 0.3954425172088518\n",
      "iterations 2182 accuracy : 0.9109430791850451  loss : 0.39540716430030054\n",
      "iterations 2183 accuracy : 0.9109430791850451  loss : 0.395371299090759\n",
      "iterations 2184 accuracy : 0.9109430791850451  loss : 0.3953360823341333\n",
      "iterations 2185 accuracy : 0.9109430791850451  loss : 0.3953011093485397\n",
      "iterations 2186 accuracy : 0.9109430791850451  loss : 0.395264795673429\n",
      "iterations 2187 accuracy : 0.9109430791850451  loss : 0.3952290840526265\n",
      "iterations 2188 accuracy : 0.9109430791850451  loss : 0.39519417099996157\n",
      "iterations 2189 accuracy : 0.9109430791850451  loss : 0.3951581669691907\n",
      "iterations 2190 accuracy : 0.9109430791850451  loss : 0.39512283730054115\n",
      "iterations 2191 accuracy : 0.9111531190926276  loss : 0.3950865062663284\n",
      "iterations 2192 accuracy : 0.9111531190926276  loss : 0.3950513029310644\n",
      "iterations 2193 accuracy : 0.91136315900021  loss : 0.3950154863960102\n",
      "iterations 2194 accuracy : 0.91136315900021  loss : 0.39497996933008117\n",
      "iterations 2195 accuracy : 0.9111531190926276  loss : 0.3949452293506143\n",
      "iterations 2196 accuracy : 0.91136315900021  loss : 0.3949105589890006\n",
      "iterations 2197 accuracy : 0.91136315900021  loss : 0.3948761672324112\n",
      "iterations 2198 accuracy : 0.91136315900021  loss : 0.3948405573468777\n",
      "iterations 2199 accuracy : 0.91136315900021  loss : 0.3948049489461929\n",
      "iterations 2200 accuracy : 0.91136315900021  loss : 0.3947700673406959\n",
      "iterations 2201 accuracy : 0.91136315900021  loss : 0.39473511955950735\n",
      "iterations 2202 accuracy : 0.91136315900021  loss : 0.39469916073367123\n",
      "iterations 2203 accuracy : 0.91136315900021  loss : 0.3946640322480532\n",
      "iterations 2204 accuracy : 0.91136315900021  loss : 0.394627136028916\n",
      "iterations 2205 accuracy : 0.91136315900021  loss : 0.3945927854873501\n",
      "iterations 2206 accuracy : 0.91136315900021  loss : 0.3945574407237998\n",
      "iterations 2207 accuracy : 0.9115731989077924  loss : 0.3945208250074503\n",
      "iterations 2208 accuracy : 0.91136315900021  loss : 0.3944874201226674\n",
      "iterations 2209 accuracy : 0.91136315900021  loss : 0.3944526994295852\n",
      "iterations 2210 accuracy : 0.91136315900021  loss : 0.3944171109127255\n",
      "iterations 2211 accuracy : 0.91136315900021  loss : 0.3943814222302097\n",
      "iterations 2212 accuracy : 0.91136315900021  loss : 0.3943449865168501\n",
      "iterations 2213 accuracy : 0.9115731989077924  loss : 0.39430978868176614\n",
      "iterations 2214 accuracy : 0.9115731989077924  loss : 0.3942736719552728\n",
      "iterations 2215 accuracy : 0.9115731989077924  loss : 0.3942379413512667\n",
      "iterations 2216 accuracy : 0.9115731989077924  loss : 0.3942027469093811\n",
      "iterations 2217 accuracy : 0.9115731989077924  loss : 0.39416842802218943\n",
      "iterations 2218 accuracy : 0.9115731989077924  loss : 0.39413380531170833\n",
      "iterations 2219 accuracy : 0.9115731989077924  loss : 0.3940995407283016\n",
      "iterations 2220 accuracy : 0.9115731989077924  loss : 0.39406447581461584\n",
      "iterations 2221 accuracy : 0.9115731989077924  loss : 0.39402902365669723\n",
      "iterations 2222 accuracy : 0.9115731989077924  loss : 0.39399503992691226\n",
      "iterations 2223 accuracy : 0.9115731989077924  loss : 0.39396009757490036\n",
      "iterations 2224 accuracy : 0.9115731989077924  loss : 0.3939254321770753\n",
      "iterations 2225 accuracy : 0.9115731989077924  loss : 0.3938904095827379\n",
      "iterations 2226 accuracy : 0.9117832388153749  loss : 0.39385520593289075\n",
      "iterations 2227 accuracy : 0.9115731989077924  loss : 0.393820646023574\n",
      "iterations 2228 accuracy : 0.9115731989077924  loss : 0.39378487869654527\n",
      "iterations 2229 accuracy : 0.9115731989077924  loss : 0.39374955980030435\n",
      "iterations 2230 accuracy : 0.9117832388153749  loss : 0.3937130684435407\n",
      "iterations 2231 accuracy : 0.9117832388153749  loss : 0.3936783039896707\n",
      "iterations 2232 accuracy : 0.9117832388153749  loss : 0.39364324322698196\n",
      "iterations 2233 accuracy : 0.9117832388153749  loss : 0.3936063850275737\n",
      "iterations 2234 accuracy : 0.9117832388153749  loss : 0.3935708913508977\n",
      "iterations 2235 accuracy : 0.9117832388153749  loss : 0.3935369181183563\n",
      "iterations 2236 accuracy : 0.9117832388153749  loss : 0.39350152057733184\n",
      "iterations 2237 accuracy : 0.9117832388153749  loss : 0.3934675267783137\n",
      "iterations 2238 accuracy : 0.9117832388153749  loss : 0.39343307915171083\n",
      "iterations 2239 accuracy : 0.9117832388153749  loss : 0.3933982081549819\n",
      "iterations 2240 accuracy : 0.9117832388153749  loss : 0.3933638160772579\n",
      "iterations 2241 accuracy : 0.9117832388153749  loss : 0.3933289583227521\n",
      "iterations 2242 accuracy : 0.9117832388153749  loss : 0.39329316933142633\n",
      "iterations 2243 accuracy : 0.9117832388153749  loss : 0.3932580261815955\n",
      "iterations 2244 accuracy : 0.9117832388153749  loss : 0.3932236092531581\n",
      "iterations 2245 accuracy : 0.9117832388153749  loss : 0.3931893429151269\n",
      "iterations 2246 accuracy : 0.9117832388153749  loss : 0.39315533004150244\n",
      "iterations 2247 accuracy : 0.9117832388153749  loss : 0.3931212385904197\n",
      "iterations 2248 accuracy : 0.9117832388153749  loss : 0.3930869245168874\n",
      "iterations 2249 accuracy : 0.9117832388153749  loss : 0.3930519714935213\n",
      "iterations 2250 accuracy : 0.9117832388153749  loss : 0.3930163882275473\n",
      "iterations 2251 accuracy : 0.9117832388153749  loss : 0.3929822638598506\n",
      "iterations 2252 accuracy : 0.9117832388153749  loss : 0.3929476903487345\n",
      "iterations 2253 accuracy : 0.9117832388153749  loss : 0.3929134784718339\n",
      "iterations 2254 accuracy : 0.9117832388153749  loss : 0.3928786935610117\n",
      "iterations 2255 accuracy : 0.9117832388153749  loss : 0.3928428985328694\n",
      "iterations 2256 accuracy : 0.9117832388153749  loss : 0.39280869546313596\n",
      "iterations 2257 accuracy : 0.9117832388153749  loss : 0.39277461544229225\n",
      "iterations 2258 accuracy : 0.9117832388153749  loss : 0.3927401131441949\n",
      "iterations 2259 accuracy : 0.9117832388153749  loss : 0.39270503520134725\n",
      "iterations 2260 accuracy : 0.9117832388153749  loss : 0.3926711333902656\n",
      "iterations 2261 accuracy : 0.9117832388153749  loss : 0.3926373026037534\n",
      "iterations 2262 accuracy : 0.9117832388153749  loss : 0.3926024821288136\n",
      "iterations 2263 accuracy : 0.9117832388153749  loss : 0.3925679194640519\n",
      "iterations 2264 accuracy : 0.9117832388153749  loss : 0.3925344395864429\n",
      "iterations 2265 accuracy : 0.9117832388153749  loss : 0.3925009116410459\n",
      "iterations 2266 accuracy : 0.9117832388153749  loss : 0.3924665429305093\n",
      "iterations 2267 accuracy : 0.9117832388153749  loss : 0.39243273912521454\n",
      "iterations 2268 accuracy : 0.9117832388153749  loss : 0.3923992602995236\n",
      "iterations 2269 accuracy : 0.9117832388153749  loss : 0.39236596359843906\n",
      "iterations 2270 accuracy : 0.9117832388153749  loss : 0.39233261867891667\n",
      "iterations 2271 accuracy : 0.9117832388153749  loss : 0.3922996755250188\n",
      "iterations 2272 accuracy : 0.9117832388153749  loss : 0.39226451155711495\n",
      "iterations 2273 accuracy : 0.9117832388153749  loss : 0.3922302098281806\n",
      "iterations 2274 accuracy : 0.9117832388153749  loss : 0.39219470448637755\n",
      "iterations 2275 accuracy : 0.9117832388153749  loss : 0.39216055583775467\n",
      "iterations 2276 accuracy : 0.9117832388153749  loss : 0.3921266285841706\n",
      "iterations 2277 accuracy : 0.9117832388153749  loss : 0.3920926362988181\n",
      "iterations 2278 accuracy : 0.9117832388153749  loss : 0.3920587996395497\n",
      "iterations 2279 accuracy : 0.9124133585381222  loss : 0.392023580040941\n",
      "iterations 2280 accuracy : 0.9126233984457047  loss : 0.3919895251805497\n",
      "iterations 2281 accuracy : 0.9126233984457047  loss : 0.3919554391673875\n",
      "iterations 2282 accuracy : 0.9126233984457047  loss : 0.39192167114854637\n",
      "iterations 2283 accuracy : 0.9128334383532871  loss : 0.39188635878302114\n",
      "iterations 2284 accuracy : 0.9128334383532871  loss : 0.39185187885686124\n",
      "iterations 2285 accuracy : 0.913253518168452  loss : 0.39181707038541475\n",
      "iterations 2286 accuracy : 0.9134635580760344  loss : 0.39178201405221186\n",
      "iterations 2287 accuracy : 0.9134635580760344  loss : 0.39174748652178504\n",
      "iterations 2288 accuracy : 0.9134635580760344  loss : 0.3917126441010986\n",
      "iterations 2289 accuracy : 0.9134635580760344  loss : 0.3916788658921203\n",
      "iterations 2290 accuracy : 0.9134635580760344  loss : 0.39164551087029575\n",
      "iterations 2291 accuracy : 0.9134635580760344  loss : 0.3916117915098417\n",
      "iterations 2292 accuracy : 0.9136735979836169  loss : 0.39157770202926295\n",
      "iterations 2293 accuracy : 0.9134635580760344  loss : 0.3915442954057335\n",
      "iterations 2294 accuracy : 0.9134635580760344  loss : 0.39150984850670156\n",
      "iterations 2295 accuracy : 0.9134635580760344  loss : 0.3914758851856721\n",
      "iterations 2296 accuracy : 0.9134635580760344  loss : 0.391442245057199\n",
      "iterations 2297 accuracy : 0.9136735979836169  loss : 0.3914083305806468\n",
      "iterations 2298 accuracy : 0.9134635580760344  loss : 0.3913749245868521\n",
      "iterations 2299 accuracy : 0.9134635580760344  loss : 0.3913402758982993\n",
      "iterations 2300 accuracy : 0.9136735979836169  loss : 0.391306391675866\n",
      "iterations 2301 accuracy : 0.9138836378911993  loss : 0.3912727900086679\n",
      "iterations 2302 accuracy : 0.9140936777987818  loss : 0.3912377920631646\n",
      "iterations 2303 accuracy : 0.9140936777987818  loss : 0.39120377015780494\n",
      "iterations 2304 accuracy : 0.9140936777987818  loss : 0.3911694362267759\n",
      "iterations 2305 accuracy : 0.9140936777987818  loss : 0.3911358306171467\n",
      "iterations 2306 accuracy : 0.9143037177063642  loss : 0.39110209487691505\n",
      "iterations 2307 accuracy : 0.9143037177063642  loss : 0.39106851869840814\n",
      "iterations 2308 accuracy : 0.9143037177063642  loss : 0.3910350762635741\n",
      "iterations 2309 accuracy : 0.9143037177063642  loss : 0.39100060070900516\n",
      "iterations 2310 accuracy : 0.9143037177063642  loss : 0.3909673163821131\n",
      "iterations 2311 accuracy : 0.9143037177063642  loss : 0.3909321880042762\n",
      "iterations 2312 accuracy : 0.9145137576139466  loss : 0.3908978194088895\n",
      "iterations 2313 accuracy : 0.9143037177063642  loss : 0.3908656592429211\n",
      "iterations 2314 accuracy : 0.9145137576139466  loss : 0.39083190503153176\n",
      "iterations 2315 accuracy : 0.9143037177063642  loss : 0.3907990407791183\n",
      "iterations 2316 accuracy : 0.9143037177063642  loss : 0.3907654314270611\n",
      "iterations 2317 accuracy : 0.9145137576139466  loss : 0.3907314735511891\n",
      "iterations 2318 accuracy : 0.9143037177063642  loss : 0.390698783294196\n",
      "iterations 2319 accuracy : 0.9143037177063642  loss : 0.3906652780547482\n",
      "iterations 2320 accuracy : 0.9143037177063642  loss : 0.39063300258600314\n",
      "iterations 2321 accuracy : 0.9143037177063642  loss : 0.3905999121109136\n",
      "iterations 2322 accuracy : 0.9143037177063642  loss : 0.39056551495972897\n",
      "iterations 2323 accuracy : 0.9143037177063642  loss : 0.3905321116157758\n",
      "iterations 2324 accuracy : 0.9143037177063642  loss : 0.3904988058833312\n",
      "iterations 2325 accuracy : 0.9143037177063642  loss : 0.39046617590741883\n",
      "iterations 2326 accuracy : 0.9143037177063642  loss : 0.3904337274221283\n",
      "iterations 2327 accuracy : 0.9143037177063642  loss : 0.3904019102042553\n",
      "iterations 2328 accuracy : 0.9143037177063642  loss : 0.39036823555177963\n",
      "iterations 2329 accuracy : 0.9143037177063642  loss : 0.3903350992034439\n",
      "iterations 2330 accuracy : 0.9143037177063642  loss : 0.39030104412432587\n",
      "iterations 2331 accuracy : 0.9143037177063642  loss : 0.3902679731412156\n",
      "iterations 2332 accuracy : 0.9145137576139466  loss : 0.39023382279406044\n",
      "iterations 2333 accuracy : 0.9145137576139466  loss : 0.3902011435103199\n",
      "iterations 2334 accuracy : 0.9145137576139466  loss : 0.39016927863963746\n",
      "iterations 2335 accuracy : 0.9145137576139466  loss : 0.390135854386808\n",
      "iterations 2336 accuracy : 0.9145137576139466  loss : 0.3901033115289903\n",
      "iterations 2337 accuracy : 0.9145137576139466  loss : 0.390069898275569\n",
      "iterations 2338 accuracy : 0.9145137576139466  loss : 0.3900357047157689\n",
      "iterations 2339 accuracy : 0.9145137576139466  loss : 0.39000244999163364\n",
      "iterations 2340 accuracy : 0.9145137576139466  loss : 0.3899698099463671\n",
      "iterations 2341 accuracy : 0.9145137576139466  loss : 0.38993594218558114\n",
      "iterations 2342 accuracy : 0.9147237975215291  loss : 0.3899018022717449\n",
      "iterations 2343 accuracy : 0.9147237975215291  loss : 0.38986875741134763\n",
      "iterations 2344 accuracy : 0.9147237975215291  loss : 0.3898351026718342\n",
      "iterations 2345 accuracy : 0.9145137576139466  loss : 0.3898031621892595\n",
      "iterations 2346 accuracy : 0.9145137576139466  loss : 0.3897701409529722\n",
      "iterations 2347 accuracy : 0.9145137576139466  loss : 0.389736659337191\n",
      "iterations 2348 accuracy : 0.9149338374291115  loss : 0.38970377568505044\n",
      "iterations 2349 accuracy : 0.9147237975215291  loss : 0.3896699536555868\n",
      "iterations 2350 accuracy : 0.915143877336694  loss : 0.38963586490676005\n",
      "iterations 2351 accuracy : 0.915143877336694  loss : 0.3896035323945051\n",
      "iterations 2352 accuracy : 0.9155639571518589  loss : 0.3895694563261389\n",
      "iterations 2353 accuracy : 0.9153539172442764  loss : 0.38953699029576677\n",
      "iterations 2354 accuracy : 0.9155639571518589  loss : 0.3895042883036125\n",
      "iterations 2355 accuracy : 0.9155639571518589  loss : 0.38947174106532434\n",
      "iterations 2356 accuracy : 0.9155639571518589  loss : 0.38943921211679255\n",
      "iterations 2357 accuracy : 0.9155639571518589  loss : 0.3894054723172168\n",
      "iterations 2358 accuracy : 0.9155639571518589  loss : 0.38937258959278714\n",
      "iterations 2359 accuracy : 0.9155639571518589  loss : 0.38933900543675853\n",
      "iterations 2360 accuracy : 0.9157739970594413  loss : 0.38930653691989364\n",
      "iterations 2361 accuracy : 0.9157739970594413  loss : 0.3892743951388396\n",
      "iterations 2362 accuracy : 0.9159840369670237  loss : 0.3892401502886934\n",
      "iterations 2363 accuracy : 0.9159840369670237  loss : 0.3892072331813622\n",
      "iterations 2364 accuracy : 0.9161940768746062  loss : 0.3891748557786236\n",
      "iterations 2365 accuracy : 0.9161940768746062  loss : 0.3891410932479759\n",
      "iterations 2366 accuracy : 0.9161940768746062  loss : 0.389109355895103\n",
      "iterations 2367 accuracy : 0.9161940768746062  loss : 0.3890774066007389\n",
      "iterations 2368 accuracy : 0.9159840369670237  loss : 0.3890451234518191\n",
      "iterations 2369 accuracy : 0.9159840369670237  loss : 0.389013372526742\n",
      "iterations 2370 accuracy : 0.9161940768746062  loss : 0.3889803012013763\n",
      "iterations 2371 accuracy : 0.9161940768746062  loss : 0.3889474172993126\n",
      "iterations 2372 accuracy : 0.9161940768746062  loss : 0.38891373595545453\n",
      "iterations 2373 accuracy : 0.9164041167821886  loss : 0.3888813625882498\n",
      "iterations 2374 accuracy : 0.9164041167821886  loss : 0.38884810625544025\n",
      "iterations 2375 accuracy : 0.9164041167821886  loss : 0.3888148124683388\n",
      "iterations 2376 accuracy : 0.9164041167821886  loss : 0.38878157567873234\n",
      "iterations 2377 accuracy : 0.9164041167821886  loss : 0.38874905358987116\n",
      "iterations 2378 accuracy : 0.9164041167821886  loss : 0.3887177630398513\n",
      "iterations 2379 accuracy : 0.9164041167821886  loss : 0.3886858400906508\n",
      "iterations 2380 accuracy : 0.9164041167821886  loss : 0.3886522848099242\n",
      "iterations 2381 accuracy : 0.9164041167821886  loss : 0.38862024323131644\n",
      "iterations 2382 accuracy : 0.9166141566897711  loss : 0.38858652602771826\n",
      "iterations 2383 accuracy : 0.9166141566897711  loss : 0.38855375538832515\n",
      "iterations 2384 accuracy : 0.9168241965973535  loss : 0.388521330604714\n",
      "iterations 2385 accuracy : 0.9168241965973535  loss : 0.3884889268482474\n",
      "iterations 2386 accuracy : 0.9168241965973535  loss : 0.3884559585634558\n",
      "iterations 2387 accuracy : 0.9168241965973535  loss : 0.38842349804092846\n",
      "iterations 2388 accuracy : 0.9168241965973535  loss : 0.3883903355169614\n",
      "iterations 2389 accuracy : 0.9168241965973535  loss : 0.3883597333274002\n",
      "iterations 2390 accuracy : 0.9168241965973535  loss : 0.38832768626074976\n",
      "iterations 2391 accuracy : 0.917034236504936  loss : 0.3882942326652677\n",
      "iterations 2392 accuracy : 0.9168241965973535  loss : 0.38826257259415775\n",
      "iterations 2393 accuracy : 0.9168241965973535  loss : 0.38822934368658707\n",
      "iterations 2394 accuracy : 0.9168241965973535  loss : 0.3881978021213855\n",
      "iterations 2395 accuracy : 0.9168241965973535  loss : 0.38816660403577913\n",
      "iterations 2396 accuracy : 0.917034236504936  loss : 0.3881344359361179\n",
      "iterations 2397 accuracy : 0.9168241965973535  loss : 0.3881024002278404\n",
      "iterations 2398 accuracy : 0.9166141566897711  loss : 0.38807076521265704\n",
      "iterations 2399 accuracy : 0.9168241965973535  loss : 0.3880380137263486\n",
      "iterations 2400 accuracy : 0.9168241965973535  loss : 0.3880051510153098\n",
      "iterations 2401 accuracy : 0.9168241965973535  loss : 0.3879726098821421\n",
      "iterations 2402 accuracy : 0.917034236504936  loss : 0.3879393913694769\n",
      "iterations 2403 accuracy : 0.917034236504936  loss : 0.3879063216457492\n",
      "iterations 2404 accuracy : 0.9172442764125184  loss : 0.3878748956923731\n",
      "iterations 2405 accuracy : 0.9172442764125184  loss : 0.3878424953298739\n",
      "iterations 2406 accuracy : 0.9172442764125184  loss : 0.3878100308547373\n",
      "iterations 2407 accuracy : 0.9172442764125184  loss : 0.3877774042667745\n",
      "iterations 2408 accuracy : 0.9172442764125184  loss : 0.3877441677253221\n",
      "iterations 2409 accuracy : 0.9172442764125184  loss : 0.38771312354489873\n",
      "iterations 2410 accuracy : 0.9172442764125184  loss : 0.3876807482448091\n",
      "iterations 2411 accuracy : 0.9172442764125184  loss : 0.3876475683894146\n",
      "iterations 2412 accuracy : 0.9172442764125184  loss : 0.3876162515227333\n",
      "iterations 2413 accuracy : 0.9172442764125184  loss : 0.387584230032661\n",
      "iterations 2414 accuracy : 0.9172442764125184  loss : 0.3875526446456504\n",
      "iterations 2415 accuracy : 0.9172442764125184  loss : 0.3875206777989447\n",
      "iterations 2416 accuracy : 0.9172442764125184  loss : 0.3874889080278217\n",
      "iterations 2417 accuracy : 0.9172442764125184  loss : 0.3874576177678443\n",
      "iterations 2418 accuracy : 0.9172442764125184  loss : 0.3874267176372244\n",
      "iterations 2419 accuracy : 0.9172442764125184  loss : 0.3873960759252625\n",
      "iterations 2420 accuracy : 0.9172442764125184  loss : 0.3873638582875435\n",
      "iterations 2421 accuracy : 0.9172442764125184  loss : 0.38733275553729646\n",
      "iterations 2422 accuracy : 0.9172442764125184  loss : 0.38730039209835665\n",
      "iterations 2423 accuracy : 0.9172442764125184  loss : 0.3872687398120647\n",
      "iterations 2424 accuracy : 0.9172442764125184  loss : 0.38723595369048275\n",
      "iterations 2425 accuracy : 0.9172442764125184  loss : 0.3872043737986226\n",
      "iterations 2426 accuracy : 0.9172442764125184  loss : 0.38717254970916126\n",
      "iterations 2427 accuracy : 0.9172442764125184  loss : 0.38713953727384015\n",
      "iterations 2428 accuracy : 0.9172442764125184  loss : 0.3871059697127708\n",
      "iterations 2429 accuracy : 0.9172442764125184  loss : 0.38707517161974153\n",
      "iterations 2430 accuracy : 0.9172442764125184  loss : 0.3870432840262712\n",
      "iterations 2431 accuracy : 0.9172442764125184  loss : 0.3870118281784196\n",
      "iterations 2432 accuracy : 0.9172442764125184  loss : 0.3869804299692869\n",
      "iterations 2433 accuracy : 0.9172442764125184  loss : 0.3869483279100616\n",
      "iterations 2434 accuracy : 0.9172442764125184  loss : 0.38691676325012236\n",
      "iterations 2435 accuracy : 0.9174543163201008  loss : 0.38688456948217076\n",
      "iterations 2436 accuracy : 0.9174543163201008  loss : 0.386852084419332\n",
      "iterations 2437 accuracy : 0.9174543163201008  loss : 0.3868207974368418\n",
      "iterations 2438 accuracy : 0.9174543163201008  loss : 0.38678919064571476\n",
      "iterations 2439 accuracy : 0.9176643562276833  loss : 0.38675790374320845\n",
      "iterations 2440 accuracy : 0.9176643562276833  loss : 0.38672580319210864\n",
      "iterations 2441 accuracy : 0.9174543163201008  loss : 0.38669469763883285\n",
      "iterations 2442 accuracy : 0.9176643562276833  loss : 0.38666258454143576\n",
      "iterations 2443 accuracy : 0.9176643562276833  loss : 0.38663070059780064\n",
      "iterations 2444 accuracy : 0.9176643562276833  loss : 0.38659858328934943\n",
      "iterations 2445 accuracy : 0.9178743961352657  loss : 0.38656632824254067\n",
      "iterations 2446 accuracy : 0.9178743961352657  loss : 0.3865346897781223\n",
      "iterations 2447 accuracy : 0.9178743961352657  loss : 0.38650396412110444\n",
      "iterations 2448 accuracy : 0.9176643562276833  loss : 0.38647329583313705\n",
      "iterations 2449 accuracy : 0.9176643562276833  loss : 0.3864416203674505\n",
      "iterations 2450 accuracy : 0.9178743961352657  loss : 0.3864105271748445\n",
      "iterations 2451 accuracy : 0.9178743961352657  loss : 0.38637962147433014\n",
      "iterations 2452 accuracy : 0.9180844360428482  loss : 0.38634746493875716\n",
      "iterations 2453 accuracy : 0.9180844360428482  loss : 0.386316935578966\n",
      "iterations 2454 accuracy : 0.9180844360428482  loss : 0.38628472412980225\n",
      "iterations 2455 accuracy : 0.9180844360428482  loss : 0.3862533824605162\n",
      "iterations 2456 accuracy : 0.9180844360428482  loss : 0.3862213175100632\n",
      "iterations 2457 accuracy : 0.9178743961352657  loss : 0.3861902719771508\n",
      "iterations 2458 accuracy : 0.9180844360428482  loss : 0.3861579388483324\n",
      "iterations 2459 accuracy : 0.9180844360428482  loss : 0.3861263485569856\n",
      "iterations 2460 accuracy : 0.9180844360428482  loss : 0.3860954121164325\n",
      "iterations 2461 accuracy : 0.9182944759504306  loss : 0.3860638950235938\n",
      "iterations 2462 accuracy : 0.9185045158580131  loss : 0.3860324428924713\n",
      "iterations 2463 accuracy : 0.9185045158580131  loss : 0.38600078978829344\n",
      "iterations 2464 accuracy : 0.9185045158580131  loss : 0.38596978341065447\n",
      "iterations 2465 accuracy : 0.9185045158580131  loss : 0.3859386268966205\n",
      "iterations 2466 accuracy : 0.9185045158580131  loss : 0.38590906557746024\n",
      "iterations 2467 accuracy : 0.9182944759504306  loss : 0.3858792472113746\n",
      "iterations 2468 accuracy : 0.9182944759504306  loss : 0.3858481496093822\n",
      "iterations 2469 accuracy : 0.9185045158580131  loss : 0.3858164499688382\n",
      "iterations 2470 accuracy : 0.9182944759504306  loss : 0.3857872359959994\n",
      "iterations 2471 accuracy : 0.9182944759504306  loss : 0.3857558267709281\n",
      "iterations 2472 accuracy : 0.9182944759504306  loss : 0.3857264002984015\n",
      "iterations 2473 accuracy : 0.9180844360428482  loss : 0.38569563663556516\n",
      "iterations 2474 accuracy : 0.9182944759504306  loss : 0.38566395100491924\n",
      "iterations 2475 accuracy : 0.9182944759504306  loss : 0.38563293464400256\n",
      "iterations 2476 accuracy : 0.9182944759504306  loss : 0.38560153479928627\n",
      "iterations 2477 accuracy : 0.9185045158580131  loss : 0.38556936611153236\n",
      "iterations 2478 accuracy : 0.9182944759504306  loss : 0.38553997847765564\n",
      "iterations 2479 accuracy : 0.9182944759504306  loss : 0.3855087979488697\n",
      "iterations 2480 accuracy : 0.9182944759504306  loss : 0.38547768933203336\n",
      "iterations 2481 accuracy : 0.9182944759504306  loss : 0.38544705709950117\n",
      "iterations 2482 accuracy : 0.9182944759504306  loss : 0.3854156557313127\n",
      "iterations 2483 accuracy : 0.9185045158580131  loss : 0.38538298509537083\n",
      "iterations 2484 accuracy : 0.9185045158580131  loss : 0.38535284945439474\n",
      "iterations 2485 accuracy : 0.9187145557655955  loss : 0.38532041941899137\n",
      "iterations 2486 accuracy : 0.9185045158580131  loss : 0.38528978116585705\n",
      "iterations 2487 accuracy : 0.9187145557655955  loss : 0.3852592882358595\n",
      "iterations 2488 accuracy : 0.9187145557655955  loss : 0.38522758575800087\n",
      "iterations 2489 accuracy : 0.9187145557655955  loss : 0.38519795048774746\n",
      "iterations 2490 accuracy : 0.9187145557655955  loss : 0.3851662561160863\n",
      "iterations 2491 accuracy : 0.9187145557655955  loss : 0.3851361073376705\n",
      "iterations 2492 accuracy : 0.9187145557655955  loss : 0.38510503091418913\n",
      "iterations 2493 accuracy : 0.9187145557655955  loss : 0.3850744602637241\n",
      "iterations 2494 accuracy : 0.9187145557655955  loss : 0.3850445578318873\n",
      "iterations 2495 accuracy : 0.9189245956731779  loss : 0.38501281402693815\n",
      "iterations 2496 accuracy : 0.9189245956731779  loss : 0.38498211923053505\n",
      "iterations 2497 accuracy : 0.9189245956731779  loss : 0.3849511852202657\n",
      "iterations 2498 accuracy : 0.9187145557655955  loss : 0.3849201597328886\n",
      "iterations 2499 accuracy : 0.9187145557655955  loss : 0.3848902550533899\n",
      "iterations 2500 accuracy : 0.9187145557655955  loss : 0.38485850829522883\n",
      "iterations 2501 accuracy : 0.9187145557655955  loss : 0.38482842401293776\n",
      "iterations 2502 accuracy : 0.9187145557655955  loss : 0.3847981046665021\n",
      "iterations 2503 accuracy : 0.9187145557655955  loss : 0.38476787256823997\n",
      "iterations 2504 accuracy : 0.9187145557655955  loss : 0.3847380912376036\n",
      "iterations 2505 accuracy : 0.9187145557655955  loss : 0.3847078745517116\n",
      "iterations 2506 accuracy : 0.9187145557655955  loss : 0.3846778759596041\n",
      "iterations 2507 accuracy : 0.9187145557655955  loss : 0.38464649472977375\n",
      "iterations 2508 accuracy : 0.9187145557655955  loss : 0.3846158401811807\n",
      "iterations 2509 accuracy : 0.9187145557655955  loss : 0.3845865575879746\n",
      "iterations 2510 accuracy : 0.9189245956731779  loss : 0.38455589446412825\n",
      "iterations 2511 accuracy : 0.9191346355807604  loss : 0.3845253718732857\n",
      "iterations 2512 accuracy : 0.9191346355807604  loss : 0.3844944073217942\n",
      "iterations 2513 accuracy : 0.9191346355807604  loss : 0.38446402050537\n",
      "iterations 2514 accuracy : 0.9191346355807604  loss : 0.38443368145349804\n",
      "iterations 2515 accuracy : 0.9191346355807604  loss : 0.38440283826224675\n",
      "iterations 2516 accuracy : 0.9191346355807604  loss : 0.38437246762217436\n",
      "iterations 2517 accuracy : 0.9191346355807604  loss : 0.3843410610239636\n",
      "iterations 2518 accuracy : 0.9191346355807604  loss : 0.3843110493605718\n",
      "iterations 2519 accuracy : 0.9191346355807604  loss : 0.3842801355551891\n",
      "iterations 2520 accuracy : 0.9191346355807604  loss : 0.38424988535148047\n",
      "iterations 2521 accuracy : 0.9191346355807604  loss : 0.3842198575644458\n",
      "iterations 2522 accuracy : 0.9191346355807604  loss : 0.38418923074320754\n",
      "iterations 2523 accuracy : 0.9191346355807604  loss : 0.38415913855432066\n",
      "iterations 2524 accuracy : 0.9191346355807604  loss : 0.38412835226796366\n",
      "iterations 2525 accuracy : 0.9191346355807604  loss : 0.38409789118158383\n",
      "iterations 2526 accuracy : 0.9191346355807604  loss : 0.3840673272610419\n",
      "iterations 2527 accuracy : 0.9191346355807604  loss : 0.38403758328322213\n",
      "iterations 2528 accuracy : 0.9191346355807604  loss : 0.3840083693731452\n",
      "iterations 2529 accuracy : 0.9191346355807604  loss : 0.38397846392869067\n",
      "iterations 2530 accuracy : 0.9191346355807604  loss : 0.3839473811765633\n",
      "iterations 2531 accuracy : 0.9191346355807604  loss : 0.3839173840859763\n",
      "iterations 2532 accuracy : 0.9191346355807604  loss : 0.3838869438872565\n",
      "iterations 2533 accuracy : 0.9191346355807604  loss : 0.3838570248069347\n",
      "iterations 2534 accuracy : 0.9191346355807604  loss : 0.3838259598277967\n",
      "iterations 2535 accuracy : 0.9193446754883428  loss : 0.3837958252208375\n",
      "iterations 2536 accuracy : 0.9193446754883428  loss : 0.3837653238385352\n",
      "iterations 2537 accuracy : 0.9193446754883428  loss : 0.3837347702802816\n",
      "iterations 2538 accuracy : 0.9193446754883428  loss : 0.38370424429139127\n",
      "iterations 2539 accuracy : 0.9193446754883428  loss : 0.38367478801814053\n",
      "iterations 2540 accuracy : 0.9193446754883428  loss : 0.38364493381754383\n",
      "iterations 2541 accuracy : 0.9193446754883428  loss : 0.3836151951376683\n",
      "iterations 2542 accuracy : 0.9193446754883428  loss : 0.38358497184903784\n",
      "iterations 2543 accuracy : 0.9193446754883428  loss : 0.3835550268474639\n",
      "iterations 2544 accuracy : 0.9193446754883428  loss : 0.3835252331142618\n",
      "iterations 2545 accuracy : 0.9193446754883428  loss : 0.38349555248490474\n",
      "iterations 2546 accuracy : 0.9193446754883428  loss : 0.3834652744912804\n",
      "iterations 2547 accuracy : 0.9193446754883428  loss : 0.3834347597275928\n",
      "iterations 2548 accuracy : 0.9193446754883428  loss : 0.38340487217848723\n",
      "iterations 2549 accuracy : 0.9193446754883428  loss : 0.3833744230460805\n",
      "iterations 2550 accuracy : 0.9193446754883428  loss : 0.3833442591054466\n",
      "iterations 2551 accuracy : 0.9193446754883428  loss : 0.38331393787718937\n",
      "iterations 2552 accuracy : 0.9195547153959253  loss : 0.38328412966308434\n",
      "iterations 2553 accuracy : 0.9195547153959253  loss : 0.38325468493321424\n",
      "iterations 2554 accuracy : 0.9195547153959253  loss : 0.3832246299771766\n",
      "iterations 2555 accuracy : 0.9195547153959253  loss : 0.3831937736339745\n",
      "iterations 2556 accuracy : 0.9195547153959253  loss : 0.38316320387875874\n",
      "iterations 2557 accuracy : 0.9195547153959253  loss : 0.38313518247913564\n",
      "iterations 2558 accuracy : 0.9195547153959253  loss : 0.3831056998261364\n",
      "iterations 2559 accuracy : 0.9193446754883428  loss : 0.38307665802424584\n",
      "iterations 2560 accuracy : 0.9195547153959253  loss : 0.38304612182083736\n",
      "iterations 2561 accuracy : 0.9195547153959253  loss : 0.38301514598951414\n",
      "iterations 2562 accuracy : 0.9199747952110902  loss : 0.38298416069372426\n",
      "iterations 2563 accuracy : 0.9199747952110902  loss : 0.38295424084952584\n",
      "iterations 2564 accuracy : 0.9199747952110902  loss : 0.3829240516509882\n",
      "iterations 2565 accuracy : 0.9197647553035077  loss : 0.3828935495165616\n",
      "iterations 2566 accuracy : 0.9199747952110902  loss : 0.3828634512868143\n",
      "iterations 2567 accuracy : 0.9199747952110902  loss : 0.3828338684638187\n",
      "iterations 2568 accuracy : 0.9199747952110902  loss : 0.3828035689697115\n",
      "iterations 2569 accuracy : 0.9199747952110902  loss : 0.3827739445969836\n",
      "iterations 2570 accuracy : 0.9199747952110902  loss : 0.3827433659406701\n",
      "iterations 2571 accuracy : 0.9199747952110902  loss : 0.3827144437190749\n",
      "iterations 2572 accuracy : 0.9199747952110902  loss : 0.3826855925496317\n",
      "iterations 2573 accuracy : 0.9199747952110902  loss : 0.3826557203830453\n",
      "iterations 2574 accuracy : 0.9199747952110902  loss : 0.38262630968094236\n",
      "iterations 2575 accuracy : 0.9201848351186726  loss : 0.38259668950550857\n",
      "iterations 2576 accuracy : 0.9201848351186726  loss : 0.3825664619353222\n",
      "iterations 2577 accuracy : 0.9201848351186726  loss : 0.38253622312674296\n",
      "iterations 2578 accuracy : 0.9201848351186726  loss : 0.3825064416462135\n",
      "iterations 2579 accuracy : 0.9201848351186726  loss : 0.3824772512370996\n",
      "iterations 2580 accuracy : 0.9201848351186726  loss : 0.3824479651729189\n",
      "iterations 2581 accuracy : 0.9201848351186726  loss : 0.3824185652658405\n",
      "iterations 2582 accuracy : 0.9203948750262549  loss : 0.3823879847039004\n",
      "iterations 2583 accuracy : 0.9208149548414198  loss : 0.38235845786222156\n",
      "iterations 2584 accuracy : 0.9208149548414198  loss : 0.38232829285385905\n",
      "iterations 2585 accuracy : 0.9208149548414198  loss : 0.382299515901986\n",
      "iterations 2586 accuracy : 0.9208149548414198  loss : 0.38226959455768506\n",
      "iterations 2587 accuracy : 0.9208149548414198  loss : 0.3822397023132768\n",
      "iterations 2588 accuracy : 0.9208149548414198  loss : 0.3822096542927474\n",
      "iterations 2589 accuracy : 0.9208149548414198  loss : 0.38217917261367923\n",
      "iterations 2590 accuracy : 0.9208149548414198  loss : 0.38214953579644806\n",
      "iterations 2591 accuracy : 0.9210249947490023  loss : 0.3821191737918553\n",
      "iterations 2592 accuracy : 0.9208149548414198  loss : 0.3820903184004357\n",
      "iterations 2593 accuracy : 0.9210249947490023  loss : 0.38206002599923733\n",
      "iterations 2594 accuracy : 0.9210249947490023  loss : 0.38203028707761777\n",
      "iterations 2595 accuracy : 0.9210249947490023  loss : 0.3820007192811148\n",
      "iterations 2596 accuracy : 0.9208149548414198  loss : 0.3819718786165781\n",
      "iterations 2597 accuracy : 0.9208149548414198  loss : 0.3819434604535027\n",
      "iterations 2598 accuracy : 0.9208149548414198  loss : 0.3819136641891753\n",
      "iterations 2599 accuracy : 0.9208149548414198  loss : 0.38188437158620453\n",
      "iterations 2600 accuracy : 0.9208149548414198  loss : 0.3818561020401368\n",
      "iterations 2601 accuracy : 0.9208149548414198  loss : 0.38182775585554035\n",
      "iterations 2602 accuracy : 0.9210249947490023  loss : 0.3817982381969719\n",
      "iterations 2603 accuracy : 0.9210249947490023  loss : 0.3817688760337005\n",
      "iterations 2604 accuracy : 0.9210249947490023  loss : 0.38173992374955806\n",
      "iterations 2605 accuracy : 0.9212350346565847  loss : 0.3817104319600798\n",
      "iterations 2606 accuracy : 0.9210249947490023  loss : 0.38168288909045495\n",
      "iterations 2607 accuracy : 0.9208149548414198  loss : 0.3816550494275576\n",
      "iterations 2608 accuracy : 0.9208149548414198  loss : 0.38162621564803956\n",
      "iterations 2609 accuracy : 0.9208149548414198  loss : 0.38159698140895976\n",
      "iterations 2610 accuracy : 0.9210249947490023  loss : 0.3815674079987786\n",
      "iterations 2611 accuracy : 0.9212350346565847  loss : 0.3815376716397083\n",
      "iterations 2612 accuracy : 0.9212350346565847  loss : 0.38150865441226733\n",
      "iterations 2613 accuracy : 0.9214450745641672  loss : 0.3814792763418629\n",
      "iterations 2614 accuracy : 0.9214450745641672  loss : 0.3814503288351714\n",
      "iterations 2615 accuracy : 0.9214450745641672  loss : 0.38142204700514876\n",
      "iterations 2616 accuracy : 0.9214450745641672  loss : 0.3813931944338192\n",
      "iterations 2617 accuracy : 0.9214450745641672  loss : 0.38136397169890335\n",
      "iterations 2618 accuracy : 0.9214450745641672  loss : 0.3813353113161728\n",
      "iterations 2619 accuracy : 0.9214450745641672  loss : 0.3813060282810155\n",
      "iterations 2620 accuracy : 0.9214450745641672  loss : 0.38127674715029114\n",
      "iterations 2621 accuracy : 0.9214450745641672  loss : 0.3812472364948015\n",
      "iterations 2622 accuracy : 0.9214450745641672  loss : 0.38121794037848183\n",
      "iterations 2623 accuracy : 0.9214450745641672  loss : 0.38118858506474446\n",
      "iterations 2624 accuracy : 0.9214450745641672  loss : 0.38115954032036903\n",
      "iterations 2625 accuracy : 0.9214450745641672  loss : 0.3811307134501058\n",
      "iterations 2626 accuracy : 0.9214450745641672  loss : 0.381101538341384\n",
      "iterations 2627 accuracy : 0.9214450745641672  loss : 0.3810726100567768\n",
      "iterations 2628 accuracy : 0.9214450745641672  loss : 0.38104358764721685\n",
      "iterations 2629 accuracy : 0.9214450745641672  loss : 0.3810145286103658\n",
      "iterations 2630 accuracy : 0.9212350346565847  loss : 0.38098576785515237\n",
      "iterations 2631 accuracy : 0.9212350346565847  loss : 0.38095660179406077\n",
      "iterations 2632 accuracy : 0.9212350346565847  loss : 0.38092839866052575\n",
      "iterations 2633 accuracy : 0.9212350346565847  loss : 0.380899589748465\n",
      "iterations 2634 accuracy : 0.9212350346565847  loss : 0.38087007989049815\n",
      "iterations 2635 accuracy : 0.9212350346565847  loss : 0.3808410229480495\n",
      "iterations 2636 accuracy : 0.9214450745641672  loss : 0.38081196638533354\n",
      "iterations 2637 accuracy : 0.9214450745641672  loss : 0.38078329023249524\n",
      "iterations 2638 accuracy : 0.9214450745641672  loss : 0.3807548190339001\n",
      "iterations 2639 accuracy : 0.9214450745641672  loss : 0.3807244182182889\n",
      "iterations 2640 accuracy : 0.9214450745641672  loss : 0.3806953780997155\n",
      "iterations 2641 accuracy : 0.9214450745641672  loss : 0.3806662057710675\n",
      "iterations 2642 accuracy : 0.9212350346565847  loss : 0.38063766635772917\n",
      "iterations 2643 accuracy : 0.9214450745641672  loss : 0.3806095833855025\n",
      "iterations 2644 accuracy : 0.9212350346565847  loss : 0.38058059022020296\n",
      "iterations 2645 accuracy : 0.9214450745641672  loss : 0.3805524854748281\n",
      "iterations 2646 accuracy : 0.9214450745641672  loss : 0.3805238287862196\n",
      "iterations 2647 accuracy : 0.9212350346565847  loss : 0.380495313382135\n",
      "iterations 2648 accuracy : 0.9212350346565847  loss : 0.38046480486647305\n",
      "iterations 2649 accuracy : 0.9212350346565847  loss : 0.3804362825643664\n",
      "iterations 2650 accuracy : 0.9216551144717496  loss : 0.38040638429009177\n",
      "iterations 2651 accuracy : 0.9216551144717496  loss : 0.3803775278766231\n",
      "iterations 2652 accuracy : 0.9216551144717496  loss : 0.38034867342867323\n",
      "iterations 2653 accuracy : 0.9216551144717496  loss : 0.38032022862625997\n",
      "iterations 2654 accuracy : 0.9216551144717496  loss : 0.380290762845015\n",
      "iterations 2655 accuracy : 0.921865154379332  loss : 0.38026268352784115\n",
      "iterations 2656 accuracy : 0.9220751942869145  loss : 0.38023399890743914\n",
      "iterations 2657 accuracy : 0.9220751942869145  loss : 0.38020517579733437\n",
      "iterations 2658 accuracy : 0.9222852341944969  loss : 0.3801761146762791\n",
      "iterations 2659 accuracy : 0.9222852341944969  loss : 0.3801473718221853\n",
      "iterations 2660 accuracy : 0.9220751942869145  loss : 0.3801178557198157\n",
      "iterations 2661 accuracy : 0.9220751942869145  loss : 0.38008929037376854\n",
      "iterations 2662 accuracy : 0.9220751942869145  loss : 0.3800601004256725\n",
      "iterations 2663 accuracy : 0.9220751942869145  loss : 0.38003165526670935\n",
      "iterations 2664 accuracy : 0.9220751942869145  loss : 0.3800039862384889\n",
      "iterations 2665 accuracy : 0.9220751942869145  loss : 0.37997514855636966\n",
      "iterations 2666 accuracy : 0.9220751942869145  loss : 0.3799459135147089\n",
      "iterations 2667 accuracy : 0.9220751942869145  loss : 0.3799172857334306\n",
      "iterations 2668 accuracy : 0.9220751942869145  loss : 0.37988898863044546\n",
      "iterations 2669 accuracy : 0.9220751942869145  loss : 0.3798600569579928\n",
      "iterations 2670 accuracy : 0.9220751942869145  loss : 0.37983163735029163\n",
      "iterations 2671 accuracy : 0.9220751942869145  loss : 0.3798038721002407\n",
      "iterations 2672 accuracy : 0.9220751942869145  loss : 0.3797754098803759\n",
      "iterations 2673 accuracy : 0.9220751942869145  loss : 0.37974644932119644\n",
      "iterations 2674 accuracy : 0.9220751942869145  loss : 0.3797170298007219\n",
      "iterations 2675 accuracy : 0.9224952741020794  loss : 0.37968831603741776\n",
      "iterations 2676 accuracy : 0.9224952741020794  loss : 0.3796598526898779\n",
      "iterations 2677 accuracy : 0.9224952741020794  loss : 0.379630766637624\n",
      "iterations 2678 accuracy : 0.9224952741020794  loss : 0.3796027412758632\n",
      "iterations 2679 accuracy : 0.9224952741020794  loss : 0.37957441762588473\n",
      "iterations 2680 accuracy : 0.9224952741020794  loss : 0.37954605587507545\n",
      "iterations 2681 accuracy : 0.9224952741020794  loss : 0.37951827431152596\n",
      "iterations 2682 accuracy : 0.9224952741020794  loss : 0.3794906749950673\n",
      "iterations 2683 accuracy : 0.9224952741020794  loss : 0.379462114872538\n",
      "iterations 2684 accuracy : 0.9224952741020794  loss : 0.379434150414003\n",
      "iterations 2685 accuracy : 0.9224952741020794  loss : 0.37940548444346517\n",
      "iterations 2686 accuracy : 0.9224952741020794  loss : 0.3793777811636376\n",
      "iterations 2687 accuracy : 0.9224952741020794  loss : 0.379349431758583\n",
      "iterations 2688 accuracy : 0.9224952741020794  loss : 0.3793214400576418\n",
      "iterations 2689 accuracy : 0.9224952741020794  loss : 0.37929256507341136\n",
      "iterations 2690 accuracy : 0.9224952741020794  loss : 0.3792646086201299\n",
      "iterations 2691 accuracy : 0.9224952741020794  loss : 0.379236953957176\n",
      "iterations 2692 accuracy : 0.9224952741020794  loss : 0.3792095700406226\n",
      "iterations 2693 accuracy : 0.9227053140096618  loss : 0.37918120259621063\n",
      "iterations 2694 accuracy : 0.9227053140096618  loss : 0.37915299206431524\n",
      "iterations 2695 accuracy : 0.9227053140096618  loss : 0.3791247660199888\n",
      "iterations 2696 accuracy : 0.9229153539172443  loss : 0.3790970717154539\n",
      "iterations 2697 accuracy : 0.9229153539172443  loss : 0.37906819983646034\n",
      "iterations 2698 accuracy : 0.9227053140096618  loss : 0.37903939935636166\n",
      "iterations 2699 accuracy : 0.9227053140096618  loss : 0.3790109001295726\n",
      "iterations 2700 accuracy : 0.9227053140096618  loss : 0.3789829934849612\n",
      "iterations 2701 accuracy : 0.9227053140096618  loss : 0.37895443391690786\n",
      "iterations 2702 accuracy : 0.9227053140096618  loss : 0.3789259720366294\n",
      "iterations 2703 accuracy : 0.9229153539172443  loss : 0.3788982015135814\n",
      "iterations 2704 accuracy : 0.9229153539172443  loss : 0.3788702079275574\n",
      "iterations 2705 accuracy : 0.9229153539172443  loss : 0.3788420727813866\n",
      "iterations 2706 accuracy : 0.9229153539172443  loss : 0.37881359747204013\n",
      "iterations 2707 accuracy : 0.9229153539172443  loss : 0.378785914695728\n",
      "iterations 2708 accuracy : 0.9229153539172443  loss : 0.37875751511433775\n",
      "iterations 2709 accuracy : 0.9229153539172443  loss : 0.37872908083559753\n",
      "iterations 2710 accuracy : 0.9231253938248267  loss : 0.37870139585496804\n",
      "iterations 2711 accuracy : 0.9229153539172443  loss : 0.3786734121176497\n",
      "iterations 2712 accuracy : 0.9231253938248267  loss : 0.378645903846502\n",
      "iterations 2713 accuracy : 0.9229153539172443  loss : 0.37861788164421656\n",
      "iterations 2714 accuracy : 0.9231253938248267  loss : 0.37858993497764803\n",
      "iterations 2715 accuracy : 0.9229153539172443  loss : 0.3785622403149694\n",
      "iterations 2716 accuracy : 0.9229153539172443  loss : 0.37853433048772295\n",
      "iterations 2717 accuracy : 0.9229153539172443  loss : 0.378505948537592\n",
      "iterations 2718 accuracy : 0.9229153539172443  loss : 0.3784787515483734\n",
      "iterations 2719 accuracy : 0.9231253938248267  loss : 0.3784510062200593\n",
      "iterations 2720 accuracy : 0.9231253938248267  loss : 0.3784233152132972\n",
      "iterations 2721 accuracy : 0.9231253938248267  loss : 0.3783963790692627\n",
      "iterations 2722 accuracy : 0.9231253938248267  loss : 0.3783678900089692\n",
      "iterations 2723 accuracy : 0.9231253938248267  loss : 0.3783394770283914\n",
      "iterations 2724 accuracy : 0.9231253938248267  loss : 0.3783112470981994\n",
      "iterations 2725 accuracy : 0.9231253938248267  loss : 0.37828346052644257\n",
      "iterations 2726 accuracy : 0.9231253938248267  loss : 0.37825530628203075\n",
      "iterations 2727 accuracy : 0.9231253938248267  loss : 0.37822717294281216\n",
      "iterations 2728 accuracy : 0.9231253938248267  loss : 0.3781991945355705\n",
      "iterations 2729 accuracy : 0.9231253938248267  loss : 0.3781707149125481\n",
      "iterations 2730 accuracy : 0.9231253938248267  loss : 0.37814233663951385\n",
      "iterations 2731 accuracy : 0.9231253938248267  loss : 0.3781148557417111\n",
      "iterations 2732 accuracy : 0.9231253938248267  loss : 0.37808684007124177\n",
      "iterations 2733 accuracy : 0.9233354337324091  loss : 0.3780592079371609\n",
      "iterations 2734 accuracy : 0.9231253938248267  loss : 0.3780316346297187\n",
      "iterations 2735 accuracy : 0.9231253938248267  loss : 0.378004075103894\n",
      "iterations 2736 accuracy : 0.9235454736399916  loss : 0.3779769995100522\n",
      "iterations 2737 accuracy : 0.9235454736399916  loss : 0.37794984341082294\n",
      "iterations 2738 accuracy : 0.9235454736399916  loss : 0.37792243181699436\n",
      "iterations 2739 accuracy : 0.9235454736399916  loss : 0.3778948491242128\n",
      "iterations 2740 accuracy : 0.9235454736399916  loss : 0.3778675485326126\n",
      "iterations 2741 accuracy : 0.9235454736399916  loss : 0.3778397220305301\n",
      "iterations 2742 accuracy : 0.9235454736399916  loss : 0.3778126658008525\n",
      "iterations 2743 accuracy : 0.9235454736399916  loss : 0.37778526862894063\n",
      "iterations 2744 accuracy : 0.9235454736399916  loss : 0.3777585423705401\n",
      "iterations 2745 accuracy : 0.9235454736399916  loss : 0.3777310154284405\n",
      "iterations 2746 accuracy : 0.9235454736399916  loss : 0.3777040093868785\n",
      "iterations 2747 accuracy : 0.9235454736399916  loss : 0.3776770804899166\n",
      "iterations 2748 accuracy : 0.9235454736399916  loss : 0.3776496764194073\n",
      "iterations 2749 accuracy : 0.9235454736399916  loss : 0.3776218831447137\n",
      "iterations 2750 accuracy : 0.9235454736399916  loss : 0.37759393513825085\n",
      "iterations 2751 accuracy : 0.9235454736399916  loss : 0.3775666136683254\n",
      "iterations 2752 accuracy : 0.9235454736399916  loss : 0.3775398116346944\n",
      "iterations 2753 accuracy : 0.9235454736399916  loss : 0.37751248912482827\n",
      "iterations 2754 accuracy : 0.9235454736399916  loss : 0.37748560870931686\n",
      "iterations 2755 accuracy : 0.9235454736399916  loss : 0.3774584345315099\n",
      "iterations 2756 accuracy : 0.9235454736399916  loss : 0.3774318501283922\n",
      "iterations 2757 accuracy : 0.9235454736399916  loss : 0.37740526321589485\n",
      "iterations 2758 accuracy : 0.9235454736399916  loss : 0.3773776902639559\n",
      "iterations 2759 accuracy : 0.9235454736399916  loss : 0.37735022585581457\n",
      "iterations 2760 accuracy : 0.9235454736399916  loss : 0.37732307157539313\n",
      "iterations 2761 accuracy : 0.9235454736399916  loss : 0.3772955064357581\n",
      "iterations 2762 accuracy : 0.9235454736399916  loss : 0.37726819588153815\n",
      "iterations 2763 accuracy : 0.9235454736399916  loss : 0.37724122273528193\n",
      "iterations 2764 accuracy : 0.9235454736399916  loss : 0.37721404993375884\n",
      "iterations 2765 accuracy : 0.9235454736399916  loss : 0.37718752708110015\n",
      "iterations 2766 accuracy : 0.9235454736399916  loss : 0.3771604189081501\n",
      "iterations 2767 accuracy : 0.9235454736399916  loss : 0.37713373521308063\n",
      "iterations 2768 accuracy : 0.9235454736399916  loss : 0.377107553556259\n",
      "iterations 2769 accuracy : 0.9235454736399916  loss : 0.37708024070153273\n",
      "iterations 2770 accuracy : 0.9235454736399916  loss : 0.37705325878543716\n",
      "iterations 2771 accuracy : 0.9235454736399916  loss : 0.3770252136611992\n",
      "iterations 2772 accuracy : 0.9235454736399916  loss : 0.3769975825649988\n",
      "iterations 2773 accuracy : 0.9235454736399916  loss : 0.3769689785570031\n",
      "iterations 2774 accuracy : 0.9235454736399916  loss : 0.37694246405619725\n",
      "iterations 2775 accuracy : 0.9235454736399916  loss : 0.3769153724853619\n",
      "iterations 2776 accuracy : 0.9235454736399916  loss : 0.3768879409176378\n",
      "iterations 2777 accuracy : 0.9235454736399916  loss : 0.3768612717836505\n",
      "iterations 2778 accuracy : 0.9235454736399916  loss : 0.37683442567227265\n",
      "iterations 2779 accuracy : 0.9235454736399916  loss : 0.3768070039835102\n",
      "iterations 2780 accuracy : 0.9235454736399916  loss : 0.3767796293262413\n",
      "iterations 2781 accuracy : 0.9235454736399916  loss : 0.3767527174379639\n",
      "iterations 2782 accuracy : 0.9235454736399916  loss : 0.3767260826102371\n",
      "iterations 2783 accuracy : 0.9235454736399916  loss : 0.37669943166990666\n",
      "iterations 2784 accuracy : 0.9235454736399916  loss : 0.37667247883014343\n",
      "iterations 2785 accuracy : 0.9235454736399916  loss : 0.37664573645990634\n",
      "iterations 2786 accuracy : 0.9235454736399916  loss : 0.3766181446855057\n",
      "iterations 2787 accuracy : 0.9239655534551565  loss : 0.3765909576367898\n",
      "iterations 2788 accuracy : 0.9239655534551565  loss : 0.3765645891617072\n",
      "iterations 2789 accuracy : 0.9239655534551565  loss : 0.37653826569670834\n",
      "iterations 2790 accuracy : 0.9239655534551565  loss : 0.37651194190855547\n",
      "iterations 2791 accuracy : 0.9239655534551565  loss : 0.3764854398232947\n",
      "iterations 2792 accuracy : 0.9239655534551565  loss : 0.3764589934293633\n",
      "iterations 2793 accuracy : 0.9239655534551565  loss : 0.3764314331744235\n",
      "iterations 2794 accuracy : 0.9239655534551565  loss : 0.3764046527566252\n",
      "iterations 2795 accuracy : 0.9243856332703214  loss : 0.3763776484123312\n",
      "iterations 2796 accuracy : 0.9241755933627389  loss : 0.3763510271787436\n",
      "iterations 2797 accuracy : 0.9243856332703214  loss : 0.37632444794524034\n",
      "iterations 2798 accuracy : 0.9245956731779038  loss : 0.3762975929506285\n",
      "iterations 2799 accuracy : 0.9245956731779038  loss : 0.37627115222064655\n",
      "iterations 2800 accuracy : 0.9245956731779038  loss : 0.3762442117780558\n",
      "iterations 2801 accuracy : 0.9245956731779038  loss : 0.37621692348939295\n",
      "iterations 2802 accuracy : 0.9245956731779038  loss : 0.37618984015348145\n",
      "iterations 2803 accuracy : 0.9245956731779038  loss : 0.37616329664793313\n",
      "iterations 2804 accuracy : 0.9245956731779038  loss : 0.376135595601515\n",
      "iterations 2805 accuracy : 0.9245956731779038  loss : 0.3761098209150534\n",
      "iterations 2806 accuracy : 0.9245956731779038  loss : 0.37608316778664586\n",
      "iterations 2807 accuracy : 0.9245956731779038  loss : 0.37605696516032466\n",
      "iterations 2808 accuracy : 0.9245956731779038  loss : 0.37603050365663987\n",
      "iterations 2809 accuracy : 0.9245956731779038  loss : 0.37600334165353255\n",
      "iterations 2810 accuracy : 0.9245956731779038  loss : 0.37597746799188453\n",
      "iterations 2811 accuracy : 0.9245956731779038  loss : 0.37595076939271443\n",
      "iterations 2812 accuracy : 0.9245956731779038  loss : 0.3759239650683041\n",
      "iterations 2813 accuracy : 0.9245956731779038  loss : 0.37589731784193214\n",
      "iterations 2814 accuracy : 0.9245956731779038  loss : 0.37587053598286724\n",
      "iterations 2815 accuracy : 0.9245956731779038  loss : 0.37584268963555933\n",
      "iterations 2816 accuracy : 0.9245956731779038  loss : 0.3758159532205086\n",
      "iterations 2817 accuracy : 0.9245956731779038  loss : 0.37578930252248843\n",
      "iterations 2818 accuracy : 0.9248057130854862  loss : 0.37576223376926665\n",
      "iterations 2819 accuracy : 0.9248057130854862  loss : 0.3757358975796597\n",
      "iterations 2820 accuracy : 0.9250157529930687  loss : 0.37570954738863377\n",
      "iterations 2821 accuracy : 0.9250157529930687  loss : 0.3756840603285947\n",
      "iterations 2822 accuracy : 0.9250157529930687  loss : 0.3756583140466156\n",
      "iterations 2823 accuracy : 0.9252257929006511  loss : 0.37563152632234265\n",
      "iterations 2824 accuracy : 0.9250157529930687  loss : 0.3756051950930879\n",
      "iterations 2825 accuracy : 0.9250157529930687  loss : 0.3755780890031524\n",
      "iterations 2826 accuracy : 0.9252257929006511  loss : 0.3755519977447738\n",
      "iterations 2827 accuracy : 0.925645872715816  loss : 0.3755256795381409\n",
      "iterations 2828 accuracy : 0.925645872715816  loss : 0.37550010576799114\n",
      "iterations 2829 accuracy : 0.9254358328082336  loss : 0.3754741262195505\n",
      "iterations 2830 accuracy : 0.9250157529930687  loss : 0.37544889032744877\n",
      "iterations 2831 accuracy : 0.9248057130854862  loss : 0.37542265874390934\n",
      "iterations 2832 accuracy : 0.9245956731779038  loss : 0.3753977108085976\n",
      "iterations 2833 accuracy : 0.9245956731779038  loss : 0.375371621895601\n",
      "iterations 2834 accuracy : 0.9245956731779038  loss : 0.3753451652337127\n",
      "iterations 2835 accuracy : 0.9245956731779038  loss : 0.37531821840202906\n",
      "iterations 2836 accuracy : 0.9245956731779038  loss : 0.37529238148338034\n",
      "iterations 2837 accuracy : 0.9245956731779038  loss : 0.3752663488454551\n",
      "iterations 2838 accuracy : 0.9245956731779038  loss : 0.37524030257264973\n",
      "iterations 2839 accuracy : 0.9245956731779038  loss : 0.37521408475774976\n",
      "iterations 2840 accuracy : 0.9245956731779038  loss : 0.37518714772652084\n",
      "iterations 2841 accuracy : 0.9245956731779038  loss : 0.375160661179707\n",
      "iterations 2842 accuracy : 0.9245956731779038  loss : 0.37513458068858635\n",
      "iterations 2843 accuracy : 0.9245956731779038  loss : 0.3751073774640885\n",
      "iterations 2844 accuracy : 0.9245956731779038  loss : 0.3750800052470461\n",
      "iterations 2845 accuracy : 0.9250157529930687  loss : 0.37505295012711654\n",
      "iterations 2846 accuracy : 0.9245956731779038  loss : 0.37502640281966976\n",
      "iterations 2847 accuracy : 0.9250157529930687  loss : 0.3750001498433614\n",
      "iterations 2848 accuracy : 0.9250157529930687  loss : 0.3749735672937009\n",
      "iterations 2849 accuracy : 0.9254358328082336  loss : 0.3749477969706751\n",
      "iterations 2850 accuracy : 0.925645872715816  loss : 0.3749218102174622\n",
      "iterations 2851 accuracy : 0.9258559126233984  loss : 0.3748959701176546\n",
      "iterations 2852 accuracy : 0.925645872715816  loss : 0.3748694702102771\n",
      "iterations 2853 accuracy : 0.925645872715816  loss : 0.3748433263002979\n",
      "iterations 2854 accuracy : 0.925645872715816  loss : 0.3748175363708584\n",
      "iterations 2855 accuracy : 0.925645872715816  loss : 0.3747915075158383\n",
      "iterations 2856 accuracy : 0.925645872715816  loss : 0.37476483839906954\n",
      "iterations 2857 accuracy : 0.925645872715816  loss : 0.3747392453577848\n",
      "iterations 2858 accuracy : 0.925645872715816  loss : 0.37471305598931537\n",
      "iterations 2859 accuracy : 0.9258559126233984  loss : 0.37468763725740123\n",
      "iterations 2860 accuracy : 0.9258559126233984  loss : 0.37466152379513895\n",
      "iterations 2861 accuracy : 0.9258559126233984  loss : 0.37463660866575327\n",
      "iterations 2862 accuracy : 0.9258559126233984  loss : 0.3746090846579881\n",
      "iterations 2863 accuracy : 0.9258559126233984  loss : 0.37458250818993155\n",
      "iterations 2864 accuracy : 0.9258559126233984  loss : 0.3745565705942925\n",
      "iterations 2865 accuracy : 0.9258559126233984  loss : 0.3745311621653709\n",
      "iterations 2866 accuracy : 0.9258559126233984  loss : 0.37450493035553406\n",
      "iterations 2867 accuracy : 0.9258559126233984  loss : 0.37447928684547865\n",
      "iterations 2868 accuracy : 0.9258559126233984  loss : 0.37445460697500227\n",
      "iterations 2869 accuracy : 0.9258559126233984  loss : 0.37442821707397705\n",
      "iterations 2870 accuracy : 0.9258559126233984  loss : 0.37440323389571895\n",
      "iterations 2871 accuracy : 0.9258559126233984  loss : 0.3743774024279262\n",
      "iterations 2872 accuracy : 0.9258559126233984  loss : 0.37435086230773856\n",
      "iterations 2873 accuracy : 0.9258559126233984  loss : 0.37432485066731225\n",
      "iterations 2874 accuracy : 0.9258559126233984  loss : 0.3742987776479945\n",
      "iterations 2875 accuracy : 0.9258559126233984  loss : 0.3742719671337219\n",
      "iterations 2876 accuracy : 0.9258559126233984  loss : 0.3742451624212683\n",
      "iterations 2877 accuracy : 0.9258559126233984  loss : 0.37422032449922854\n",
      "iterations 2878 accuracy : 0.9258559126233984  loss : 0.3741948873778453\n",
      "iterations 2879 accuracy : 0.9258559126233984  loss : 0.3741695130865313\n",
      "iterations 2880 accuracy : 0.9258559126233984  loss : 0.37414313369702407\n",
      "iterations 2881 accuracy : 0.9258559126233984  loss : 0.37411683542508817\n",
      "iterations 2882 accuracy : 0.9260659525309809  loss : 0.374090945390132\n",
      "iterations 2883 accuracy : 0.9258559126233984  loss : 0.3740649808631135\n",
      "iterations 2884 accuracy : 0.9258559126233984  loss : 0.37403908227078275\n",
      "iterations 2885 accuracy : 0.9258559126233984  loss : 0.3740127576659111\n",
      "iterations 2886 accuracy : 0.9260659525309809  loss : 0.37398659556651\n",
      "iterations 2887 accuracy : 0.9260659525309809  loss : 0.37396010324263246\n",
      "iterations 2888 accuracy : 0.9260659525309809  loss : 0.3739347484408836\n",
      "iterations 2889 accuracy : 0.9260659525309809  loss : 0.373909347790255\n",
      "iterations 2890 accuracy : 0.9260659525309809  loss : 0.3738826966625459\n",
      "iterations 2891 accuracy : 0.9260659525309809  loss : 0.3738580489857778\n",
      "iterations 2892 accuracy : 0.9260659525309809  loss : 0.373832442540242\n",
      "iterations 2893 accuracy : 0.9260659525309809  loss : 0.3738065379068395\n",
      "iterations 2894 accuracy : 0.9260659525309809  loss : 0.37378131660907005\n",
      "iterations 2895 accuracy : 0.9258559126233984  loss : 0.3737562869703241\n",
      "iterations 2896 accuracy : 0.9258559126233984  loss : 0.3737307337861833\n",
      "iterations 2897 accuracy : 0.9258559126233984  loss : 0.37370555404896905\n",
      "iterations 2898 accuracy : 0.9258559126233984  loss : 0.3736791419892797\n",
      "iterations 2899 accuracy : 0.9258559126233984  loss : 0.37365314986478\n",
      "iterations 2900 accuracy : 0.9258559126233984  loss : 0.37362717403766654\n",
      "iterations 2901 accuracy : 0.9260659525309809  loss : 0.37360085692985623\n",
      "iterations 2902 accuracy : 0.9260659525309809  loss : 0.3735754454969552\n",
      "iterations 2903 accuracy : 0.9258559126233984  loss : 0.37355072715671045\n",
      "iterations 2904 accuracy : 0.9260659525309809  loss : 0.3735247746392233\n",
      "iterations 2905 accuracy : 0.9260659525309809  loss : 0.37349926857755217\n",
      "iterations 2906 accuracy : 0.9260659525309809  loss : 0.373473617267988\n",
      "iterations 2907 accuracy : 0.9260659525309809  loss : 0.37344827093608407\n",
      "iterations 2908 accuracy : 0.9260659525309809  loss : 0.373422240429587\n",
      "iterations 2909 accuracy : 0.9260659525309809  loss : 0.3733958510585066\n",
      "iterations 2910 accuracy : 0.9260659525309809  loss : 0.3733701822143552\n",
      "iterations 2911 accuracy : 0.9260659525309809  loss : 0.37334418896499294\n",
      "iterations 2912 accuracy : 0.9260659525309809  loss : 0.37331778410699834\n",
      "iterations 2913 accuracy : 0.9260659525309809  loss : 0.3732927683246108\n",
      "iterations 2914 accuracy : 0.9260659525309809  loss : 0.37326724778828574\n",
      "iterations 2915 accuracy : 0.9260659525309809  loss : 0.37324084555500237\n",
      "iterations 2916 accuracy : 0.9260659525309809  loss : 0.37321517968663803\n",
      "iterations 2917 accuracy : 0.9260659525309809  loss : 0.3731893829659204\n",
      "iterations 2918 accuracy : 0.9260659525309809  loss : 0.37316426965057214\n",
      "iterations 2919 accuracy : 0.9260659525309809  loss : 0.37313932851724047\n",
      "iterations 2920 accuracy : 0.9260659525309809  loss : 0.3731148088866391\n",
      "iterations 2921 accuracy : 0.9260659525309809  loss : 0.3730893334603605\n",
      "iterations 2922 accuracy : 0.9260659525309809  loss : 0.3730637982662005\n",
      "iterations 2923 accuracy : 0.9260659525309809  loss : 0.3730381387482308\n",
      "iterations 2924 accuracy : 0.9260659525309809  loss : 0.3730128049774843\n",
      "iterations 2925 accuracy : 0.9260659525309809  loss : 0.3729868528238615\n",
      "iterations 2926 accuracy : 0.9260659525309809  loss : 0.3729616770109833\n",
      "iterations 2927 accuracy : 0.9260659525309809  loss : 0.37293739171786416\n",
      "iterations 2928 accuracy : 0.9260659525309809  loss : 0.3729111833824963\n",
      "iterations 2929 accuracy : 0.9262759924385633  loss : 0.3728854335911094\n",
      "iterations 2930 accuracy : 0.9262759924385633  loss : 0.37285924481646443\n",
      "iterations 2931 accuracy : 0.9260659525309809  loss : 0.3728350816782029\n",
      "iterations 2932 accuracy : 0.9262759924385633  loss : 0.37280920171725357\n",
      "iterations 2933 accuracy : 0.9262759924385633  loss : 0.37278277926522424\n",
      "iterations 2934 accuracy : 0.9262759924385633  loss : 0.37275799356920336\n",
      "iterations 2935 accuracy : 0.9262759924385633  loss : 0.3727332723198428\n",
      "iterations 2936 accuracy : 0.9262759924385633  loss : 0.3727086793950997\n",
      "iterations 2937 accuracy : 0.9262759924385633  loss : 0.37268330813746076\n",
      "iterations 2938 accuracy : 0.9262759924385633  loss : 0.3726584421130325\n",
      "iterations 2939 accuracy : 0.9262759924385633  loss : 0.3726334436609202\n",
      "iterations 2940 accuracy : 0.9262759924385633  loss : 0.37260856705560214\n",
      "iterations 2941 accuracy : 0.9264860323461458  loss : 0.37258277783589\n",
      "iterations 2942 accuracy : 0.9264860323461458  loss : 0.3725570818590497\n",
      "iterations 2943 accuracy : 0.9264860323461458  loss : 0.37253123944429073\n",
      "iterations 2944 accuracy : 0.9264860323461458  loss : 0.3725065922411643\n",
      "iterations 2945 accuracy : 0.9264860323461458  loss : 0.3724816214457427\n",
      "iterations 2946 accuracy : 0.9264860323461458  loss : 0.3724568866911425\n",
      "iterations 2947 accuracy : 0.9264860323461458  loss : 0.3724314209826571\n",
      "iterations 2948 accuracy : 0.9266960722537282  loss : 0.3724068101722249\n",
      "iterations 2949 accuracy : 0.9266960722537282  loss : 0.3723814326117269\n",
      "iterations 2950 accuracy : 0.9266960722537282  loss : 0.37235681073764426\n",
      "iterations 2951 accuracy : 0.9266960722537282  loss : 0.3723314004691325\n",
      "iterations 2952 accuracy : 0.9266960722537282  loss : 0.37230618601492443\n",
      "iterations 2953 accuracy : 0.9266960722537282  loss : 0.37228108058687004\n",
      "iterations 2954 accuracy : 0.9266960722537282  loss : 0.37225637753620794\n",
      "iterations 2955 accuracy : 0.9266960722537282  loss : 0.37223099377951113\n",
      "iterations 2956 accuracy : 0.9266960722537282  loss : 0.37220595455164646\n",
      "iterations 2957 accuracy : 0.9266960722537282  loss : 0.3721801774880354\n",
      "iterations 2958 accuracy : 0.9266960722537282  loss : 0.37215503305454994\n",
      "iterations 2959 accuracy : 0.9266960722537282  loss : 0.3721306550820559\n",
      "iterations 2960 accuracy : 0.9264860323461458  loss : 0.37210613300783285\n",
      "iterations 2961 accuracy : 0.9266960722537282  loss : 0.3720814357184488\n",
      "iterations 2962 accuracy : 0.9266960722537282  loss : 0.3720569764726852\n",
      "iterations 2963 accuracy : 0.9264860323461458  loss : 0.3720327598966663\n",
      "iterations 2964 accuracy : 0.9262759924385633  loss : 0.37200732631844124\n",
      "iterations 2965 accuracy : 0.9264860323461458  loss : 0.37198289614454744\n",
      "iterations 2966 accuracy : 0.9264860323461458  loss : 0.3719585588322553\n",
      "iterations 2967 accuracy : 0.9262759924385633  loss : 0.37193373852357986\n",
      "iterations 2968 accuracy : 0.9262759924385633  loss : 0.37190934814896753\n",
      "iterations 2969 accuracy : 0.9262759924385633  loss : 0.371884002874302\n",
      "iterations 2970 accuracy : 0.9264860323461458  loss : 0.371859144851436\n",
      "iterations 2971 accuracy : 0.9264860323461458  loss : 0.37183375542835906\n",
      "iterations 2972 accuracy : 0.9262759924385633  loss : 0.37181027419088536\n",
      "iterations 2973 accuracy : 0.9262759924385633  loss : 0.37178508857460174\n",
      "iterations 2974 accuracy : 0.9262759924385633  loss : 0.3717606819907955\n",
      "iterations 2975 accuracy : 0.9262759924385633  loss : 0.3717363642506752\n",
      "iterations 2976 accuracy : 0.9262759924385633  loss : 0.3717115642248421\n",
      "iterations 2977 accuracy : 0.9264860323461458  loss : 0.3716870472936498\n",
      "iterations 2978 accuracy : 0.9264860323461458  loss : 0.3716616671692751\n",
      "iterations 2979 accuracy : 0.9264860323461458  loss : 0.3716363832143544\n",
      "iterations 2980 accuracy : 0.9264860323461458  loss : 0.37161248452491585\n",
      "iterations 2981 accuracy : 0.9264860323461458  loss : 0.3715876433093602\n",
      "iterations 2982 accuracy : 0.9264860323461458  loss : 0.37156341994785336\n",
      "iterations 2983 accuracy : 0.9264860323461458  loss : 0.3715386582396308\n",
      "iterations 2984 accuracy : 0.9264860323461458  loss : 0.37151457741999583\n",
      "iterations 2985 accuracy : 0.9266960722537282  loss : 0.37149076113567425\n",
      "iterations 2986 accuracy : 0.9264860323461458  loss : 0.3714669155808631\n",
      "iterations 2987 accuracy : 0.9266960722537282  loss : 0.3714422725262522\n",
      "iterations 2988 accuracy : 0.9266960722537282  loss : 0.371417336690079\n",
      "iterations 2989 accuracy : 0.9266960722537282  loss : 0.37139277728410686\n",
      "iterations 2990 accuracy : 0.9266960722537282  loss : 0.3713686540811452\n",
      "iterations 2991 accuracy : 0.9266960722537282  loss : 0.37134378236726007\n",
      "iterations 2992 accuracy : 0.9266960722537282  loss : 0.3713191116839699\n",
      "iterations 2993 accuracy : 0.9266960722537282  loss : 0.3712935308759867\n",
      "iterations 2994 accuracy : 0.9266960722537282  loss : 0.3712682808605745\n",
      "iterations 2995 accuracy : 0.9266960722537282  loss : 0.3712435699244879\n",
      "iterations 2996 accuracy : 0.9266960722537282  loss : 0.37121864035491847\n",
      "iterations 2997 accuracy : 0.9266960722537282  loss : 0.3711942104606284\n",
      "iterations 2998 accuracy : 0.9266960722537282  loss : 0.371169956393695\n",
      "iterations 2999 accuracy : 0.9266960722537282  loss : 0.371145530881284\n",
      "iterations 3000 accuracy : 0.9266960722537282  loss : 0.3711215471301013\n",
      "iterations 3001 accuracy : 0.9266960722537282  loss : 0.37109653116149977\n",
      "iterations 3002 accuracy : 0.9266960722537282  loss : 0.37107212013941165\n",
      "iterations 3003 accuracy : 0.9266960722537282  loss : 0.3710464169459007\n",
      "iterations 3004 accuracy : 0.9269061121613107  loss : 0.3710218376616897\n",
      "iterations 3005 accuracy : 0.9266960722537282  loss : 0.37099799620151114\n",
      "iterations 3006 accuracy : 0.9266960722537282  loss : 0.3709734129975348\n",
      "iterations 3007 accuracy : 0.9266960722537282  loss : 0.3709490901579953\n",
      "iterations 3008 accuracy : 0.9266960722537282  loss : 0.3709253974794372\n",
      "iterations 3009 accuracy : 0.9266960722537282  loss : 0.37090017933342656\n",
      "iterations 3010 accuracy : 0.9266960722537282  loss : 0.37087633762630706\n",
      "iterations 3011 accuracy : 0.9266960722537282  loss : 0.37085129378049453\n",
      "iterations 3012 accuracy : 0.9269061121613107  loss : 0.37082702375478\n",
      "iterations 3013 accuracy : 0.9269061121613107  loss : 0.3708029019524828\n",
      "iterations 3014 accuracy : 0.9269061121613107  loss : 0.3707780784472749\n",
      "iterations 3015 accuracy : 0.9269061121613107  loss : 0.37075287970801424\n",
      "iterations 3016 accuracy : 0.9269061121613107  loss : 0.37072767457744427\n",
      "iterations 3017 accuracy : 0.9269061121613107  loss : 0.3707025303914945\n",
      "iterations 3018 accuracy : 0.9269061121613107  loss : 0.3706781128131015\n",
      "iterations 3019 accuracy : 0.9271161520688931  loss : 0.3706533216763133\n",
      "iterations 3020 accuracy : 0.9269061121613107  loss : 0.3706288325945198\n",
      "iterations 3021 accuracy : 0.9271161520688931  loss : 0.3706035435459552\n",
      "iterations 3022 accuracy : 0.9269061121613107  loss : 0.370579522346995\n",
      "iterations 3023 accuracy : 0.9269061121613107  loss : 0.3705555003220657\n",
      "iterations 3024 accuracy : 0.9271161520688931  loss : 0.37053183142074275\n",
      "iterations 3025 accuracy : 0.9271161520688931  loss : 0.3705080781617901\n",
      "iterations 3026 accuracy : 0.9271161520688931  loss : 0.37048394453607775\n",
      "iterations 3027 accuracy : 0.9271161520688931  loss : 0.3704590657277202\n",
      "iterations 3028 accuracy : 0.9271161520688931  loss : 0.37043550176171836\n",
      "iterations 3029 accuracy : 0.9271161520688931  loss : 0.37041065761786635\n",
      "iterations 3030 accuracy : 0.9271161520688931  loss : 0.37038688216788423\n",
      "iterations 3031 accuracy : 0.9271161520688931  loss : 0.3703625502717673\n",
      "iterations 3032 accuracy : 0.9271161520688931  loss : 0.37033827313255874\n",
      "iterations 3033 accuracy : 0.9271161520688931  loss : 0.37031349402210484\n",
      "iterations 3034 accuracy : 0.9271161520688931  loss : 0.3702892619505378\n",
      "iterations 3035 accuracy : 0.9271161520688931  loss : 0.370265639184962\n",
      "iterations 3036 accuracy : 0.9271161520688931  loss : 0.3702420354085268\n",
      "iterations 3037 accuracy : 0.9271161520688931  loss : 0.37021697849218776\n",
      "iterations 3038 accuracy : 0.9271161520688931  loss : 0.3701927247676394\n",
      "iterations 3039 accuracy : 0.9271161520688931  loss : 0.37016909275190746\n",
      "iterations 3040 accuracy : 0.9271161520688931  loss : 0.3701447453920299\n",
      "iterations 3041 accuracy : 0.9271161520688931  loss : 0.3701209827382599\n",
      "iterations 3042 accuracy : 0.9271161520688931  loss : 0.37009680248014226\n",
      "iterations 3043 accuracy : 0.9277462717916404  loss : 0.37007197972835343\n",
      "iterations 3044 accuracy : 0.9273261919764755  loss : 0.37004857236731226\n",
      "iterations 3045 accuracy : 0.9271161520688931  loss : 0.37002530951772195\n",
      "iterations 3046 accuracy : 0.9273261919764755  loss : 0.3700009007213556\n",
      "iterations 3047 accuracy : 0.927536231884058  loss : 0.3699763819235514\n",
      "iterations 3048 accuracy : 0.9273261919764755  loss : 0.369952579708081\n",
      "iterations 3049 accuracy : 0.9273261919764755  loss : 0.3699283361221075\n",
      "iterations 3050 accuracy : 0.9277462717916404  loss : 0.3699046220698712\n",
      "iterations 3051 accuracy : 0.9273261919764755  loss : 0.3698801475003963\n",
      "iterations 3052 accuracy : 0.9277462717916404  loss : 0.36985609772280575\n",
      "iterations 3053 accuracy : 0.927536231884058  loss : 0.36983143772731886\n",
      "iterations 3054 accuracy : 0.9273261919764755  loss : 0.369807057783405\n",
      "iterations 3055 accuracy : 0.9271161520688931  loss : 0.3697835858969657\n",
      "iterations 3056 accuracy : 0.9271161520688931  loss : 0.369760764108454\n",
      "iterations 3057 accuracy : 0.9271161520688931  loss : 0.36973759274053386\n",
      "iterations 3058 accuracy : 0.9271161520688931  loss : 0.3697142103074289\n",
      "iterations 3059 accuracy : 0.9271161520688931  loss : 0.36968950970853776\n",
      "iterations 3060 accuracy : 0.9271161520688931  loss : 0.3696668056435602\n",
      "iterations 3061 accuracy : 0.9271161520688931  loss : 0.36964287674458907\n",
      "iterations 3062 accuracy : 0.927536231884058  loss : 0.36961933422193227\n",
      "iterations 3063 accuracy : 0.9277462717916404  loss : 0.3695949033137648\n",
      "iterations 3064 accuracy : 0.9277462717916404  loss : 0.36957113596937646\n",
      "iterations 3065 accuracy : 0.9277462717916404  loss : 0.3695474327449806\n",
      "iterations 3066 accuracy : 0.9277462717916404  loss : 0.3695227330801469\n",
      "iterations 3067 accuracy : 0.9277462717916404  loss : 0.36949936017559964\n",
      "iterations 3068 accuracy : 0.9277462717916404  loss : 0.36947568155919686\n",
      "iterations 3069 accuracy : 0.9277462717916404  loss : 0.3694519619472943\n",
      "iterations 3070 accuracy : 0.9277462717916404  loss : 0.3694288278354972\n",
      "iterations 3071 accuracy : 0.9277462717916404  loss : 0.3694052029222657\n",
      "iterations 3072 accuracy : 0.9277462717916404  loss : 0.3693803853322305\n",
      "iterations 3073 accuracy : 0.9277462717916404  loss : 0.36935666667428474\n",
      "iterations 3074 accuracy : 0.9277462717916404  loss : 0.36933291499033855\n",
      "iterations 3075 accuracy : 0.9277462717916404  loss : 0.3693093919491952\n",
      "iterations 3076 accuracy : 0.9277462717916404  loss : 0.36928600998636085\n",
      "iterations 3077 accuracy : 0.9277462717916404  loss : 0.36926242063419945\n",
      "iterations 3078 accuracy : 0.9277462717916404  loss : 0.3692385804665031\n",
      "iterations 3079 accuracy : 0.9277462717916404  loss : 0.36921428234892967\n",
      "iterations 3080 accuracy : 0.9277462717916404  loss : 0.36919093925084967\n",
      "iterations 3081 accuracy : 0.9277462717916404  loss : 0.36916777504207837\n",
      "iterations 3082 accuracy : 0.9277462717916404  loss : 0.36914390482447657\n",
      "iterations 3083 accuracy : 0.9277462717916404  loss : 0.369120556132697\n",
      "iterations 3084 accuracy : 0.9277462717916404  loss : 0.36909661520752585\n",
      "iterations 3085 accuracy : 0.9277462717916404  loss : 0.36907298346287787\n",
      "iterations 3086 accuracy : 0.9277462717916404  loss : 0.3690491024576987\n",
      "iterations 3087 accuracy : 0.9277462717916404  loss : 0.36902555001568027\n",
      "iterations 3088 accuracy : 0.9277462717916404  loss : 0.36900199369042985\n",
      "iterations 3089 accuracy : 0.9277462717916404  loss : 0.3689785206394181\n",
      "iterations 3090 accuracy : 0.9281663516068053  loss : 0.36895520325743775\n",
      "iterations 3091 accuracy : 0.9277462717916404  loss : 0.3689314115952206\n",
      "iterations 3092 accuracy : 0.9277462717916404  loss : 0.36890800380117234\n",
      "iterations 3093 accuracy : 0.9281663516068053  loss : 0.36888511818381337\n",
      "iterations 3094 accuracy : 0.9281663516068053  loss : 0.3688614448667774\n",
      "iterations 3095 accuracy : 0.9281663516068053  loss : 0.3688373951762576\n",
      "iterations 3096 accuracy : 0.9281663516068053  loss : 0.3688138566719776\n",
      "iterations 3097 accuracy : 0.9281663516068053  loss : 0.3687898435023594\n",
      "iterations 3098 accuracy : 0.9281663516068053  loss : 0.36876659233855\n",
      "iterations 3099 accuracy : 0.9281663516068053  loss : 0.3687429347196302\n",
      "iterations 3100 accuracy : 0.9281663516068053  loss : 0.3687197114492483\n",
      "iterations 3101 accuracy : 0.9283763915143878  loss : 0.3686959343042699\n",
      "iterations 3102 accuracy : 0.9283763915143878  loss : 0.3686727491554599\n",
      "iterations 3103 accuracy : 0.9283763915143878  loss : 0.368649768620372\n",
      "iterations 3104 accuracy : 0.9283763915143878  loss : 0.36862624399670163\n",
      "iterations 3105 accuracy : 0.9283763915143878  loss : 0.36860333034802206\n",
      "iterations 3106 accuracy : 0.9281663516068053  loss : 0.3685800552890971\n",
      "iterations 3107 accuracy : 0.9281663516068053  loss : 0.3685559495237057\n",
      "iterations 3108 accuracy : 0.9283763915143878  loss : 0.3685329744177313\n",
      "iterations 3109 accuracy : 0.9283763915143878  loss : 0.3685100959664428\n",
      "iterations 3110 accuracy : 0.9283763915143878  loss : 0.36848659357021735\n",
      "iterations 3111 accuracy : 0.9283763915143878  loss : 0.3684628436182833\n",
      "iterations 3112 accuracy : 0.9285864314219702  loss : 0.36843903866706634\n",
      "iterations 3113 accuracy : 0.9285864314219702  loss : 0.3684162069609179\n",
      "iterations 3114 accuracy : 0.9285864314219702  loss : 0.3683929648070989\n",
      "iterations 3115 accuracy : 0.9285864314219702  loss : 0.3683685153250102\n",
      "iterations 3116 accuracy : 0.9285864314219702  loss : 0.36834465005109795\n",
      "iterations 3117 accuracy : 0.9285864314219702  loss : 0.36832012683301857\n",
      "iterations 3118 accuracy : 0.9285864314219702  loss : 0.3682969625415377\n",
      "iterations 3119 accuracy : 0.9285864314219702  loss : 0.3682738854984722\n",
      "iterations 3120 accuracy : 0.9285864314219702  loss : 0.3682503502641136\n",
      "iterations 3121 accuracy : 0.9285864314219702  loss : 0.368227569801588\n",
      "iterations 3122 accuracy : 0.9285864314219702  loss : 0.36820368589186525\n",
      "iterations 3123 accuracy : 0.9285864314219702  loss : 0.3681799645008956\n",
      "iterations 3124 accuracy : 0.9285864314219702  loss : 0.3681567350067305\n",
      "iterations 3125 accuracy : 0.9285864314219702  loss : 0.368133915585396\n",
      "iterations 3126 accuracy : 0.9285864314219702  loss : 0.3681108628198875\n",
      "iterations 3127 accuracy : 0.9285864314219702  loss : 0.3680879554221955\n",
      "iterations 3128 accuracy : 0.9285864314219702  loss : 0.3680655194803059\n",
      "iterations 3129 accuracy : 0.9285864314219702  loss : 0.36804257019471065\n",
      "iterations 3130 accuracy : 0.9285864314219702  loss : 0.3680193210983695\n",
      "iterations 3131 accuracy : 0.9285864314219702  loss : 0.3679966228196042\n",
      "iterations 3132 accuracy : 0.9285864314219702  loss : 0.36797396344534794\n",
      "iterations 3133 accuracy : 0.9285864314219702  loss : 0.36795107046703573\n",
      "iterations 3134 accuracy : 0.9285864314219702  loss : 0.3679281532862936\n",
      "iterations 3135 accuracy : 0.9285864314219702  loss : 0.3679043864543927\n",
      "iterations 3136 accuracy : 0.9285864314219702  loss : 0.36788060011059653\n",
      "iterations 3137 accuracy : 0.9285864314219702  loss : 0.36785632626426773\n",
      "iterations 3138 accuracy : 0.9285864314219702  loss : 0.36783328642220686\n",
      "iterations 3139 accuracy : 0.9285864314219702  loss : 0.3678098074690547\n",
      "iterations 3140 accuracy : 0.9285864314219702  loss : 0.3677860446649915\n",
      "iterations 3141 accuracy : 0.9285864314219702  loss : 0.3677627520828945\n",
      "iterations 3142 accuracy : 0.9285864314219702  loss : 0.36773950822758394\n",
      "iterations 3143 accuracy : 0.9285864314219702  loss : 0.36771675581652813\n",
      "iterations 3144 accuracy : 0.9287964713295526  loss : 0.3676936684300741\n",
      "iterations 3145 accuracy : 0.9290065112371351  loss : 0.367670275803705\n",
      "iterations 3146 accuracy : 0.9290065112371351  loss : 0.3676465432767737\n",
      "iterations 3147 accuracy : 0.9290065112371351  loss : 0.3676240081131851\n",
      "iterations 3148 accuracy : 0.9290065112371351  loss : 0.3676007491132118\n",
      "iterations 3149 accuracy : 0.9290065112371351  loss : 0.36757740059085214\n",
      "iterations 3150 accuracy : 0.9290065112371351  loss : 0.36755479540457975\n",
      "iterations 3151 accuracy : 0.9290065112371351  loss : 0.367531679191762\n",
      "iterations 3152 accuracy : 0.9290065112371351  loss : 0.36750829923166184\n",
      "iterations 3153 accuracy : 0.9290065112371351  loss : 0.3674859430886167\n",
      "iterations 3154 accuracy : 0.9290065112371351  loss : 0.36746299941176047\n",
      "iterations 3155 accuracy : 0.9290065112371351  loss : 0.36744035066988645\n",
      "iterations 3156 accuracy : 0.9290065112371351  loss : 0.36741663948363285\n",
      "iterations 3157 accuracy : 0.9290065112371351  loss : 0.36739350570205953\n",
      "iterations 3158 accuracy : 0.9290065112371351  loss : 0.3673706072762176\n",
      "iterations 3159 accuracy : 0.9290065112371351  loss : 0.3673478418228548\n",
      "iterations 3160 accuracy : 0.9290065112371351  loss : 0.3673250607778578\n",
      "iterations 3161 accuracy : 0.9292165511447175  loss : 0.3673022866647177\n",
      "iterations 3162 accuracy : 0.9290065112371351  loss : 0.36728005982769757\n",
      "iterations 3163 accuracy : 0.9290065112371351  loss : 0.3672571187249761\n",
      "iterations 3164 accuracy : 0.9290065112371351  loss : 0.36723501022147476\n",
      "iterations 3165 accuracy : 0.9290065112371351  loss : 0.3672121912133418\n",
      "iterations 3166 accuracy : 0.9290065112371351  loss : 0.36718992214928425\n",
      "iterations 3167 accuracy : 0.9290065112371351  loss : 0.36716746644410564\n",
      "iterations 3168 accuracy : 0.9290065112371351  loss : 0.36714492904016893\n",
      "iterations 3169 accuracy : 0.9294265910523  loss : 0.36712217260665286\n",
      "iterations 3170 accuracy : 0.9294265910523  loss : 0.36709978876154614\n",
      "iterations 3171 accuracy : 0.9292165511447175  loss : 0.36707685127504136\n",
      "iterations 3172 accuracy : 0.9294265910523  loss : 0.3670554487664296\n",
      "iterations 3173 accuracy : 0.9292165511447175  loss : 0.367032773628026\n",
      "iterations 3174 accuracy : 0.9294265910523  loss : 0.3670101198446179\n",
      "iterations 3175 accuracy : 0.9294265910523  loss : 0.3669876020912652\n",
      "iterations 3176 accuracy : 0.9294265910523  loss : 0.3669646164780052\n",
      "iterations 3177 accuracy : 0.9294265910523  loss : 0.36694139459694847\n",
      "iterations 3178 accuracy : 0.9294265910523  loss : 0.36691916312131356\n",
      "iterations 3179 accuracy : 0.9294265910523  loss : 0.3668956676527581\n",
      "iterations 3180 accuracy : 0.9294265910523  loss : 0.3668726383858298\n",
      "iterations 3181 accuracy : 0.9294265910523  loss : 0.3668503139673473\n",
      "iterations 3182 accuracy : 0.9294265910523  loss : 0.36682711888391195\n",
      "iterations 3183 accuracy : 0.9294265910523  loss : 0.3668048033516567\n",
      "iterations 3184 accuracy : 0.9292165511447175  loss : 0.3667820271225539\n",
      "iterations 3185 accuracy : 0.9294265910523  loss : 0.36676016206541817\n",
      "iterations 3186 accuracy : 0.9294265910523  loss : 0.3667380019324796\n",
      "iterations 3187 accuracy : 0.9292165511447175  loss : 0.366714979388864\n",
      "iterations 3188 accuracy : 0.9292165511447175  loss : 0.3666921500384621\n",
      "iterations 3189 accuracy : 0.9294265910523  loss : 0.3666705570984989\n",
      "iterations 3190 accuracy : 0.9294265910523  loss : 0.3666477559454274\n",
      "iterations 3191 accuracy : 0.9294265910523  loss : 0.36662471490560467\n",
      "iterations 3192 accuracy : 0.9292165511447175  loss : 0.36660285251814106\n",
      "iterations 3193 accuracy : 0.9292165511447175  loss : 0.3665811742941861\n",
      "iterations 3194 accuracy : 0.9292165511447175  loss : 0.3665583502721973\n",
      "iterations 3195 accuracy : 0.9294265910523  loss : 0.36653572429423886\n",
      "iterations 3196 accuracy : 0.9292165511447175  loss : 0.3665138136904593\n",
      "iterations 3197 accuracy : 0.9292165511447175  loss : 0.3664916700232015\n",
      "iterations 3198 accuracy : 0.9294265910523  loss : 0.3664690892819216\n",
      "iterations 3199 accuracy : 0.9296366309598824  loss : 0.3664467915541314\n",
      "iterations 3200 accuracy : 0.9298466708674649  loss : 0.36642441373374146\n",
      "iterations 3201 accuracy : 0.9298466708674649  loss : 0.3664008119502423\n",
      "iterations 3202 accuracy : 0.9298466708674649  loss : 0.3663784632297166\n",
      "iterations 3203 accuracy : 0.9298466708674649  loss : 0.3663556963532399\n",
      "iterations 3204 accuracy : 0.9298466708674649  loss : 0.36633335433647307\n",
      "iterations 3205 accuracy : 0.9298466708674649  loss : 0.3663109321492708\n",
      "iterations 3206 accuracy : 0.9298466708674649  loss : 0.3662886677893781\n",
      "iterations 3207 accuracy : 0.9298466708674649  loss : 0.36626585901755204\n",
      "iterations 3208 accuracy : 0.9298466708674649  loss : 0.3662445728745311\n",
      "iterations 3209 accuracy : 0.9298466708674649  loss : 0.36622201197004073\n",
      "iterations 3210 accuracy : 0.9298466708674649  loss : 0.3661996502598071\n",
      "iterations 3211 accuracy : 0.9296366309598824  loss : 0.3661771844660965\n",
      "iterations 3212 accuracy : 0.9294265910523  loss : 0.36615441359210077\n",
      "iterations 3213 accuracy : 0.9298466708674649  loss : 0.36613221477570773\n",
      "iterations 3214 accuracy : 0.9298466708674649  loss : 0.3661100664524463\n",
      "iterations 3215 accuracy : 0.9298466708674649  loss : 0.3660876930676027\n",
      "iterations 3216 accuracy : 0.9298466708674649  loss : 0.366065830473317\n",
      "iterations 3217 accuracy : 0.9296366309598824  loss : 0.3660430847897749\n",
      "iterations 3218 accuracy : 0.9294265910523  loss : 0.3660211284654628\n",
      "iterations 3219 accuracy : 0.9296366309598824  loss : 0.3659986365848159\n",
      "iterations 3220 accuracy : 0.9296366309598824  loss : 0.3659772411238275\n",
      "iterations 3221 accuracy : 0.9296366309598824  loss : 0.3659549060275174\n",
      "iterations 3222 accuracy : 0.9296366309598824  loss : 0.3659333065606236\n",
      "iterations 3223 accuracy : 0.9296366309598824  loss : 0.3659096899009097\n",
      "iterations 3224 accuracy : 0.9296366309598824  loss : 0.36588717663121795\n",
      "iterations 3225 accuracy : 0.9296366309598824  loss : 0.365864824641593\n",
      "iterations 3226 accuracy : 0.9296366309598824  loss : 0.36584250317170464\n",
      "iterations 3227 accuracy : 0.9296366309598824  loss : 0.36582020957843947\n",
      "iterations 3228 accuracy : 0.9296366309598824  loss : 0.36579811207830215\n",
      "iterations 3229 accuracy : 0.9296366309598824  loss : 0.36577524579592613\n",
      "iterations 3230 accuracy : 0.9296366309598824  loss : 0.3657531267708261\n",
      "iterations 3231 accuracy : 0.9296366309598824  loss : 0.36573101333899366\n",
      "iterations 3232 accuracy : 0.9296366309598824  loss : 0.3657086615375785\n",
      "iterations 3233 accuracy : 0.9296366309598824  loss : 0.3656872703689069\n",
      "iterations 3234 accuracy : 0.9296366309598824  loss : 0.36566495033181134\n",
      "iterations 3235 accuracy : 0.9296366309598824  loss : 0.36564293226901573\n",
      "iterations 3236 accuracy : 0.9296366309598824  loss : 0.36562048775014094\n",
      "iterations 3237 accuracy : 0.9296366309598824  loss : 0.36559897143888664\n",
      "iterations 3238 accuracy : 0.9298466708674649  loss : 0.3655772221623932\n",
      "iterations 3239 accuracy : 0.9298466708674649  loss : 0.36555490417305425\n",
      "iterations 3240 accuracy : 0.9298466708674649  loss : 0.3655326215487194\n",
      "iterations 3241 accuracy : 0.9298466708674649  loss : 0.3655107531289817\n",
      "iterations 3242 accuracy : 0.9298466708674649  loss : 0.3654887497647785\n",
      "iterations 3243 accuracy : 0.9298466708674649  loss : 0.36546640879332076\n",
      "iterations 3244 accuracy : 0.9298466708674649  loss : 0.3654445099401502\n",
      "iterations 3245 accuracy : 0.9298466708674649  loss : 0.36542244620804176\n",
      "iterations 3246 accuracy : 0.9298466708674649  loss : 0.36540066258775306\n",
      "iterations 3247 accuracy : 0.9296366309598824  loss : 0.3653783386255911\n",
      "iterations 3248 accuracy : 0.9298466708674649  loss : 0.365356590687541\n",
      "iterations 3249 accuracy : 0.9298466708674649  loss : 0.3653354888960988\n",
      "iterations 3250 accuracy : 0.9298466708674649  loss : 0.3653124323027863\n",
      "iterations 3251 accuracy : 0.9296366309598824  loss : 0.36528936185240557\n",
      "iterations 3252 accuracy : 0.9298466708674649  loss : 0.3652674255069351\n",
      "iterations 3253 accuracy : 0.9298466708674649  loss : 0.3652449682710246\n",
      "iterations 3254 accuracy : 0.9298466708674649  loss : 0.36522400537842403\n",
      "iterations 3255 accuracy : 0.9296366309598824  loss : 0.365202309739396\n",
      "iterations 3256 accuracy : 0.9296366309598824  loss : 0.3651806214581663\n",
      "iterations 3257 accuracy : 0.9298466708674649  loss : 0.3651598775922514\n",
      "iterations 3258 accuracy : 0.9298466708674649  loss : 0.365138741157372\n",
      "iterations 3259 accuracy : 0.9298466708674649  loss : 0.36511713838583953\n",
      "iterations 3260 accuracy : 0.9298466708674649  loss : 0.3650952648916305\n",
      "iterations 3261 accuracy : 0.9298466708674649  loss : 0.3650734522585073\n",
      "iterations 3262 accuracy : 0.9298466708674649  loss : 0.36505093043343234\n",
      "iterations 3263 accuracy : 0.9296366309598824  loss : 0.3650285260979347\n",
      "iterations 3264 accuracy : 0.9296366309598824  loss : 0.36500731504830236\n",
      "iterations 3265 accuracy : 0.9296366309598824  loss : 0.36498587460087617\n",
      "iterations 3266 accuracy : 0.9296366309598824  loss : 0.36496390427239117\n",
      "iterations 3267 accuracy : 0.9296366309598824  loss : 0.36494132124975553\n",
      "iterations 3268 accuracy : 0.9296366309598824  loss : 0.3649193826532865\n",
      "iterations 3269 accuracy : 0.9296366309598824  loss : 0.36489756010141516\n",
      "iterations 3270 accuracy : 0.9296366309598824  loss : 0.3648761726242081\n",
      "iterations 3271 accuracy : 0.9296366309598824  loss : 0.3648553098844301\n",
      "iterations 3272 accuracy : 0.9296366309598824  loss : 0.3648331874170284\n",
      "iterations 3273 accuracy : 0.9296366309598824  loss : 0.3648110998070259\n",
      "iterations 3274 accuracy : 0.9296366309598824  loss : 0.36478934994365825\n",
      "iterations 3275 accuracy : 0.9296366309598824  loss : 0.3647670886009174\n",
      "iterations 3276 accuracy : 0.9296366309598824  loss : 0.36474554537067044\n",
      "iterations 3277 accuracy : 0.9296366309598824  loss : 0.3647242787443296\n",
      "iterations 3278 accuracy : 0.9296366309598824  loss : 0.3647024411110072\n",
      "iterations 3279 accuracy : 0.9296366309598824  loss : 0.36468002108101333\n",
      "iterations 3280 accuracy : 0.9296366309598824  loss : 0.3646584640917039\n",
      "iterations 3281 accuracy : 0.9296366309598824  loss : 0.3646363725031698\n",
      "iterations 3282 accuracy : 0.9296366309598824  loss : 0.3646144980184085\n",
      "iterations 3283 accuracy : 0.9300567107750473  loss : 0.36459280682190115\n",
      "iterations 3284 accuracy : 0.9300567107750473  loss : 0.364571780354499\n",
      "iterations 3285 accuracy : 0.9302667506826297  loss : 0.36455005452535705\n",
      "iterations 3286 accuracy : 0.9302667506826297  loss : 0.3645283839267971\n",
      "iterations 3287 accuracy : 0.9302667506826297  loss : 0.364507129503755\n",
      "iterations 3288 accuracy : 0.9300567107750473  loss : 0.3644861229297271\n",
      "iterations 3289 accuracy : 0.9300567107750473  loss : 0.364465161449906\n",
      "iterations 3290 accuracy : 0.9300567107750473  loss : 0.36444401915397784\n",
      "iterations 3291 accuracy : 0.9300567107750473  loss : 0.36442256867787415\n",
      "iterations 3292 accuracy : 0.9300567107750473  loss : 0.3644013034858006\n",
      "iterations 3293 accuracy : 0.9300567107750473  loss : 0.3643802547387409\n",
      "iterations 3294 accuracy : 0.9302667506826297  loss : 0.36435779738024693\n",
      "iterations 3295 accuracy : 0.9300567107750473  loss : 0.3643367016391901\n",
      "iterations 3296 accuracy : 0.9302667506826297  loss : 0.3643145387517136\n",
      "iterations 3297 accuracy : 0.9302667506826297  loss : 0.36429246591231496\n",
      "iterations 3298 accuracy : 0.9302667506826297  loss : 0.3642711171023959\n",
      "iterations 3299 accuracy : 0.9302667506826297  loss : 0.3642495475903276\n",
      "iterations 3300 accuracy : 0.9302667506826297  loss : 0.3642278930205436\n",
      "iterations 3301 accuracy : 0.9302667506826297  loss : 0.36420623966689225\n",
      "iterations 3302 accuracy : 0.9300567107750473  loss : 0.3641842007489597\n",
      "iterations 3303 accuracy : 0.9300567107750473  loss : 0.3641622589149208\n",
      "iterations 3304 accuracy : 0.9300567107750473  loss : 0.3641404991302734\n",
      "iterations 3305 accuracy : 0.9300567107750473  loss : 0.36411850324959166\n",
      "iterations 3306 accuracy : 0.9298466708674649  loss : 0.3640974425173986\n",
      "iterations 3307 accuracy : 0.9298466708674649  loss : 0.36407543180379015\n",
      "iterations 3308 accuracy : 0.9300567107750473  loss : 0.3640549338455272\n",
      "iterations 3309 accuracy : 0.9300567107750473  loss : 0.36403384877336387\n",
      "iterations 3310 accuracy : 0.9300567107750473  loss : 0.36401212675967315\n",
      "iterations 3311 accuracy : 0.9298466708674649  loss : 0.3639909006074791\n",
      "iterations 3312 accuracy : 0.9298466708674649  loss : 0.3639689701762965\n",
      "iterations 3313 accuracy : 0.9298466708674649  loss : 0.3639472700855459\n",
      "iterations 3314 accuracy : 0.9298466708674649  loss : 0.3639264147336855\n",
      "iterations 3315 accuracy : 0.9298466708674649  loss : 0.3639047900323958\n",
      "iterations 3316 accuracy : 0.9298466708674649  loss : 0.36388366539192335\n",
      "iterations 3317 accuracy : 0.9298466708674649  loss : 0.363861395011712\n",
      "iterations 3318 accuracy : 0.9298466708674649  loss : 0.36384029450642297\n",
      "iterations 3319 accuracy : 0.9298466708674649  loss : 0.3638200777816482\n",
      "iterations 3320 accuracy : 0.9298466708674649  loss : 0.3637990091247126\n",
      "iterations 3321 accuracy : 0.9298466708674649  loss : 0.3637769306771016\n",
      "iterations 3322 accuracy : 0.9298466708674649  loss : 0.3637560544198861\n",
      "iterations 3323 accuracy : 0.9298466708674649  loss : 0.3637342952271002\n",
      "iterations 3324 accuracy : 0.9298466708674649  loss : 0.3637125515368201\n",
      "iterations 3325 accuracy : 0.9298466708674649  loss : 0.3636910346415746\n",
      "iterations 3326 accuracy : 0.9298466708674649  loss : 0.3636706415000158\n",
      "iterations 3327 accuracy : 0.9298466708674649  loss : 0.36364964267757227\n",
      "iterations 3328 accuracy : 0.9298466708674649  loss : 0.3636279962577188\n",
      "iterations 3329 accuracy : 0.9298466708674649  loss : 0.36360600983469293\n",
      "iterations 3330 accuracy : 0.9300567107750473  loss : 0.36358570289398756\n",
      "iterations 3331 accuracy : 0.9298466708674649  loss : 0.36356466747477423\n",
      "iterations 3332 accuracy : 0.9302667506826297  loss : 0.3635444471673372\n",
      "iterations 3333 accuracy : 0.9302667506826297  loss : 0.36352355381503026\n",
      "iterations 3334 accuracy : 0.9302667506826297  loss : 0.3635025450710669\n",
      "iterations 3335 accuracy : 0.9302667506826297  loss : 0.3634806414378274\n",
      "iterations 3336 accuracy : 0.9302667506826297  loss : 0.3634591705881292\n",
      "iterations 3337 accuracy : 0.9302667506826297  loss : 0.36343812374462064\n",
      "iterations 3338 accuracy : 0.9302667506826297  loss : 0.3634168644249192\n",
      "iterations 3339 accuracy : 0.9302667506826297  loss : 0.36339514217489216\n",
      "iterations 3340 accuracy : 0.9302667506826297  loss : 0.36337403971377513\n",
      "iterations 3341 accuracy : 0.9302667506826297  loss : 0.3633533005950049\n",
      "iterations 3342 accuracy : 0.9302667506826297  loss : 0.36333308851398266\n",
      "iterations 3343 accuracy : 0.9302667506826297  loss : 0.36331125404279074\n",
      "iterations 3344 accuracy : 0.9302667506826297  loss : 0.36329018236400973\n",
      "iterations 3345 accuracy : 0.9302667506826297  loss : 0.36326944127493277\n",
      "iterations 3346 accuracy : 0.9302667506826297  loss : 0.36324878787740894\n",
      "iterations 3347 accuracy : 0.9302667506826297  loss : 0.3632276608869799\n",
      "iterations 3348 accuracy : 0.9302667506826297  loss : 0.3632069089960271\n",
      "iterations 3349 accuracy : 0.9302667506826297  loss : 0.36318643284953517\n",
      "iterations 3350 accuracy : 0.9302667506826297  loss : 0.36316440674672873\n",
      "iterations 3351 accuracy : 0.9302667506826297  loss : 0.36314267332333927\n",
      "iterations 3352 accuracy : 0.9302667506826297  loss : 0.36312230962145253\n",
      "iterations 3353 accuracy : 0.9302667506826297  loss : 0.36310078461720613\n",
      "iterations 3354 accuracy : 0.9302667506826297  loss : 0.3630799991089477\n",
      "iterations 3355 accuracy : 0.9302667506826297  loss : 0.36305971861926567\n",
      "iterations 3356 accuracy : 0.9302667506826297  loss : 0.3630390933287023\n",
      "iterations 3357 accuracy : 0.9300567107750473  loss : 0.36301794091695905\n",
      "iterations 3358 accuracy : 0.9302667506826297  loss : 0.3629975993430483\n",
      "iterations 3359 accuracy : 0.9302667506826297  loss : 0.36297685434217436\n",
      "iterations 3360 accuracy : 0.9298466708674649  loss : 0.36295533142292935\n",
      "iterations 3361 accuracy : 0.9298466708674649  loss : 0.36293449421777646\n",
      "iterations 3362 accuracy : 0.9298466708674649  loss : 0.3629139156250163\n",
      "iterations 3363 accuracy : 0.9298466708674649  loss : 0.3628928328353162\n",
      "iterations 3364 accuracy : 0.9298466708674649  loss : 0.36287137234719274\n",
      "iterations 3365 accuracy : 0.9302667506826297  loss : 0.3628511454774265\n",
      "iterations 3366 accuracy : 0.9298466708674649  loss : 0.3628298512777432\n",
      "iterations 3367 accuracy : 0.9298466708674649  loss : 0.36280871203284076\n",
      "iterations 3368 accuracy : 0.9298466708674649  loss : 0.3627873434903786\n",
      "iterations 3369 accuracy : 0.9298466708674649  loss : 0.3627666507048359\n",
      "iterations 3370 accuracy : 0.9298466708674649  loss : 0.3627447619664941\n",
      "iterations 3371 accuracy : 0.9300567107750473  loss : 0.36272390364005436\n",
      "iterations 3372 accuracy : 0.9298466708674649  loss : 0.36270331223120555\n",
      "iterations 3373 accuracy : 0.9298466708674649  loss : 0.36268330667301496\n",
      "iterations 3374 accuracy : 0.9300567107750473  loss : 0.3626624587713031\n",
      "iterations 3375 accuracy : 0.9300567107750473  loss : 0.36264125387305607\n",
      "iterations 3376 accuracy : 0.9300567107750473  loss : 0.3626195688961421\n",
      "iterations 3377 accuracy : 0.9300567107750473  loss : 0.3625992989950325\n",
      "iterations 3378 accuracy : 0.9300567107750473  loss : 0.3625782714972117\n",
      "iterations 3379 accuracy : 0.9300567107750473  loss : 0.36255758200332455\n",
      "iterations 3380 accuracy : 0.9300567107750473  loss : 0.36253641764245464\n",
      "iterations 3381 accuracy : 0.9300567107750473  loss : 0.3625158194294969\n",
      "iterations 3382 accuracy : 0.9300567107750473  loss : 0.36249497078084236\n",
      "iterations 3383 accuracy : 0.9300567107750473  loss : 0.3624735265405035\n",
      "iterations 3384 accuracy : 0.9300567107750473  loss : 0.3624524950517303\n",
      "iterations 3385 accuracy : 0.9300567107750473  loss : 0.3624306183289152\n",
      "iterations 3386 accuracy : 0.9300567107750473  loss : 0.36240952647943553\n",
      "iterations 3387 accuracy : 0.9300567107750473  loss : 0.36238908276679493\n",
      "iterations 3388 accuracy : 0.9300567107750473  loss : 0.362369312561034\n",
      "iterations 3389 accuracy : 0.9300567107750473  loss : 0.36234884007804485\n",
      "iterations 3390 accuracy : 0.9300567107750473  loss : 0.36232816617389796\n",
      "iterations 3391 accuracy : 0.9300567107750473  loss : 0.36230875027521725\n",
      "iterations 3392 accuracy : 0.9300567107750473  loss : 0.36228830240006804\n",
      "iterations 3393 accuracy : 0.9300567107750473  loss : 0.3622679294371268\n",
      "iterations 3394 accuracy : 0.9300567107750473  loss : 0.3622476232218117\n",
      "iterations 3395 accuracy : 0.9298466708674649  loss : 0.3622281259203943\n",
      "iterations 3396 accuracy : 0.9298466708674649  loss : 0.36220700318573223\n",
      "iterations 3397 accuracy : 0.9300567107750473  loss : 0.3621862581091281\n",
      "iterations 3398 accuracy : 0.9300567107750473  loss : 0.3621666610928322\n",
      "iterations 3399 accuracy : 0.9300567107750473  loss : 0.36214623723829986\n",
      "iterations 3400 accuracy : 0.9298466708674649  loss : 0.3621267580787553\n",
      "iterations 3401 accuracy : 0.9298466708674649  loss : 0.3621063743943221\n",
      "iterations 3402 accuracy : 0.9300567107750473  loss : 0.36208472996693686\n",
      "iterations 3403 accuracy : 0.9300567107750473  loss : 0.3620632308386829\n",
      "iterations 3404 accuracy : 0.9300567107750473  loss : 0.362041845830338\n",
      "iterations 3405 accuracy : 0.9300567107750473  loss : 0.36202133988438806\n",
      "iterations 3406 accuracy : 0.9298466708674649  loss : 0.36200011222309464\n",
      "iterations 3407 accuracy : 0.9298466708674649  loss : 0.36197932982306563\n",
      "iterations 3408 accuracy : 0.9300567107750473  loss : 0.3619598775838615\n",
      "iterations 3409 accuracy : 0.9300567107750473  loss : 0.3619387179614381\n",
      "iterations 3410 accuracy : 0.9298466708674649  loss : 0.36191750696270275\n",
      "iterations 3411 accuracy : 0.9298466708674649  loss : 0.3618969090972227\n",
      "iterations 3412 accuracy : 0.9298466708674649  loss : 0.3618758306418563\n",
      "iterations 3413 accuracy : 0.9298466708674649  loss : 0.3618543099665537\n",
      "iterations 3414 accuracy : 0.9298466708674649  loss : 0.3618345440106164\n",
      "iterations 3415 accuracy : 0.9298466708674649  loss : 0.3618140322037484\n",
      "iterations 3416 accuracy : 0.9298466708674649  loss : 0.3617933104500921\n",
      "iterations 3417 accuracy : 0.9298466708674649  loss : 0.36177300254903366\n",
      "iterations 3418 accuracy : 0.9298466708674649  loss : 0.3617524773562454\n",
      "iterations 3419 accuracy : 0.9298466708674649  loss : 0.3617321578543516\n",
      "iterations 3420 accuracy : 0.9298466708674649  loss : 0.36171071922127646\n",
      "iterations 3421 accuracy : 0.9298466708674649  loss : 0.36169073716788513\n",
      "iterations 3422 accuracy : 0.9298466708674649  loss : 0.3616706525326499\n",
      "iterations 3423 accuracy : 0.9298466708674649  loss : 0.36165046952239954\n",
      "iterations 3424 accuracy : 0.9298466708674649  loss : 0.3616302350241285\n",
      "iterations 3425 accuracy : 0.9298466708674649  loss : 0.36160967856969856\n",
      "iterations 3426 accuracy : 0.9298466708674649  loss : 0.36158896927347917\n",
      "iterations 3427 accuracy : 0.9298466708674649  loss : 0.36156861261683004\n",
      "iterations 3428 accuracy : 0.9298466708674649  loss : 0.3615475779568341\n",
      "iterations 3429 accuracy : 0.9298466708674649  loss : 0.3615268537442852\n",
      "iterations 3430 accuracy : 0.9298466708674649  loss : 0.3615063583957182\n",
      "iterations 3431 accuracy : 0.9298466708674649  loss : 0.3614857352720386\n",
      "iterations 3432 accuracy : 0.9298466708674649  loss : 0.36146640123236556\n",
      "iterations 3433 accuracy : 0.9298466708674649  loss : 0.36144522273567037\n",
      "iterations 3434 accuracy : 0.9298466708674649  loss : 0.3614246252648484\n",
      "iterations 3435 accuracy : 0.9298466708674649  loss : 0.3614038611047945\n",
      "iterations 3436 accuracy : 0.9298466708674649  loss : 0.36138350958640386\n",
      "iterations 3437 accuracy : 0.9298466708674649  loss : 0.3613630112171673\n",
      "iterations 3438 accuracy : 0.9298466708674649  loss : 0.36134356931154243\n",
      "iterations 3439 accuracy : 0.9298466708674649  loss : 0.3613227194428048\n",
      "iterations 3440 accuracy : 0.9298466708674649  loss : 0.36130280154022926\n",
      "iterations 3441 accuracy : 0.9298466708674649  loss : 0.3612835196807088\n",
      "iterations 3442 accuracy : 0.9298466708674649  loss : 0.3612637996450843\n",
      "iterations 3443 accuracy : 0.9298466708674649  loss : 0.3612437113208251\n",
      "iterations 3444 accuracy : 0.9298466708674649  loss : 0.36122372149847515\n",
      "iterations 3445 accuracy : 0.9298466708674649  loss : 0.3612033940292465\n",
      "iterations 3446 accuracy : 0.9298466708674649  loss : 0.3611824340304673\n",
      "iterations 3447 accuracy : 0.9298466708674649  loss : 0.3611631346597852\n",
      "iterations 3448 accuracy : 0.9298466708674649  loss : 0.3611430258765184\n",
      "iterations 3449 accuracy : 0.9298466708674649  loss : 0.3611236892904812\n",
      "iterations 3450 accuracy : 0.9298466708674649  loss : 0.36110383269865226\n",
      "iterations 3451 accuracy : 0.9298466708674649  loss : 0.36108327388840755\n",
      "iterations 3452 accuracy : 0.9298466708674649  loss : 0.3610628105772709\n",
      "iterations 3453 accuracy : 0.9298466708674649  loss : 0.36104166391649106\n",
      "iterations 3454 accuracy : 0.9298466708674649  loss : 0.361021015301303\n",
      "iterations 3455 accuracy : 0.9298466708674649  loss : 0.3610006737539093\n",
      "iterations 3456 accuracy : 0.9298466708674649  loss : 0.3609809402962921\n",
      "iterations 3457 accuracy : 0.9298466708674649  loss : 0.36095994408161164\n",
      "iterations 3458 accuracy : 0.9298466708674649  loss : 0.3609396351187714\n",
      "iterations 3459 accuracy : 0.9298466708674649  loss : 0.36091966721572954\n",
      "iterations 3460 accuracy : 0.9298466708674649  loss : 0.36089989377650056\n",
      "iterations 3461 accuracy : 0.9298466708674649  loss : 0.36087930578629474\n",
      "iterations 3462 accuracy : 0.9298466708674649  loss : 0.36085888797969046\n",
      "iterations 3463 accuracy : 0.9298466708674649  loss : 0.36083898746585646\n",
      "iterations 3464 accuracy : 0.9298466708674649  loss : 0.3608185237557415\n",
      "iterations 3465 accuracy : 0.9298466708674649  loss : 0.3607983073380722\n",
      "iterations 3466 accuracy : 0.9298466708674649  loss : 0.36077877797885355\n",
      "iterations 3467 accuracy : 0.9298466708674649  loss : 0.36075852796741853\n",
      "iterations 3468 accuracy : 0.9300567107750473  loss : 0.36073733513318545\n",
      "iterations 3469 accuracy : 0.9300567107750473  loss : 0.3607167041244794\n",
      "iterations 3470 accuracy : 0.9300567107750473  loss : 0.3606968776794392\n",
      "iterations 3471 accuracy : 0.9300567107750473  loss : 0.3606766945234926\n",
      "iterations 3472 accuracy : 0.9300567107750473  loss : 0.3606568183620217\n",
      "iterations 3473 accuracy : 0.9300567107750473  loss : 0.36063604690574674\n",
      "iterations 3474 accuracy : 0.9300567107750473  loss : 0.36061594736492364\n",
      "iterations 3475 accuracy : 0.9300567107750473  loss : 0.36059609333062226\n",
      "iterations 3476 accuracy : 0.9300567107750473  loss : 0.36057645055816806\n",
      "iterations 3477 accuracy : 0.9298466708674649  loss : 0.3605564408201901\n",
      "iterations 3478 accuracy : 0.9298466708674649  loss : 0.36053594788846105\n",
      "iterations 3479 accuracy : 0.9298466708674649  loss : 0.3605159927023068\n",
      "iterations 3480 accuracy : 0.9298466708674649  loss : 0.36049657917668365\n",
      "iterations 3481 accuracy : 0.9298466708674649  loss : 0.3604765266085013\n",
      "iterations 3482 accuracy : 0.9298466708674649  loss : 0.3604560054161778\n",
      "iterations 3483 accuracy : 0.9300567107750473  loss : 0.3604354788577854\n",
      "iterations 3484 accuracy : 0.9300567107750473  loss : 0.3604151536178409\n",
      "iterations 3485 accuracy : 0.9300567107750473  loss : 0.36039492760385594\n",
      "iterations 3486 accuracy : 0.9300567107750473  loss : 0.3603756703534995\n",
      "iterations 3487 accuracy : 0.9300567107750473  loss : 0.3603565420465184\n",
      "iterations 3488 accuracy : 0.9300567107750473  loss : 0.36033684195805077\n",
      "iterations 3489 accuracy : 0.9300567107750473  loss : 0.36031619382936847\n",
      "iterations 3490 accuracy : 0.9300567107750473  loss : 0.36029560560161167\n",
      "iterations 3491 accuracy : 0.9300567107750473  loss : 0.3602758138979106\n",
      "iterations 3492 accuracy : 0.9300567107750473  loss : 0.3602555164979332\n",
      "iterations 3493 accuracy : 0.9300567107750473  loss : 0.3602364425030448\n",
      "iterations 3494 accuracy : 0.9300567107750473  loss : 0.36021699709620075\n",
      "iterations 3495 accuracy : 0.9300567107750473  loss : 0.3601968085480568\n",
      "iterations 3496 accuracy : 0.9300567107750473  loss : 0.36017709455540314\n",
      "iterations 3497 accuracy : 0.9300567107750473  loss : 0.3601578799779223\n",
      "iterations 3498 accuracy : 0.9300567107750473  loss : 0.3601383527389522\n",
      "iterations 3499 accuracy : 0.9300567107750473  loss : 0.3601179166473858\n",
      "iterations 3500 accuracy : 0.9300567107750473  loss : 0.36009721327552324\n",
      "iterations 3501 accuracy : 0.9300567107750473  loss : 0.36007721610825066\n",
      "iterations 3502 accuracy : 0.9300567107750473  loss : 0.36005746252116905\n",
      "iterations 3503 accuracy : 0.9300567107750473  loss : 0.36003782899116704\n",
      "iterations 3504 accuracy : 0.9300567107750473  loss : 0.3600179320723643\n",
      "iterations 3505 accuracy : 0.9300567107750473  loss : 0.35999658952186475\n",
      "iterations 3506 accuracy : 0.9300567107750473  loss : 0.3599771327822298\n",
      "iterations 3507 accuracy : 0.9300567107750473  loss : 0.3599583461379437\n",
      "iterations 3508 accuracy : 0.9300567107750473  loss : 0.3599387268090155\n",
      "iterations 3509 accuracy : 0.9300567107750473  loss : 0.35991889670613897\n",
      "iterations 3510 accuracy : 0.9300567107750473  loss : 0.3598994365494645\n",
      "iterations 3511 accuracy : 0.9300567107750473  loss : 0.3598800261243696\n",
      "iterations 3512 accuracy : 0.9300567107750473  loss : 0.3598595330799074\n",
      "iterations 3513 accuracy : 0.9300567107750473  loss : 0.3598396804322144\n",
      "iterations 3514 accuracy : 0.9300567107750473  loss : 0.3598203696639437\n",
      "iterations 3515 accuracy : 0.9300567107750473  loss : 0.35980018012555115\n",
      "iterations 3516 accuracy : 0.9300567107750473  loss : 0.35977959422754935\n",
      "iterations 3517 accuracy : 0.9300567107750473  loss : 0.3597597214051004\n",
      "iterations 3518 accuracy : 0.9300567107750473  loss : 0.35974008688482145\n",
      "iterations 3519 accuracy : 0.9300567107750473  loss : 0.3597215036263673\n",
      "iterations 3520 accuracy : 0.9300567107750473  loss : 0.3597015903037218\n",
      "iterations 3521 accuracy : 0.9300567107750473  loss : 0.35968159053102017\n",
      "iterations 3522 accuracy : 0.9300567107750473  loss : 0.35966198729735194\n",
      "iterations 3523 accuracy : 0.9300567107750473  loss : 0.35964242036419414\n",
      "iterations 3524 accuracy : 0.9300567107750473  loss : 0.35962267089460437\n",
      "iterations 3525 accuracy : 0.9302667506826297  loss : 0.3596028130311409\n",
      "iterations 3526 accuracy : 0.9300567107750473  loss : 0.3595825809928465\n",
      "iterations 3527 accuracy : 0.9300567107750473  loss : 0.3595635921338268\n",
      "iterations 3528 accuracy : 0.9300567107750473  loss : 0.3595433767784519\n",
      "iterations 3529 accuracy : 0.9300567107750473  loss : 0.35952422715968174\n",
      "iterations 3530 accuracy : 0.9300567107750473  loss : 0.35950561549446886\n",
      "iterations 3531 accuracy : 0.9300567107750473  loss : 0.359485605446726\n",
      "iterations 3532 accuracy : 0.9302667506826297  loss : 0.3594649891505577\n",
      "iterations 3533 accuracy : 0.9302667506826297  loss : 0.3594450499488324\n",
      "iterations 3534 accuracy : 0.9304767905902122  loss : 0.35942531606137246\n",
      "iterations 3535 accuracy : 0.9304767905902122  loss : 0.35940549165765234\n",
      "iterations 3536 accuracy : 0.9304767905902122  loss : 0.3593852440690728\n",
      "iterations 3537 accuracy : 0.9304767905902122  loss : 0.35936601316017597\n",
      "iterations 3538 accuracy : 0.9306868304977945  loss : 0.35934625543653415\n",
      "iterations 3539 accuracy : 0.9304767905902122  loss : 0.35932714041757974\n",
      "iterations 3540 accuracy : 0.9304767905902122  loss : 0.3593075654209221\n",
      "iterations 3541 accuracy : 0.9304767905902122  loss : 0.3592883604822463\n",
      "iterations 3542 accuracy : 0.9304767905902122  loss : 0.35926812656246027\n",
      "iterations 3543 accuracy : 0.9304767905902122  loss : 0.3592491820916016\n",
      "iterations 3544 accuracy : 0.9304767905902122  loss : 0.3592302326326525\n",
      "iterations 3545 accuracy : 0.9304767905902122  loss : 0.35921017811009015\n",
      "iterations 3546 accuracy : 0.9304767905902122  loss : 0.3591894287252864\n",
      "iterations 3547 accuracy : 0.9304767905902122  loss : 0.35916945491667934\n",
      "iterations 3548 accuracy : 0.9306868304977945  loss : 0.3591492817223979\n",
      "iterations 3549 accuracy : 0.9306868304977945  loss : 0.3591294432387115\n",
      "iterations 3550 accuracy : 0.9306868304977945  loss : 0.3591093818771826\n",
      "iterations 3551 accuracy : 0.9306868304977945  loss : 0.35909024433494413\n",
      "iterations 3552 accuracy : 0.9306868304977945  loss : 0.3590709452624551\n",
      "iterations 3553 accuracy : 0.9306868304977945  loss : 0.3590509915668966\n",
      "iterations 3554 accuracy : 0.9306868304977945  loss : 0.3590315331482888\n",
      "iterations 3555 accuracy : 0.9306868304977945  loss : 0.35901145830236614\n",
      "iterations 3556 accuracy : 0.9306868304977945  loss : 0.3589921574046257\n",
      "iterations 3557 accuracy : 0.9306868304977945  loss : 0.35897211221140995\n",
      "iterations 3558 accuracy : 0.9306868304977945  loss : 0.3589521985398684\n",
      "iterations 3559 accuracy : 0.9306868304977945  loss : 0.35893252104133905\n",
      "iterations 3560 accuracy : 0.9306868304977945  loss : 0.3589141211226618\n",
      "iterations 3561 accuracy : 0.9306868304977945  loss : 0.35889444540907645\n",
      "iterations 3562 accuracy : 0.9304767905902122  loss : 0.3588747266051157\n",
      "iterations 3563 accuracy : 0.9304767905902122  loss : 0.3588550530367124\n",
      "iterations 3564 accuracy : 0.9304767905902122  loss : 0.35883563622619563\n",
      "iterations 3565 accuracy : 0.9304767905902122  loss : 0.3588169182486859\n",
      "iterations 3566 accuracy : 0.9304767905902122  loss : 0.35879770925113097\n",
      "iterations 3567 accuracy : 0.9304767905902122  loss : 0.35877836405946895\n",
      "iterations 3568 accuracy : 0.9304767905902122  loss : 0.35875877103513376\n",
      "iterations 3569 accuracy : 0.9304767905902122  loss : 0.3587401055792228\n",
      "iterations 3570 accuracy : 0.9304767905902122  loss : 0.3587209566977455\n",
      "iterations 3571 accuracy : 0.9304767905902122  loss : 0.35870178660947594\n",
      "iterations 3572 accuracy : 0.9304767905902122  loss : 0.35868187916443484\n",
      "iterations 3573 accuracy : 0.9304767905902122  loss : 0.35866204287925246\n",
      "iterations 3574 accuracy : 0.9304767905902122  loss : 0.3586426060904133\n",
      "iterations 3575 accuracy : 0.9304767905902122  loss : 0.3586233363459097\n",
      "iterations 3576 accuracy : 0.9304767905902122  loss : 0.3586039014885637\n",
      "iterations 3577 accuracy : 0.9304767905902122  loss : 0.3585845000223378\n",
      "iterations 3578 accuracy : 0.9304767905902122  loss : 0.35856500943534597\n",
      "iterations 3579 accuracy : 0.9304767905902122  loss : 0.3585455915553099\n",
      "iterations 3580 accuracy : 0.9304767905902122  loss : 0.358526464643426\n",
      "iterations 3581 accuracy : 0.9304767905902122  loss : 0.35850675879837357\n",
      "iterations 3582 accuracy : 0.9304767905902122  loss : 0.35848741260973904\n",
      "iterations 3583 accuracy : 0.9304767905902122  loss : 0.35846769567839293\n",
      "iterations 3584 accuracy : 0.9304767905902122  loss : 0.3584483735380911\n",
      "iterations 3585 accuracy : 0.930896870405377  loss : 0.3584289359088827\n",
      "iterations 3586 accuracy : 0.9304767905902122  loss : 0.3584104023961485\n",
      "iterations 3587 accuracy : 0.9304767905902122  loss : 0.35839138075777716\n",
      "iterations 3588 accuracy : 0.930896870405377  loss : 0.3583722268561265\n",
      "iterations 3589 accuracy : 0.930896870405377  loss : 0.3583528975546221\n",
      "iterations 3590 accuracy : 0.9311069103129594  loss : 0.3583330095737419\n",
      "iterations 3591 accuracy : 0.9311069103129594  loss : 0.35831321865337334\n",
      "iterations 3592 accuracy : 0.9311069103129594  loss : 0.358294677590525\n",
      "iterations 3593 accuracy : 0.9311069103129594  loss : 0.35827514141317446\n",
      "iterations 3594 accuracy : 0.9311069103129594  loss : 0.35825601088703096\n",
      "iterations 3595 accuracy : 0.9311069103129594  loss : 0.35823591527503645\n",
      "iterations 3596 accuracy : 0.9311069103129594  loss : 0.3582167887206846\n",
      "iterations 3597 accuracy : 0.9311069103129594  loss : 0.35819726199537205\n",
      "iterations 3598 accuracy : 0.9311069103129594  loss : 0.3581778171953062\n",
      "iterations 3599 accuracy : 0.9311069103129594  loss : 0.3581586603564683\n",
      "iterations 3600 accuracy : 0.9311069103129594  loss : 0.358138043081603\n",
      "iterations 3601 accuracy : 0.930896870405377  loss : 0.35811836070644526\n",
      "iterations 3602 accuracy : 0.930896870405377  loss : 0.35809838695540114\n",
      "iterations 3603 accuracy : 0.930896870405377  loss : 0.35807981943526196\n",
      "iterations 3604 accuracy : 0.930896870405377  loss : 0.35806077906892014\n",
      "iterations 3605 accuracy : 0.930896870405377  loss : 0.35804188440201795\n",
      "iterations 3606 accuracy : 0.931316950220542  loss : 0.35802224703336955\n",
      "iterations 3607 accuracy : 0.931316950220542  loss : 0.3580034347908441\n",
      "iterations 3608 accuracy : 0.9311069103129594  loss : 0.3579853580025227\n",
      "iterations 3609 accuracy : 0.9311069103129594  loss : 0.35796605256022307\n",
      "iterations 3610 accuracy : 0.930896870405377  loss : 0.3579476068749875\n",
      "iterations 3611 accuracy : 0.930896870405377  loss : 0.35792881569844925\n",
      "iterations 3612 accuracy : 0.931316950220542  loss : 0.35790878176604973\n",
      "iterations 3613 accuracy : 0.931316950220542  loss : 0.35789021368844676\n",
      "iterations 3614 accuracy : 0.9315269901281243  loss : 0.3578711973754399\n",
      "iterations 3615 accuracy : 0.9315269901281243  loss : 0.3578513809587273\n",
      "iterations 3616 accuracy : 0.9315269901281243  loss : 0.3578320337286953\n",
      "iterations 3617 accuracy : 0.9315269901281243  loss : 0.3578127521440275\n",
      "iterations 3618 accuracy : 0.9315269901281243  loss : 0.35779368214219176\n",
      "iterations 3619 accuracy : 0.9315269901281243  loss : 0.35777497579378414\n",
      "iterations 3620 accuracy : 0.9315269901281243  loss : 0.3577559213112296\n",
      "iterations 3621 accuracy : 0.9315269901281243  loss : 0.3577374392964642\n",
      "iterations 3622 accuracy : 0.9315269901281243  loss : 0.35771902987363574\n",
      "iterations 3623 accuracy : 0.931316950220542  loss : 0.35770031334023467\n",
      "iterations 3624 accuracy : 0.931316950220542  loss : 0.357681814501419\n",
      "iterations 3625 accuracy : 0.9315269901281243  loss : 0.35766281669938105\n",
      "iterations 3626 accuracy : 0.9315269901281243  loss : 0.35764449268722875\n",
      "iterations 3627 accuracy : 0.931316950220542  loss : 0.3576256475140032\n",
      "iterations 3628 accuracy : 0.9315269901281243  loss : 0.35760659392307037\n",
      "iterations 3629 accuracy : 0.9315269901281243  loss : 0.35758803650875576\n",
      "iterations 3630 accuracy : 0.9315269901281243  loss : 0.35756976125994666\n",
      "iterations 3631 accuracy : 0.9315269901281243  loss : 0.3575504878723012\n",
      "iterations 3632 accuracy : 0.9315269901281243  loss : 0.35753118361242975\n",
      "iterations 3633 accuracy : 0.9315269901281243  loss : 0.3575116832695201\n",
      "iterations 3634 accuracy : 0.9317370300357067  loss : 0.3574925567765579\n",
      "iterations 3635 accuracy : 0.9317370300357067  loss : 0.35747351092202756\n",
      "iterations 3636 accuracy : 0.9317370300357067  loss : 0.3574538432720701\n",
      "iterations 3637 accuracy : 0.9317370300357067  loss : 0.35743565113118914\n",
      "iterations 3638 accuracy : 0.9319470699432892  loss : 0.3574164295426571\n",
      "iterations 3639 accuracy : 0.9319470699432892  loss : 0.35739749619158573\n",
      "iterations 3640 accuracy : 0.9319470699432892  loss : 0.35737887067755975\n",
      "iterations 3641 accuracy : 0.9319470699432892  loss : 0.35735953292274714\n",
      "iterations 3642 accuracy : 0.9319470699432892  loss : 0.35734066959343397\n",
      "iterations 3643 accuracy : 0.9319470699432892  loss : 0.3573218545309989\n",
      "iterations 3644 accuracy : 0.9319470699432892  loss : 0.357302899923882\n",
      "iterations 3645 accuracy : 0.9319470699432892  loss : 0.35728367451811255\n",
      "iterations 3646 accuracy : 0.9319470699432892  loss : 0.3572649262816654\n",
      "iterations 3647 accuracy : 0.9319470699432892  loss : 0.3572455447582377\n",
      "iterations 3648 accuracy : 0.9319470699432892  loss : 0.3572266632893386\n",
      "iterations 3649 accuracy : 0.9319470699432892  loss : 0.35720761878085855\n",
      "iterations 3650 accuracy : 0.9319470699432892  loss : 0.35718888021349227\n",
      "iterations 3651 accuracy : 0.9317370300357067  loss : 0.35717000921203723\n",
      "iterations 3652 accuracy : 0.9317370300357067  loss : 0.357151049316971\n",
      "iterations 3653 accuracy : 0.9317370300357067  loss : 0.3571322192905254\n",
      "iterations 3654 accuracy : 0.9317370300357067  loss : 0.3571135785568067\n",
      "iterations 3655 accuracy : 0.9317370300357067  loss : 0.3570948144792743\n",
      "iterations 3656 accuracy : 0.9317370300357067  loss : 0.3570760332024173\n",
      "iterations 3657 accuracy : 0.9317370300357067  loss : 0.3570570565720293\n",
      "iterations 3658 accuracy : 0.9317370300357067  loss : 0.35703907700496684\n",
      "iterations 3659 accuracy : 0.9317370300357067  loss : 0.3570202713234205\n",
      "iterations 3660 accuracy : 0.9317370300357067  loss : 0.357002000201775\n",
      "iterations 3661 accuracy : 0.9317370300357067  loss : 0.3569837391462345\n",
      "iterations 3662 accuracy : 0.9317370300357067  loss : 0.3569659256101048\n",
      "iterations 3663 accuracy : 0.9317370300357067  loss : 0.35694791190923475\n",
      "iterations 3664 accuracy : 0.9317370300357067  loss : 0.3569285554039418\n",
      "iterations 3665 accuracy : 0.9317370300357067  loss : 0.356910219780024\n",
      "iterations 3666 accuracy : 0.9317370300357067  loss : 0.35689132790063643\n",
      "iterations 3667 accuracy : 0.9317370300357067  loss : 0.35687241812726833\n",
      "iterations 3668 accuracy : 0.9317370300357067  loss : 0.3568541279281201\n",
      "iterations 3669 accuracy : 0.9317370300357067  loss : 0.3568358227216671\n",
      "iterations 3670 accuracy : 0.9317370300357067  loss : 0.35681700452516407\n",
      "iterations 3671 accuracy : 0.9317370300357067  loss : 0.35679815773674595\n",
      "iterations 3672 accuracy : 0.9317370300357067  loss : 0.3567803800851472\n",
      "iterations 3673 accuracy : 0.9317370300357067  loss : 0.3567617395477321\n",
      "iterations 3674 accuracy : 0.9317370300357067  loss : 0.35674296201815264\n",
      "iterations 3675 accuracy : 0.9315269901281243  loss : 0.3567240356383354\n",
      "iterations 3676 accuracy : 0.9317370300357067  loss : 0.35670606592472653\n",
      "iterations 3677 accuracy : 0.9315269901281243  loss : 0.3566867871424222\n",
      "iterations 3678 accuracy : 0.9317370300357067  loss : 0.3566693109042648\n",
      "iterations 3679 accuracy : 0.9317370300357067  loss : 0.3566510169362102\n",
      "iterations 3680 accuracy : 0.9317370300357067  loss : 0.35663266568790014\n",
      "iterations 3681 accuracy : 0.9317370300357067  loss : 0.35661414904888955\n",
      "iterations 3682 accuracy : 0.9317370300357067  loss : 0.35659563236382064\n",
      "iterations 3683 accuracy : 0.9317370300357067  loss : 0.3565771996671629\n",
      "iterations 3684 accuracy : 0.9317370300357067  loss : 0.35655884124155446\n",
      "iterations 3685 accuracy : 0.9317370300357067  loss : 0.3565402398235938\n",
      "iterations 3686 accuracy : 0.9317370300357067  loss : 0.35652187841904753\n",
      "iterations 3687 accuracy : 0.9317370300357067  loss : 0.35650342830738774\n",
      "iterations 3688 accuracy : 0.9317370300357067  loss : 0.3564859160594787\n",
      "iterations 3689 accuracy : 0.9317370300357067  loss : 0.35646770508290143\n",
      "iterations 3690 accuracy : 0.9317370300357067  loss : 0.35644899285686993\n",
      "iterations 3691 accuracy : 0.9317370300357067  loss : 0.3564303680984041\n",
      "iterations 3692 accuracy : 0.9317370300357067  loss : 0.35641200723473604\n",
      "iterations 3693 accuracy : 0.9317370300357067  loss : 0.35639486583465296\n",
      "iterations 3694 accuracy : 0.9317370300357067  loss : 0.35637627100737884\n",
      "iterations 3695 accuracy : 0.9317370300357067  loss : 0.3563569206867986\n",
      "iterations 3696 accuracy : 0.9317370300357067  loss : 0.3563385232933634\n",
      "iterations 3697 accuracy : 0.9317370300357067  loss : 0.356319234701093\n",
      "iterations 3698 accuracy : 0.9315269901281243  loss : 0.35630116120257005\n",
      "iterations 3699 accuracy : 0.9315269901281243  loss : 0.3562829119196021\n",
      "iterations 3700 accuracy : 0.9315269901281243  loss : 0.3562642534745313\n",
      "iterations 3701 accuracy : 0.9315269901281243  loss : 0.35624708279758915\n",
      "iterations 3702 accuracy : 0.9317370300357067  loss : 0.35622910604462676\n",
      "iterations 3703 accuracy : 0.9317370300357067  loss : 0.35621156742289156\n",
      "iterations 3704 accuracy : 0.9317370300357067  loss : 0.3561926926005502\n",
      "iterations 3705 accuracy : 0.9315269901281243  loss : 0.3561736044354764\n",
      "iterations 3706 accuracy : 0.9315269901281243  loss : 0.35615524342776905\n",
      "iterations 3707 accuracy : 0.9315269901281243  loss : 0.3561371028546255\n",
      "iterations 3708 accuracy : 0.9315269901281243  loss : 0.3561192626935687\n",
      "iterations 3709 accuracy : 0.9315269901281243  loss : 0.3561011584463542\n",
      "iterations 3710 accuracy : 0.9315269901281243  loss : 0.35608285763129593\n",
      "iterations 3711 accuracy : 0.9317370300357067  loss : 0.35606451772516834\n",
      "iterations 3712 accuracy : 0.9317370300357067  loss : 0.35604637634316266\n",
      "iterations 3713 accuracy : 0.9317370300357067  loss : 0.3560273603538635\n",
      "iterations 3714 accuracy : 0.9317370300357067  loss : 0.35600868224456905\n",
      "iterations 3715 accuracy : 0.9317370300357067  loss : 0.35599022443863837\n",
      "iterations 3716 accuracy : 0.9317370300357067  loss : 0.3559719772258589\n",
      "iterations 3717 accuracy : 0.9315269901281243  loss : 0.35595326253569526\n",
      "iterations 3718 accuracy : 0.9315269901281243  loss : 0.35593518846059924\n",
      "iterations 3719 accuracy : 0.9315269901281243  loss : 0.3559168859891949\n",
      "iterations 3720 accuracy : 0.9315269901281243  loss : 0.3558982366902876\n",
      "iterations 3721 accuracy : 0.9315269901281243  loss : 0.35588074802591013\n",
      "iterations 3722 accuracy : 0.9315269901281243  loss : 0.35586247401580395\n",
      "iterations 3723 accuracy : 0.9315269901281243  loss : 0.3558445431604196\n",
      "iterations 3724 accuracy : 0.9315269901281243  loss : 0.3558262046874662\n",
      "iterations 3725 accuracy : 0.9315269901281243  loss : 0.3558071872161716\n",
      "iterations 3726 accuracy : 0.9315269901281243  loss : 0.3557883259071896\n",
      "iterations 3727 accuracy : 0.9315269901281243  loss : 0.3557704192733808\n",
      "iterations 3728 accuracy : 0.9315269901281243  loss : 0.35575256888861406\n",
      "iterations 3729 accuracy : 0.9315269901281243  loss : 0.3557342164086621\n",
      "iterations 3730 accuracy : 0.9315269901281243  loss : 0.3557158774975398\n",
      "iterations 3731 accuracy : 0.9315269901281243  loss : 0.35569811583267286\n",
      "iterations 3732 accuracy : 0.9315269901281243  loss : 0.3556790733965975\n",
      "iterations 3733 accuracy : 0.9315269901281243  loss : 0.3556604267381911\n",
      "iterations 3734 accuracy : 0.9315269901281243  loss : 0.35564238101926293\n",
      "iterations 3735 accuracy : 0.9315269901281243  loss : 0.3556254167045422\n",
      "iterations 3736 accuracy : 0.9315269901281243  loss : 0.35560703095134333\n",
      "iterations 3737 accuracy : 0.9315269901281243  loss : 0.355588648926904\n",
      "iterations 3738 accuracy : 0.9315269901281243  loss : 0.3555707178363494\n",
      "iterations 3739 accuracy : 0.9317370300357067  loss : 0.3555523378114511\n",
      "iterations 3740 accuracy : 0.9317370300357067  loss : 0.3555338386644775\n",
      "iterations 3741 accuracy : 0.9315269901281243  loss : 0.355516107228666\n",
      "iterations 3742 accuracy : 0.9317370300357067  loss : 0.35549819275780753\n",
      "iterations 3743 accuracy : 0.9317370300357067  loss : 0.3554795760313253\n",
      "iterations 3744 accuracy : 0.9317370300357067  loss : 0.35546109415346666\n",
      "iterations 3745 accuracy : 0.9317370300357067  loss : 0.3554431438632048\n",
      "iterations 3746 accuracy : 0.9317370300357067  loss : 0.3554251153571118\n",
      "iterations 3747 accuracy : 0.9317370300357067  loss : 0.3554079609078421\n",
      "iterations 3748 accuracy : 0.9317370300357067  loss : 0.3553897037075725\n",
      "iterations 3749 accuracy : 0.9317370300357067  loss : 0.35537146733239255\n",
      "iterations 3750 accuracy : 0.9319470699432892  loss : 0.35535284344565066\n",
      "iterations 3751 accuracy : 0.9319470699432892  loss : 0.35533388260360593\n",
      "iterations 3752 accuracy : 0.9319470699432892  loss : 0.3553165172808216\n",
      "iterations 3753 accuracy : 0.9319470699432892  loss : 0.3552985165822919\n",
      "iterations 3754 accuracy : 0.9319470699432892  loss : 0.35528032501684864\n",
      "iterations 3755 accuracy : 0.9319470699432892  loss : 0.35526236304996556\n",
      "iterations 3756 accuracy : 0.9319470699432892  loss : 0.3552439505860537\n",
      "iterations 3757 accuracy : 0.9319470699432892  loss : 0.35522560399425274\n",
      "iterations 3758 accuracy : 0.9319470699432892  loss : 0.35520733529546783\n",
      "iterations 3759 accuracy : 0.9319470699432892  loss : 0.35518879682965115\n",
      "iterations 3760 accuracy : 0.9319470699432892  loss : 0.3551706391295181\n",
      "iterations 3761 accuracy : 0.9319470699432892  loss : 0.35515261537757253\n",
      "iterations 3762 accuracy : 0.9319470699432892  loss : 0.35513523550534637\n",
      "iterations 3763 accuracy : 0.9319470699432892  loss : 0.3551180344741925\n",
      "iterations 3764 accuracy : 0.9319470699432892  loss : 0.3551003046647168\n",
      "iterations 3765 accuracy : 0.9317370300357067  loss : 0.3550821603670784\n",
      "iterations 3766 accuracy : 0.9319470699432892  loss : 0.35506375486305214\n",
      "iterations 3767 accuracy : 0.9319470699432892  loss : 0.3550453414904079\n",
      "iterations 3768 accuracy : 0.9319470699432892  loss : 0.3550272260926497\n",
      "iterations 3769 accuracy : 0.9319470699432892  loss : 0.35501006088595094\n",
      "iterations 3770 accuracy : 0.9319470699432892  loss : 0.35499181787224504\n",
      "iterations 3771 accuracy : 0.9319470699432892  loss : 0.3549742907780228\n",
      "iterations 3772 accuracy : 0.9319470699432892  loss : 0.3549565815744687\n",
      "iterations 3773 accuracy : 0.9317370300357067  loss : 0.354939239574739\n",
      "iterations 3774 accuracy : 0.9317370300357067  loss : 0.3549217384567123\n",
      "iterations 3775 accuracy : 0.9317370300357067  loss : 0.3549038282350841\n",
      "iterations 3776 accuracy : 0.9317370300357067  loss : 0.3548861798028521\n",
      "iterations 3777 accuracy : 0.9317370300357067  loss : 0.3548691525942739\n",
      "iterations 3778 accuracy : 0.9317370300357067  loss : 0.3548511423306254\n",
      "iterations 3779 accuracy : 0.9317370300357067  loss : 0.3548338076762767\n",
      "iterations 3780 accuracy : 0.9317370300357067  loss : 0.3548160307744128\n",
      "iterations 3781 accuracy : 0.9317370300357067  loss : 0.3547982439951129\n",
      "iterations 3782 accuracy : 0.9317370300357067  loss : 0.35478034246244394\n",
      "iterations 3783 accuracy : 0.9317370300357067  loss : 0.35476228696371537\n",
      "iterations 3784 accuracy : 0.9317370300357067  loss : 0.3547445317367415\n",
      "iterations 3785 accuracy : 0.9317370300357067  loss : 0.3547271602205248\n",
      "iterations 3786 accuracy : 0.9317370300357067  loss : 0.35470892022046907\n",
      "iterations 3787 accuracy : 0.9317370300357067  loss : 0.3546912987728159\n",
      "iterations 3788 accuracy : 0.9317370300357067  loss : 0.35467341186269735\n",
      "iterations 3789 accuracy : 0.9317370300357067  loss : 0.3546553188126128\n",
      "iterations 3790 accuracy : 0.9319470699432892  loss : 0.3546367110197187\n",
      "iterations 3791 accuracy : 0.9319470699432892  loss : 0.35461890854472083\n",
      "iterations 3792 accuracy : 0.9317370300357067  loss : 0.3546017775890412\n",
      "iterations 3793 accuracy : 0.9317370300357067  loss : 0.3545838021673768\n",
      "iterations 3794 accuracy : 0.9317370300357067  loss : 0.35456609844934545\n",
      "iterations 3795 accuracy : 0.9317370300357067  loss : 0.3545483759018633\n",
      "iterations 3796 accuracy : 0.9317370300357067  loss : 0.3545308041399823\n",
      "iterations 3797 accuracy : 0.9317370300357067  loss : 0.3545133446806383\n",
      "iterations 3798 accuracy : 0.9317370300357067  loss : 0.3544957593152227\n",
      "iterations 3799 accuracy : 0.9317370300357067  loss : 0.35447775963665223\n",
      "iterations 3800 accuracy : 0.9317370300357067  loss : 0.35446016615421555\n",
      "iterations 3801 accuracy : 0.9317370300357067  loss : 0.35444305921152874\n",
      "iterations 3802 accuracy : 0.9317370300357067  loss : 0.35442494854517503\n",
      "iterations 3803 accuracy : 0.9319470699432892  loss : 0.35440655938079546\n",
      "iterations 3804 accuracy : 0.9319470699432892  loss : 0.3543885145528966\n",
      "iterations 3805 accuracy : 0.9319470699432892  loss : 0.3543712312050976\n",
      "iterations 3806 accuracy : 0.9319470699432892  loss : 0.3543539623315714\n",
      "iterations 3807 accuracy : 0.9319470699432892  loss : 0.3543361754371837\n",
      "iterations 3808 accuracy : 0.9319470699432892  loss : 0.35431821561313365\n",
      "iterations 3809 accuracy : 0.9319470699432892  loss : 0.35430083417920405\n",
      "iterations 3810 accuracy : 0.9319470699432892  loss : 0.3542828919500535\n",
      "iterations 3811 accuracy : 0.9319470699432892  loss : 0.3542648256806634\n",
      "iterations 3812 accuracy : 0.9319470699432892  loss : 0.35424722343582854\n",
      "iterations 3813 accuracy : 0.9319470699432892  loss : 0.3542302681622238\n",
      "iterations 3814 accuracy : 0.9317370300357067  loss : 0.3542133381378309\n",
      "iterations 3815 accuracy : 0.9317370300357067  loss : 0.35419606749718385\n",
      "iterations 3816 accuracy : 0.9317370300357067  loss : 0.3541782373366037\n",
      "iterations 3817 accuracy : 0.9317370300357067  loss : 0.3541611539369513\n",
      "iterations 3818 accuracy : 0.9317370300357067  loss : 0.35414369205581153\n",
      "iterations 3819 accuracy : 0.9319470699432892  loss : 0.3541256561567915\n",
      "iterations 3820 accuracy : 0.9319470699432892  loss : 0.35410724762473417\n",
      "iterations 3821 accuracy : 0.9319470699432892  loss : 0.3540893638486155\n",
      "iterations 3822 accuracy : 0.9319470699432892  loss : 0.3540716174988538\n",
      "iterations 3823 accuracy : 0.9319470699432892  loss : 0.3540539830647322\n",
      "iterations 3824 accuracy : 0.9319470699432892  loss : 0.3540349753585297\n",
      "iterations 3825 accuracy : 0.9321571098508716  loss : 0.3540166748937993\n",
      "iterations 3826 accuracy : 0.9321571098508716  loss : 0.3539997476699529\n",
      "iterations 3827 accuracy : 0.9321571098508716  loss : 0.35398188035380923\n",
      "iterations 3828 accuracy : 0.9321571098508716  loss : 0.3539637385840879\n",
      "iterations 3829 accuracy : 0.9321571098508716  loss : 0.3539470176045988\n",
      "iterations 3830 accuracy : 0.9321571098508716  loss : 0.3539292189213012\n",
      "iterations 3831 accuracy : 0.9319470699432892  loss : 0.35391256681377375\n",
      "iterations 3832 accuracy : 0.9319470699432892  loss : 0.3538955490804693\n",
      "iterations 3833 accuracy : 0.9319470699432892  loss : 0.35387784312193227\n",
      "iterations 3834 accuracy : 0.9321571098508716  loss : 0.35385900243598417\n",
      "iterations 3835 accuracy : 0.9321571098508716  loss : 0.3538418853207568\n",
      "iterations 3836 accuracy : 0.9321571098508716  loss : 0.3538246675706706\n",
      "iterations 3837 accuracy : 0.9321571098508716  loss : 0.3538070602966175\n",
      "iterations 3838 accuracy : 0.9321571098508716  loss : 0.3537889411096995\n",
      "iterations 3839 accuracy : 0.9321571098508716  loss : 0.3537717044404919\n",
      "iterations 3840 accuracy : 0.9321571098508716  loss : 0.3537544670267006\n",
      "iterations 3841 accuracy : 0.9321571098508716  loss : 0.3537364730541898\n",
      "iterations 3842 accuracy : 0.9321571098508716  loss : 0.35371869317302207\n",
      "iterations 3843 accuracy : 0.9321571098508716  loss : 0.353700688744186\n",
      "iterations 3844 accuracy : 0.9321571098508716  loss : 0.3536833604885597\n",
      "iterations 3845 accuracy : 0.9321571098508716  loss : 0.35366605936713563\n",
      "iterations 3846 accuracy : 0.9321571098508716  loss : 0.3536501514066892\n",
      "iterations 3847 accuracy : 0.9321571098508716  loss : 0.3536329493847848\n",
      "iterations 3848 accuracy : 0.9321571098508716  loss : 0.3536149561200315\n",
      "iterations 3849 accuracy : 0.9321571098508716  loss : 0.35359769710741384\n",
      "iterations 3850 accuracy : 0.9321571098508716  loss : 0.3535801362056039\n",
      "iterations 3851 accuracy : 0.9321571098508716  loss : 0.3535625555592545\n",
      "iterations 3852 accuracy : 0.9321571098508716  loss : 0.35354521444019243\n",
      "iterations 3853 accuracy : 0.9321571098508716  loss : 0.3535281761983125\n",
      "iterations 3854 accuracy : 0.9321571098508716  loss : 0.3535110661238131\n",
      "iterations 3855 accuracy : 0.9321571098508716  loss : 0.35349403151880016\n",
      "iterations 3856 accuracy : 0.9321571098508716  loss : 0.35347703935901204\n",
      "iterations 3857 accuracy : 0.9321571098508716  loss : 0.35345969085025225\n",
      "iterations 3858 accuracy : 0.9321571098508716  loss : 0.35344328338943054\n",
      "iterations 3859 accuracy : 0.9321571098508716  loss : 0.35342588742430187\n",
      "iterations 3860 accuracy : 0.9321571098508716  loss : 0.35340843162359337\n",
      "iterations 3861 accuracy : 0.9321571098508716  loss : 0.3533912119716364\n",
      "iterations 3862 accuracy : 0.9321571098508716  loss : 0.35337326702796984\n",
      "iterations 3863 accuracy : 0.9321571098508716  loss : 0.3533556210086814\n",
      "iterations 3864 accuracy : 0.9321571098508716  loss : 0.3533378680130709\n",
      "iterations 3865 accuracy : 0.9321571098508716  loss : 0.3533215165772514\n",
      "iterations 3866 accuracy : 0.9321571098508716  loss : 0.35330383580422264\n",
      "iterations 3867 accuracy : 0.9321571098508716  loss : 0.3532859197487701\n",
      "iterations 3868 accuracy : 0.9321571098508716  loss : 0.3532681917571928\n",
      "iterations 3869 accuracy : 0.9321571098508716  loss : 0.35325236509426666\n",
      "iterations 3870 accuracy : 0.9321571098508716  loss : 0.35323422549011435\n",
      "iterations 3871 accuracy : 0.9321571098508716  loss : 0.3532167464550065\n",
      "iterations 3872 accuracy : 0.9321571098508716  loss : 0.3531982559713058\n",
      "iterations 3873 accuracy : 0.9321571098508716  loss : 0.35318055085248046\n",
      "iterations 3874 accuracy : 0.9321571098508716  loss : 0.3531645980890444\n",
      "iterations 3875 accuracy : 0.9321571098508716  loss : 0.35314668044071074\n",
      "iterations 3876 accuracy : 0.9321571098508716  loss : 0.35312991143719424\n",
      "iterations 3877 accuracy : 0.9321571098508716  loss : 0.35311260730007454\n",
      "iterations 3878 accuracy : 0.9321571098508716  loss : 0.3530949601256678\n",
      "iterations 3879 accuracy : 0.9321571098508716  loss : 0.3530778506809955\n",
      "iterations 3880 accuracy : 0.9321571098508716  loss : 0.353060664092471\n",
      "iterations 3881 accuracy : 0.9321571098508716  loss : 0.35304298977608073\n",
      "iterations 3882 accuracy : 0.9321571098508716  loss : 0.35302544752084736\n",
      "iterations 3883 accuracy : 0.9321571098508716  loss : 0.35300844560040445\n",
      "iterations 3884 accuracy : 0.9321571098508716  loss : 0.3529906452957128\n",
      "iterations 3885 accuracy : 0.9321571098508716  loss : 0.3529737833353916\n",
      "iterations 3886 accuracy : 0.9321571098508716  loss : 0.3529566573660144\n",
      "iterations 3887 accuracy : 0.9321571098508716  loss : 0.35293999881212534\n",
      "iterations 3888 accuracy : 0.9321571098508716  loss : 0.3529239323927052\n",
      "iterations 3889 accuracy : 0.9321571098508716  loss : 0.35290605725400537\n",
      "iterations 3890 accuracy : 0.9321571098508716  loss : 0.3528884973795987\n",
      "iterations 3891 accuracy : 0.9321571098508716  loss : 0.35287107672276674\n",
      "iterations 3892 accuracy : 0.9321571098508716  loss : 0.35285395327619373\n",
      "iterations 3893 accuracy : 0.9321571098508716  loss : 0.3528376510434834\n",
      "iterations 3894 accuracy : 0.9321571098508716  loss : 0.3528200675829872\n",
      "iterations 3895 accuracy : 0.9321571098508716  loss : 0.3528029093519537\n",
      "iterations 3896 accuracy : 0.9321571098508716  loss : 0.35278640002212514\n",
      "iterations 3897 accuracy : 0.9321571098508716  loss : 0.3527692566749651\n",
      "iterations 3898 accuracy : 0.9323671497584541  loss : 0.352751446259207\n",
      "iterations 3899 accuracy : 0.9323671497584541  loss : 0.35273443867226784\n",
      "iterations 3900 accuracy : 0.9323671497584541  loss : 0.3527178011154121\n",
      "iterations 3901 accuracy : 0.9323671497584541  loss : 0.3527004167704582\n",
      "iterations 3902 accuracy : 0.9323671497584541  loss : 0.35268393559046823\n",
      "iterations 3903 accuracy : 0.9321571098508716  loss : 0.35266540403417507\n",
      "iterations 3904 accuracy : 0.9317370300357067  loss : 0.35264821127037466\n",
      "iterations 3905 accuracy : 0.9317370300357067  loss : 0.3526308060365781\n",
      "iterations 3906 accuracy : 0.931316950220542  loss : 0.3526134649305706\n",
      "iterations 3907 accuracy : 0.931316950220542  loss : 0.35259740076582713\n",
      "iterations 3908 accuracy : 0.931316950220542  loss : 0.35258000967585673\n",
      "iterations 3909 accuracy : 0.931316950220542  loss : 0.35256274742432253\n",
      "iterations 3910 accuracy : 0.931316950220542  loss : 0.35254536839738326\n",
      "iterations 3911 accuracy : 0.931316950220542  loss : 0.3525286528608257\n",
      "iterations 3912 accuracy : 0.931316950220542  loss : 0.35251151780141954\n",
      "iterations 3913 accuracy : 0.931316950220542  loss : 0.3524949760308713\n",
      "iterations 3914 accuracy : 0.931316950220542  loss : 0.35247816729874226\n",
      "iterations 3915 accuracy : 0.931316950220542  loss : 0.3524608121880267\n",
      "iterations 3916 accuracy : 0.931316950220542  loss : 0.35244458663902856\n",
      "iterations 3917 accuracy : 0.931316950220542  loss : 0.3524281646946281\n",
      "iterations 3918 accuracy : 0.931316950220542  loss : 0.3524114077401525\n",
      "iterations 3919 accuracy : 0.931316950220542  loss : 0.3523942824284812\n",
      "iterations 3920 accuracy : 0.931316950220542  loss : 0.3523770799365764\n",
      "iterations 3921 accuracy : 0.931316950220542  loss : 0.352360111497629\n",
      "iterations 3922 accuracy : 0.931316950220542  loss : 0.3523431486260545\n",
      "iterations 3923 accuracy : 0.9315269901281243  loss : 0.35232634949453434\n",
      "iterations 3924 accuracy : 0.9315269901281243  loss : 0.35230994911468927\n",
      "iterations 3925 accuracy : 0.9315269901281243  loss : 0.3522920706277006\n",
      "iterations 3926 accuracy : 0.9315269901281243  loss : 0.35227500976058473\n",
      "iterations 3927 accuracy : 0.9315269901281243  loss : 0.352258626273542\n",
      "iterations 3928 accuracy : 0.9315269901281243  loss : 0.35224175352809145\n",
      "iterations 3929 accuracy : 0.9315269901281243  loss : 0.35222489513126115\n",
      "iterations 3930 accuracy : 0.9315269901281243  loss : 0.35220802235491827\n",
      "iterations 3931 accuracy : 0.9315269901281243  loss : 0.3521913958931253\n",
      "iterations 3932 accuracy : 0.9315269901281243  loss : 0.3521745372403419\n",
      "iterations 3933 accuracy : 0.9315269901281243  loss : 0.352158421866885\n",
      "iterations 3934 accuracy : 0.9315269901281243  loss : 0.35214119716664677\n",
      "iterations 3935 accuracy : 0.9315269901281243  loss : 0.35212424267034287\n",
      "iterations 3936 accuracy : 0.9315269901281243  loss : 0.3521072456310749\n",
      "iterations 3937 accuracy : 0.9315269901281243  loss : 0.35209025352822765\n",
      "iterations 3938 accuracy : 0.9315269901281243  loss : 0.35207366758934133\n",
      "iterations 3939 accuracy : 0.9315269901281243  loss : 0.35205710242283417\n",
      "iterations 3940 accuracy : 0.9315269901281243  loss : 0.3520402133410034\n",
      "iterations 3941 accuracy : 0.9315269901281243  loss : 0.3520234255110123\n",
      "iterations 3942 accuracy : 0.9315269901281243  loss : 0.35200653235746476\n",
      "iterations 3943 accuracy : 0.9315269901281243  loss : 0.3519892968251248\n",
      "iterations 3944 accuracy : 0.9315269901281243  loss : 0.3519724266515806\n",
      "iterations 3945 accuracy : 0.9315269901281243  loss : 0.35195576532397416\n",
      "iterations 3946 accuracy : 0.9315269901281243  loss : 0.3519390036836339\n",
      "iterations 3947 accuracy : 0.9315269901281243  loss : 0.35192188371001715\n",
      "iterations 3948 accuracy : 0.9315269901281243  loss : 0.35190455024707873\n",
      "iterations 3949 accuracy : 0.9315269901281243  loss : 0.3518883355489925\n",
      "iterations 3950 accuracy : 0.9315269901281243  loss : 0.3518724242090618\n",
      "iterations 3951 accuracy : 0.9315269901281243  loss : 0.3518560333345962\n",
      "iterations 3952 accuracy : 0.9315269901281243  loss : 0.3518399549231678\n",
      "iterations 3953 accuracy : 0.9315269901281243  loss : 0.35182308199790174\n",
      "iterations 3954 accuracy : 0.9315269901281243  loss : 0.35180578471647594\n",
      "iterations 3955 accuracy : 0.9315269901281243  loss : 0.3517892959321135\n",
      "iterations 3956 accuracy : 0.9315269901281243  loss : 0.35177215826974356\n",
      "iterations 3957 accuracy : 0.9315269901281243  loss : 0.3517548367645861\n",
      "iterations 3958 accuracy : 0.9317370300357067  loss : 0.3517377504879423\n",
      "iterations 3959 accuracy : 0.9317370300357067  loss : 0.35172111606882445\n",
      "iterations 3960 accuracy : 0.9317370300357067  loss : 0.35170405318931497\n",
      "iterations 3961 accuracy : 0.9317370300357067  loss : 0.351687520885691\n",
      "iterations 3962 accuracy : 0.9317370300357067  loss : 0.3516714889737442\n",
      "iterations 3963 accuracy : 0.9317370300357067  loss : 0.35165405202509065\n",
      "iterations 3964 accuracy : 0.9317370300357067  loss : 0.35163794038237106\n",
      "iterations 3965 accuracy : 0.9317370300357067  loss : 0.3516213605138365\n",
      "iterations 3966 accuracy : 0.9317370300357067  loss : 0.3516049486935876\n",
      "iterations 3967 accuracy : 0.9317370300357067  loss : 0.35158846379175723\n",
      "iterations 3968 accuracy : 0.9317370300357067  loss : 0.35157187485959035\n",
      "iterations 3969 accuracy : 0.9317370300357067  loss : 0.35155480886000445\n",
      "iterations 3970 accuracy : 0.9317370300357067  loss : 0.35153846074218953\n",
      "iterations 3971 accuracy : 0.9317370300357067  loss : 0.35152155252751144\n",
      "iterations 3972 accuracy : 0.9317370300357067  loss : 0.35150507587407276\n",
      "iterations 3973 accuracy : 0.9317370300357067  loss : 0.35148801401804763\n",
      "iterations 3974 accuracy : 0.9317370300357067  loss : 0.3514724316650952\n",
      "iterations 3975 accuracy : 0.9317370300357067  loss : 0.35145591218876376\n",
      "iterations 3976 accuracy : 0.9317370300357067  loss : 0.35143913939552707\n",
      "iterations 3977 accuracy : 0.9317370300357067  loss : 0.35142207798555375\n",
      "iterations 3978 accuracy : 0.9317370300357067  loss : 0.35140545963179753\n",
      "iterations 3979 accuracy : 0.9317370300357067  loss : 0.351388537485587\n",
      "iterations 3980 accuracy : 0.9317370300357067  loss : 0.3513717128676358\n",
      "iterations 3981 accuracy : 0.9317370300357067  loss : 0.35135516294334274\n",
      "iterations 3982 accuracy : 0.9317370300357067  loss : 0.35133894198317267\n",
      "iterations 3983 accuracy : 0.9317370300357067  loss : 0.3513226641794754\n",
      "iterations 3984 accuracy : 0.9317370300357067  loss : 0.35130586313876666\n",
      "iterations 3985 accuracy : 0.9317370300357067  loss : 0.35128931141208514\n",
      "iterations 3986 accuracy : 0.9317370300357067  loss : 0.3512721033997325\n",
      "iterations 3987 accuracy : 0.9317370300357067  loss : 0.35125594381952935\n",
      "iterations 3988 accuracy : 0.9317370300357067  loss : 0.3512397399554434\n",
      "iterations 3989 accuracy : 0.9317370300357067  loss : 0.35122392144841835\n",
      "iterations 3990 accuracy : 0.9317370300357067  loss : 0.3512073658826456\n",
      "iterations 3991 accuracy : 0.9317370300357067  loss : 0.3511902494619833\n",
      "iterations 3992 accuracy : 0.9317370300357067  loss : 0.35117371485283694\n",
      "iterations 3993 accuracy : 0.9317370300357067  loss : 0.35115719160483233\n",
      "iterations 3994 accuracy : 0.9317370300357067  loss : 0.35114042138628804\n",
      "iterations 3995 accuracy : 0.9317370300357067  loss : 0.3511242857883443\n",
      "iterations 3996 accuracy : 0.9317370300357067  loss : 0.3511080164683338\n",
      "iterations 3997 accuracy : 0.9317370300357067  loss : 0.35109155846059703\n",
      "iterations 3998 accuracy : 0.9317370300357067  loss : 0.35107440742498164\n",
      "iterations 3999 accuracy : 0.9317370300357067  loss : 0.35105769825527744\n",
      "iterations 4000 accuracy : 0.9317370300357067  loss : 0.35104154565429807\n",
      "iterations 4001 accuracy : 0.9317370300357067  loss : 0.3510254368440801\n",
      "iterations 4002 accuracy : 0.9317370300357067  loss : 0.3510089759497841\n",
      "iterations 4003 accuracy : 0.9317370300357067  loss : 0.35099241356913746\n",
      "iterations 4004 accuracy : 0.9317370300357067  loss : 0.3509762945690126\n",
      "iterations 4005 accuracy : 0.9317370300357067  loss : 0.3509597968228208\n",
      "iterations 4006 accuracy : 0.9317370300357067  loss : 0.3509428648144214\n",
      "iterations 4007 accuracy : 0.9317370300357067  loss : 0.35092650883324983\n",
      "iterations 4008 accuracy : 0.9317370300357067  loss : 0.3509097782834756\n",
      "iterations 4009 accuracy : 0.9317370300357067  loss : 0.350892553880948\n",
      "iterations 4010 accuracy : 0.9317370300357067  loss : 0.35087677748288765\n",
      "iterations 4011 accuracy : 0.9317370300357067  loss : 0.3508605529106057\n",
      "iterations 4012 accuracy : 0.9317370300357067  loss : 0.35084434583295426\n",
      "iterations 4013 accuracy : 0.9317370300357067  loss : 0.35082765617174605\n",
      "iterations 4014 accuracy : 0.9317370300357067  loss : 0.35081228021932126\n",
      "iterations 4015 accuracy : 0.9317370300357067  loss : 0.3507965296171134\n",
      "iterations 4016 accuracy : 0.9317370300357067  loss : 0.35078087977511485\n",
      "iterations 4017 accuracy : 0.9317370300357067  loss : 0.35076433550824654\n",
      "iterations 4018 accuracy : 0.9317370300357067  loss : 0.35074783519170155\n",
      "iterations 4019 accuracy : 0.9317370300357067  loss : 0.35073207611190915\n",
      "iterations 4020 accuracy : 0.9317370300357067  loss : 0.3507156665353773\n",
      "iterations 4021 accuracy : 0.9317370300357067  loss : 0.3506995921429188\n",
      "iterations 4022 accuracy : 0.9317370300357067  loss : 0.350684139938417\n",
      "iterations 4023 accuracy : 0.9317370300357067  loss : 0.3506674501001081\n",
      "iterations 4024 accuracy : 0.9317370300357067  loss : 0.35065055889574503\n",
      "iterations 4025 accuracy : 0.9317370300357067  loss : 0.35063437372608086\n",
      "iterations 4026 accuracy : 0.9317370300357067  loss : 0.350618486550779\n",
      "iterations 4027 accuracy : 0.9317370300357067  loss : 0.3506022181471782\n",
      "iterations 4028 accuracy : 0.9317370300357067  loss : 0.3505862601652575\n",
      "iterations 4029 accuracy : 0.9317370300357067  loss : 0.3505698545191799\n",
      "iterations 4030 accuracy : 0.9317370300357067  loss : 0.35055424147611813\n",
      "iterations 4031 accuracy : 0.9317370300357067  loss : 0.3505387247765036\n",
      "iterations 4032 accuracy : 0.9317370300357067  loss : 0.3505224593023053\n",
      "iterations 4033 accuracy : 0.9317370300357067  loss : 0.350506431082429\n",
      "iterations 4034 accuracy : 0.9317370300357067  loss : 0.3504896999620212\n",
      "iterations 4035 accuracy : 0.9317370300357067  loss : 0.3504732131795452\n",
      "iterations 4036 accuracy : 0.9317370300357067  loss : 0.35045736007277634\n",
      "iterations 4037 accuracy : 0.9317370300357067  loss : 0.35044167016581396\n",
      "iterations 4038 accuracy : 0.9317370300357067  loss : 0.3504258814395166\n",
      "iterations 4039 accuracy : 0.9317370300357067  loss : 0.3504092619447033\n",
      "iterations 4040 accuracy : 0.9317370300357067  loss : 0.350392527495688\n",
      "iterations 4041 accuracy : 0.9317370300357067  loss : 0.3503768938533466\n",
      "iterations 4042 accuracy : 0.9317370300357067  loss : 0.35036090166969636\n",
      "iterations 4043 accuracy : 0.9317370300357067  loss : 0.350344741999012\n",
      "iterations 4044 accuracy : 0.9317370300357067  loss : 0.3503291343199103\n",
      "iterations 4045 accuracy : 0.9317370300357067  loss : 0.35031289784563585\n",
      "iterations 4046 accuracy : 0.9317370300357067  loss : 0.35029760904421936\n",
      "iterations 4047 accuracy : 0.9317370300357067  loss : 0.3502816811411841\n",
      "iterations 4048 accuracy : 0.9317370300357067  loss : 0.35026571417174424\n",
      "iterations 4049 accuracy : 0.9317370300357067  loss : 0.3502501958583171\n",
      "iterations 4050 accuracy : 0.9317370300357067  loss : 0.3502343614351795\n",
      "iterations 4051 accuracy : 0.9317370300357067  loss : 0.350218243477783\n",
      "iterations 4052 accuracy : 0.9317370300357067  loss : 0.3502024818017818\n",
      "iterations 4053 accuracy : 0.9317370300357067  loss : 0.3501866675619067\n",
      "iterations 4054 accuracy : 0.9317370300357067  loss : 0.35016962691514664\n",
      "iterations 4055 accuracy : 0.9317370300357067  loss : 0.35015392361267744\n",
      "iterations 4056 accuracy : 0.9317370300357067  loss : 0.3501382803588963\n",
      "iterations 4057 accuracy : 0.9317370300357067  loss : 0.3501215763168902\n",
      "iterations 4058 accuracy : 0.9317370300357067  loss : 0.35010579554178123\n",
      "iterations 4059 accuracy : 0.9317370300357067  loss : 0.35008950545643086\n",
      "iterations 4060 accuracy : 0.9317370300357067  loss : 0.3500732950398794\n",
      "iterations 4061 accuracy : 0.9317370300357067  loss : 0.3500567640546393\n",
      "iterations 4062 accuracy : 0.9317370300357067  loss : 0.35004114232562256\n",
      "iterations 4063 accuracy : 0.9317370300357067  loss : 0.35002571445541814\n",
      "iterations 4064 accuracy : 0.9317370300357067  loss : 0.35000982013150095\n",
      "iterations 4065 accuracy : 0.9317370300357067  loss : 0.34999376342490235\n",
      "iterations 4066 accuracy : 0.9317370300357067  loss : 0.34997840341005576\n",
      "iterations 4067 accuracy : 0.9317370300357067  loss : 0.3499614345067764\n",
      "iterations 4068 accuracy : 0.9317370300357067  loss : 0.34994560885176706\n",
      "iterations 4069 accuracy : 0.9317370300357067  loss : 0.3499290037269196\n",
      "iterations 4070 accuracy : 0.9317370300357067  loss : 0.3499128451927192\n",
      "iterations 4071 accuracy : 0.9317370300357067  loss : 0.3498969865178246\n",
      "iterations 4072 accuracy : 0.9317370300357067  loss : 0.34988097113688316\n",
      "iterations 4073 accuracy : 0.9317370300357067  loss : 0.3498649602281794\n",
      "iterations 4074 accuracy : 0.9317370300357067  loss : 0.34984856240020906\n",
      "iterations 4075 accuracy : 0.9317370300357067  loss : 0.3498319583452012\n",
      "iterations 4076 accuracy : 0.9317370300357067  loss : 0.3498161186158843\n",
      "iterations 4077 accuracy : 0.9317370300357067  loss : 0.34980062115851485\n",
      "iterations 4078 accuracy : 0.9317370300357067  loss : 0.3497850229778407\n",
      "iterations 4079 accuracy : 0.9317370300357067  loss : 0.34976986009926647\n",
      "iterations 4080 accuracy : 0.9317370300357067  loss : 0.34975372534148347\n",
      "iterations 4081 accuracy : 0.9317370300357067  loss : 0.34973772306217643\n",
      "iterations 4082 accuracy : 0.9317370300357067  loss : 0.34972144896836205\n",
      "iterations 4083 accuracy : 0.9317370300357067  loss : 0.34970546801673863\n",
      "iterations 4084 accuracy : 0.9315269901281243  loss : 0.34968921787324\n",
      "iterations 4085 accuracy : 0.9315269901281243  loss : 0.3496724593386487\n",
      "iterations 4086 accuracy : 0.9315269901281243  loss : 0.34965681786562497\n",
      "iterations 4087 accuracy : 0.9317370300357067  loss : 0.3496406302806423\n",
      "iterations 4088 accuracy : 0.9317370300357067  loss : 0.3496252591662295\n",
      "iterations 4089 accuracy : 0.9317370300357067  loss : 0.34960905783696705\n",
      "iterations 4090 accuracy : 0.9315269901281243  loss : 0.34959358497860205\n",
      "iterations 4091 accuracy : 0.9317370300357067  loss : 0.3495772802908267\n",
      "iterations 4092 accuracy : 0.9317370300357067  loss : 0.3495612869614443\n",
      "iterations 4093 accuracy : 0.9317370300357067  loss : 0.3495447371013124\n",
      "iterations 4094 accuracy : 0.9317370300357067  loss : 0.34952945475482194\n",
      "iterations 4095 accuracy : 0.9317370300357067  loss : 0.3495139932465537\n",
      "iterations 4096 accuracy : 0.9317370300357067  loss : 0.34949798827519163\n",
      "iterations 4097 accuracy : 0.9317370300357067  loss : 0.3494817757926416\n",
      "iterations 4098 accuracy : 0.9317370300357067  loss : 0.3494653171760338\n",
      "iterations 4099 accuracy : 0.9317370300357067  loss : 0.34944889125759543\n",
      "iterations 4100 accuracy : 0.9317370300357067  loss : 0.34943313210688903\n",
      "iterations 4101 accuracy : 0.9317370300357067  loss : 0.3494169699051591\n",
      "iterations 4102 accuracy : 0.9317370300357067  loss : 0.3494007956592814\n",
      "iterations 4103 accuracy : 0.9317370300357067  loss : 0.349384863095682\n",
      "iterations 4104 accuracy : 0.9317370300357067  loss : 0.3493691469208529\n",
      "iterations 4105 accuracy : 0.9317370300357067  loss : 0.34935355164045134\n",
      "iterations 4106 accuracy : 0.9317370300357067  loss : 0.34933756703332985\n",
      "iterations 4107 accuracy : 0.9317370300357067  loss : 0.3493219926521484\n",
      "iterations 4108 accuracy : 0.9317370300357067  loss : 0.3493061858194428\n",
      "iterations 4109 accuracy : 0.9317370300357067  loss : 0.3492908622183823\n",
      "iterations 4110 accuracy : 0.9317370300357067  loss : 0.3492743550529505\n",
      "iterations 4111 accuracy : 0.9317370300357067  loss : 0.3492587606279902\n",
      "iterations 4112 accuracy : 0.9317370300357067  loss : 0.34924339286038997\n",
      "iterations 4113 accuracy : 0.9317370300357067  loss : 0.3492272911402961\n",
      "iterations 4114 accuracy : 0.9317370300357067  loss : 0.3492113796440045\n",
      "iterations 4115 accuracy : 0.9317370300357067  loss : 0.3491960390919601\n",
      "iterations 4116 accuracy : 0.9317370300357067  loss : 0.3491804526699759\n",
      "iterations 4117 accuracy : 0.9317370300357067  loss : 0.3491650828951238\n",
      "iterations 4118 accuracy : 0.9317370300357067  loss : 0.3491493339879834\n",
      "iterations 4119 accuracy : 0.9317370300357067  loss : 0.3491337831812232\n",
      "iterations 4120 accuracy : 0.9317370300357067  loss : 0.34911785217845637\n",
      "iterations 4121 accuracy : 0.9317370300357067  loss : 0.34910168220641397\n",
      "iterations 4122 accuracy : 0.9317370300357067  loss : 0.3490859929846772\n",
      "iterations 4123 accuracy : 0.9317370300357067  loss : 0.3490711164639399\n",
      "iterations 4124 accuracy : 0.9317370300357067  loss : 0.3490551476768776\n",
      "iterations 4125 accuracy : 0.9317370300357067  loss : 0.3490391032800838\n",
      "iterations 4126 accuracy : 0.9317370300357067  loss : 0.3490236944163773\n",
      "iterations 4127 accuracy : 0.9317370300357067  loss : 0.3490084485072811\n",
      "iterations 4128 accuracy : 0.9317370300357067  loss : 0.34899300195292215\n",
      "iterations 4129 accuracy : 0.9317370300357067  loss : 0.34897702396620117\n",
      "iterations 4130 accuracy : 0.9317370300357067  loss : 0.34896197975311405\n",
      "iterations 4131 accuracy : 0.9317370300357067  loss : 0.34894663478531646\n",
      "iterations 4132 accuracy : 0.9317370300357067  loss : 0.34893095646198335\n",
      "iterations 4133 accuracy : 0.9317370300357067  loss : 0.3489147477994689\n",
      "iterations 4134 accuracy : 0.9317370300357067  loss : 0.348899587798392\n",
      "iterations 4135 accuracy : 0.9317370300357067  loss : 0.3488830531024796\n",
      "iterations 4136 accuracy : 0.9315269901281243  loss : 0.34886745119550994\n",
      "iterations 4137 accuracy : 0.9317370300357067  loss : 0.34885198249730304\n",
      "iterations 4138 accuracy : 0.9317370300357067  loss : 0.3488362750686444\n",
      "iterations 4139 accuracy : 0.9315269901281243  loss : 0.34882058062999305\n",
      "iterations 4140 accuracy : 0.9317370300357067  loss : 0.3488052786239001\n",
      "iterations 4141 accuracy : 0.9315269901281243  loss : 0.3487891839798809\n",
      "iterations 4142 accuracy : 0.9315269901281243  loss : 0.3487735617743641\n",
      "iterations 4143 accuracy : 0.9315269901281243  loss : 0.3487581437952337\n",
      "iterations 4144 accuracy : 0.9315269901281243  loss : 0.34874264224511914\n",
      "iterations 4145 accuracy : 0.9315269901281243  loss : 0.34872714746986605\n",
      "iterations 4146 accuracy : 0.9315269901281243  loss : 0.34871107548929364\n",
      "iterations 4147 accuracy : 0.9315269901281243  loss : 0.34869563233046785\n",
      "iterations 4148 accuracy : 0.9315269901281243  loss : 0.3486801646869564\n",
      "iterations 4149 accuracy : 0.9315269901281243  loss : 0.34866501968690305\n",
      "iterations 4150 accuracy : 0.9315269901281243  loss : 0.3486488978928395\n",
      "iterations 4151 accuracy : 0.9315269901281243  loss : 0.34863298662942904\n",
      "iterations 4152 accuracy : 0.9315269901281243  loss : 0.34861744220896934\n",
      "iterations 4153 accuracy : 0.9315269901281243  loss : 0.34860174945252775\n",
      "iterations 4154 accuracy : 0.9315269901281243  loss : 0.34858668021117106\n",
      "iterations 4155 accuracy : 0.9315269901281243  loss : 0.34857112603918405\n",
      "iterations 4156 accuracy : 0.9315269901281243  loss : 0.34855535011807015\n",
      "iterations 4157 accuracy : 0.9315269901281243  loss : 0.348539735963279\n",
      "iterations 4158 accuracy : 0.9315269901281243  loss : 0.3485235802643422\n",
      "iterations 4159 accuracy : 0.9315269901281243  loss : 0.3485077476949981\n",
      "iterations 4160 accuracy : 0.9315269901281243  loss : 0.34849240672880144\n",
      "iterations 4161 accuracy : 0.9315269901281243  loss : 0.3484773777983649\n",
      "iterations 4162 accuracy : 0.9315269901281243  loss : 0.34846210704863556\n",
      "iterations 4163 accuracy : 0.9315269901281243  loss : 0.34844664465570274\n",
      "iterations 4164 accuracy : 0.9315269901281243  loss : 0.3484311963450368\n",
      "iterations 4165 accuracy : 0.9315269901281243  loss : 0.34841615398238823\n",
      "iterations 4166 accuracy : 0.9315269901281243  loss : 0.3484003796534954\n",
      "iterations 4167 accuracy : 0.9315269901281243  loss : 0.34838466654371564\n",
      "iterations 4168 accuracy : 0.9315269901281243  loss : 0.3483692381419706\n",
      "iterations 4169 accuracy : 0.9315269901281243  loss : 0.34835343878202385\n",
      "iterations 4170 accuracy : 0.9315269901281243  loss : 0.3483378743972906\n",
      "iterations 4171 accuracy : 0.9315269901281243  loss : 0.34832228629828604\n",
      "iterations 4172 accuracy : 0.9315269901281243  loss : 0.34830689553171607\n",
      "iterations 4173 accuracy : 0.9315269901281243  loss : 0.34829156256319044\n",
      "iterations 4174 accuracy : 0.9315269901281243  loss : 0.3482755346691489\n",
      "iterations 4175 accuracy : 0.9315269901281243  loss : 0.3482599944453656\n",
      "iterations 4176 accuracy : 0.9315269901281243  loss : 0.34824434574550256\n",
      "iterations 4177 accuracy : 0.9315269901281243  loss : 0.348229355444536\n",
      "iterations 4178 accuracy : 0.9315269901281243  loss : 0.3482138909337016\n",
      "iterations 4179 accuracy : 0.9315269901281243  loss : 0.34819937662159534\n",
      "iterations 4180 accuracy : 0.9315269901281243  loss : 0.3481845897009474\n",
      "iterations 4181 accuracy : 0.9315269901281243  loss : 0.34816935532726434\n",
      "iterations 4182 accuracy : 0.9315269901281243  loss : 0.34815406712217306\n",
      "iterations 4183 accuracy : 0.9315269901281243  loss : 0.3481393876929254\n",
      "iterations 4184 accuracy : 0.9315269901281243  loss : 0.34812416094855536\n",
      "iterations 4185 accuracy : 0.9315269901281243  loss : 0.3481087932896954\n",
      "iterations 4186 accuracy : 0.9315269901281243  loss : 0.34809332379461233\n",
      "iterations 4187 accuracy : 0.9315269901281243  loss : 0.34807843350644885\n",
      "iterations 4188 accuracy : 0.9315269901281243  loss : 0.34806344559894176\n",
      "iterations 4189 accuracy : 0.9315269901281243  loss : 0.34804779045026774\n",
      "iterations 4190 accuracy : 0.9315269901281243  loss : 0.34803208476949665\n",
      "iterations 4191 accuracy : 0.9315269901281243  loss : 0.348016333301825\n",
      "iterations 4192 accuracy : 0.9315269901281243  loss : 0.3480020430551328\n",
      "iterations 4193 accuracy : 0.9315269901281243  loss : 0.347987313854437\n",
      "iterations 4194 accuracy : 0.9315269901281243  loss : 0.34797169097993663\n",
      "iterations 4195 accuracy : 0.9315269901281243  loss : 0.3479564082065408\n",
      "iterations 4196 accuracy : 0.9315269901281243  loss : 0.34794121311079845\n",
      "iterations 4197 accuracy : 0.9315269901281243  loss : 0.3479258252989054\n",
      "iterations 4198 accuracy : 0.9315269901281243  loss : 0.3479105139346382\n",
      "iterations 4199 accuracy : 0.9315269901281243  loss : 0.3478954533218302\n",
      "iterations 4200 accuracy : 0.9315269901281243  loss : 0.347880128750269\n",
      "iterations 4201 accuracy : 0.9315269901281243  loss : 0.3478642063768825\n",
      "iterations 4202 accuracy : 0.9315269901281243  loss : 0.3478493203478875\n",
      "iterations 4203 accuracy : 0.9315269901281243  loss : 0.34783452675531573\n",
      "iterations 4204 accuracy : 0.9315269901281243  loss : 0.3478191564315975\n",
      "iterations 4205 accuracy : 0.9315269901281243  loss : 0.34780368182393034\n",
      "iterations 4206 accuracy : 0.931316950220542  loss : 0.34778823412371407\n",
      "iterations 4207 accuracy : 0.931316950220542  loss : 0.34777311964546087\n",
      "iterations 4208 accuracy : 0.9315269901281243  loss : 0.3477575196655326\n",
      "iterations 4209 accuracy : 0.9315269901281243  loss : 0.3477426465107179\n",
      "iterations 4210 accuracy : 0.9315269901281243  loss : 0.3477275859824292\n",
      "iterations 4211 accuracy : 0.931316950220542  loss : 0.3477129481080451\n",
      "iterations 4212 accuracy : 0.9315269901281243  loss : 0.34769893229628485\n",
      "iterations 4213 accuracy : 0.9315269901281243  loss : 0.3476843629344524\n",
      "iterations 4214 accuracy : 0.9315269901281243  loss : 0.34766914395091864\n",
      "iterations 4215 accuracy : 0.931316950220542  loss : 0.3476539149907596\n",
      "iterations 4216 accuracy : 0.931316950220542  loss : 0.34763872567992116\n",
      "iterations 4217 accuracy : 0.931316950220542  loss : 0.34762399274114403\n",
      "iterations 4218 accuracy : 0.931316950220542  loss : 0.3476086527571264\n",
      "iterations 4219 accuracy : 0.931316950220542  loss : 0.3475935603824795\n",
      "iterations 4220 accuracy : 0.9315269901281243  loss : 0.347578475500058\n",
      "iterations 4221 accuracy : 0.931316950220542  loss : 0.3475631485844309\n",
      "iterations 4222 accuracy : 0.9315269901281243  loss : 0.3475472490913308\n",
      "iterations 4223 accuracy : 0.9315269901281243  loss : 0.34753253984051585\n",
      "iterations 4224 accuracy : 0.931316950220542  loss : 0.34751751319029367\n",
      "iterations 4225 accuracy : 0.9315269901281243  loss : 0.3475024041235717\n",
      "iterations 4226 accuracy : 0.931316950220542  loss : 0.3474872684505229\n",
      "iterations 4227 accuracy : 0.9315269901281243  loss : 0.3474724159352221\n",
      "iterations 4228 accuracy : 0.9315269901281243  loss : 0.3474578160612791\n",
      "iterations 4229 accuracy : 0.931316950220542  loss : 0.3474427166224814\n",
      "iterations 4230 accuracy : 0.931316950220542  loss : 0.34742763793996817\n",
      "iterations 4231 accuracy : 0.9315269901281243  loss : 0.34741242945973994\n",
      "iterations 4232 accuracy : 0.9315269901281243  loss : 0.3473975188230636\n",
      "iterations 4233 accuracy : 0.9315269901281243  loss : 0.3473824299448629\n",
      "iterations 4234 accuracy : 0.9315269901281243  loss : 0.3473676403126262\n",
      "iterations 4235 accuracy : 0.931316950220542  loss : 0.34735205444394607\n",
      "iterations 4236 accuracy : 0.931316950220542  loss : 0.34733705100803064\n",
      "iterations 4237 accuracy : 0.931316950220542  loss : 0.347321885393136\n",
      "iterations 4238 accuracy : 0.931316950220542  loss : 0.3473071082168114\n",
      "iterations 4239 accuracy : 0.931316950220542  loss : 0.3472919191634437\n",
      "iterations 4240 accuracy : 0.931316950220542  loss : 0.3472770439233522\n",
      "iterations 4241 accuracy : 0.9315269901281243  loss : 0.3472619854936288\n",
      "iterations 4242 accuracy : 0.931316950220542  loss : 0.34724723342380304\n",
      "iterations 4243 accuracy : 0.931316950220542  loss : 0.34723222662160014\n",
      "iterations 4244 accuracy : 0.931316950220542  loss : 0.34721724598669396\n",
      "iterations 4245 accuracy : 0.931316950220542  loss : 0.3472021305744602\n",
      "iterations 4246 accuracy : 0.931316950220542  loss : 0.3471868528832962\n",
      "iterations 4247 accuracy : 0.931316950220542  loss : 0.3471720151265313\n",
      "iterations 4248 accuracy : 0.9315269901281243  loss : 0.3471578211239075\n",
      "iterations 4249 accuracy : 0.9315269901281243  loss : 0.3471430152876617\n",
      "iterations 4250 accuracy : 0.9315269901281243  loss : 0.34712780520223874\n",
      "iterations 4251 accuracy : 0.9315269901281243  loss : 0.3471129455520202\n",
      "iterations 4252 accuracy : 0.9315269901281243  loss : 0.34709824891703034\n",
      "iterations 4253 accuracy : 0.9315269901281243  loss : 0.347082833917635\n",
      "iterations 4254 accuracy : 0.9315269901281243  loss : 0.3470676915661968\n",
      "iterations 4255 accuracy : 0.931316950220542  loss : 0.34705221378777035\n",
      "iterations 4256 accuracy : 0.9315269901281243  loss : 0.34703692370648376\n",
      "iterations 4257 accuracy : 0.9315269901281243  loss : 0.34702233674652033\n",
      "iterations 4258 accuracy : 0.9315269901281243  loss : 0.3470062518516063\n",
      "iterations 4259 accuracy : 0.9315269901281243  loss : 0.346991028131862\n",
      "iterations 4260 accuracy : 0.9315269901281243  loss : 0.3469757182491992\n",
      "iterations 4261 accuracy : 0.9315269901281243  loss : 0.34696127838088253\n",
      "iterations 4262 accuracy : 0.9315269901281243  loss : 0.34694546315903785\n",
      "iterations 4263 accuracy : 0.9315269901281243  loss : 0.3469304010446287\n",
      "iterations 4264 accuracy : 0.9315269901281243  loss : 0.3469156220092527\n",
      "iterations 4265 accuracy : 0.9315269901281243  loss : 0.3469017336621721\n",
      "iterations 4266 accuracy : 0.9315269901281243  loss : 0.3468869963943863\n",
      "iterations 4267 accuracy : 0.9315269901281243  loss : 0.34687277008485295\n",
      "iterations 4268 accuracy : 0.9315269901281243  loss : 0.34685777960855624\n",
      "iterations 4269 accuracy : 0.9315269901281243  loss : 0.34684309028610966\n",
      "iterations 4270 accuracy : 0.9315269901281243  loss : 0.34682857321854466\n",
      "iterations 4271 accuracy : 0.9315269901281243  loss : 0.3468135409147319\n",
      "iterations 4272 accuracy : 0.9315269901281243  loss : 0.3467992293363891\n",
      "iterations 4273 accuracy : 0.9315269901281243  loss : 0.34678497504711836\n",
      "iterations 4274 accuracy : 0.9315269901281243  loss : 0.34676979609336567\n",
      "iterations 4275 accuracy : 0.9315269901281243  loss : 0.34675542742927645\n",
      "iterations 4276 accuracy : 0.9315269901281243  loss : 0.3467398420868127\n",
      "iterations 4277 accuracy : 0.9315269901281243  loss : 0.3467254664541961\n",
      "iterations 4278 accuracy : 0.9315269901281243  loss : 0.3467113481248143\n",
      "iterations 4279 accuracy : 0.9315269901281243  loss : 0.3466969094500318\n",
      "iterations 4280 accuracy : 0.9315269901281243  loss : 0.3466826005685574\n",
      "iterations 4281 accuracy : 0.9315269901281243  loss : 0.34666772644646904\n",
      "iterations 4282 accuracy : 0.9315269901281243  loss : 0.3466539257424064\n",
      "iterations 4283 accuracy : 0.9315269901281243  loss : 0.3466392700727804\n",
      "iterations 4284 accuracy : 0.9315269901281243  loss : 0.3466248773503058\n",
      "iterations 4285 accuracy : 0.9315269901281243  loss : 0.3466098904857152\n",
      "iterations 4286 accuracy : 0.9315269901281243  loss : 0.3465953873395479\n",
      "iterations 4287 accuracy : 0.9315269901281243  loss : 0.34658125852156524\n",
      "iterations 4288 accuracy : 0.9315269901281243  loss : 0.34656694093555224\n",
      "iterations 4289 accuracy : 0.9315269901281243  loss : 0.3465520690788209\n",
      "iterations 4290 accuracy : 0.931316950220542  loss : 0.3465380570450924\n",
      "iterations 4291 accuracy : 0.9315269901281243  loss : 0.3465232226611352\n",
      "iterations 4292 accuracy : 0.9315269901281243  loss : 0.3465087822580962\n",
      "iterations 4293 accuracy : 0.9315269901281243  loss : 0.3464945720018615\n",
      "iterations 4294 accuracy : 0.9315269901281243  loss : 0.34647999358602366\n",
      "iterations 4295 accuracy : 0.9315269901281243  loss : 0.34646477596809483\n",
      "iterations 4296 accuracy : 0.9315269901281243  loss : 0.3464492036104174\n",
      "iterations 4297 accuracy : 0.9315269901281243  loss : 0.3464338424904816\n",
      "iterations 4298 accuracy : 0.9315269901281243  loss : 0.34641895375022486\n",
      "iterations 4299 accuracy : 0.9315269901281243  loss : 0.34640393595135077\n",
      "iterations 4300 accuracy : 0.9315269901281243  loss : 0.34638946479431115\n",
      "iterations 4301 accuracy : 0.9315269901281243  loss : 0.3463743268617068\n",
      "iterations 4302 accuracy : 0.9315269901281243  loss : 0.3463593004461733\n",
      "iterations 4303 accuracy : 0.9315269901281243  loss : 0.34634483423638723\n",
      "iterations 4304 accuracy : 0.9315269901281243  loss : 0.34632945799854964\n",
      "iterations 4305 accuracy : 0.9315269901281243  loss : 0.34631589384346634\n",
      "iterations 4306 accuracy : 0.9315269901281243  loss : 0.3463011931332194\n",
      "iterations 4307 accuracy : 0.9315269901281243  loss : 0.346286545043181\n",
      "iterations 4308 accuracy : 0.9315269901281243  loss : 0.3462712841142959\n",
      "iterations 4309 accuracy : 0.9315269901281243  loss : 0.34625734897364685\n",
      "iterations 4310 accuracy : 0.9315269901281243  loss : 0.3462429747661882\n",
      "iterations 4311 accuracy : 0.9315269901281243  loss : 0.34622824204636654\n",
      "iterations 4312 accuracy : 0.9315269901281243  loss : 0.34621355073201937\n",
      "iterations 4313 accuracy : 0.9315269901281243  loss : 0.34619893896292286\n",
      "iterations 4314 accuracy : 0.9315269901281243  loss : 0.3461840953767687\n",
      "iterations 4315 accuracy : 0.9315269901281243  loss : 0.34616935892676626\n",
      "iterations 4316 accuracy : 0.9315269901281243  loss : 0.3461547594184691\n",
      "iterations 4317 accuracy : 0.9315269901281243  loss : 0.34614084209803464\n",
      "iterations 4318 accuracy : 0.9315269901281243  loss : 0.34612606738796914\n",
      "iterations 4319 accuracy : 0.9315269901281243  loss : 0.34611084792422\n",
      "iterations 4320 accuracy : 0.9315269901281243  loss : 0.34609632426204934\n",
      "iterations 4321 accuracy : 0.9315269901281243  loss : 0.34608064661140336\n",
      "iterations 4322 accuracy : 0.9315269901281243  loss : 0.3460652632865874\n",
      "iterations 4323 accuracy : 0.9315269901281243  loss : 0.34604996722810416\n",
      "iterations 4324 accuracy : 0.9315269901281243  loss : 0.34603549901670894\n",
      "iterations 4325 accuracy : 0.9315269901281243  loss : 0.34602131497462196\n",
      "iterations 4326 accuracy : 0.9315269901281243  loss : 0.34600679636103626\n",
      "iterations 4327 accuracy : 0.9315269901281243  loss : 0.3459925708486882\n",
      "iterations 4328 accuracy : 0.9315269901281243  loss : 0.345979199418008\n",
      "iterations 4329 accuracy : 0.9315269901281243  loss : 0.34596555408114726\n",
      "iterations 4330 accuracy : 0.9315269901281243  loss : 0.3459507749002934\n",
      "iterations 4331 accuracy : 0.9315269901281243  loss : 0.3459364589101272\n",
      "iterations 4332 accuracy : 0.9315269901281243  loss : 0.3459218111079521\n",
      "iterations 4333 accuracy : 0.9315269901281243  loss : 0.3459078875518417\n",
      "iterations 4334 accuracy : 0.9315269901281243  loss : 0.3458933599811109\n",
      "iterations 4335 accuracy : 0.9315269901281243  loss : 0.3458789225229153\n",
      "iterations 4336 accuracy : 0.9315269901281243  loss : 0.34586470054960794\n",
      "iterations 4337 accuracy : 0.9315269901281243  loss : 0.34585044466477904\n",
      "iterations 4338 accuracy : 0.9315269901281243  loss : 0.34583611335439524\n",
      "iterations 4339 accuracy : 0.9315269901281243  loss : 0.34582193747821094\n",
      "iterations 4340 accuracy : 0.9315269901281243  loss : 0.345806080573351\n",
      "iterations 4341 accuracy : 0.9315269901281243  loss : 0.34579101938950296\n",
      "iterations 4342 accuracy : 0.9315269901281243  loss : 0.3457764292220358\n",
      "iterations 4343 accuracy : 0.9315269901281243  loss : 0.34576202944820805\n",
      "iterations 4344 accuracy : 0.9315269901281243  loss : 0.34574690221175897\n",
      "iterations 4345 accuracy : 0.9315269901281243  loss : 0.34573200444078706\n",
      "iterations 4346 accuracy : 0.9315269901281243  loss : 0.345717486234954\n",
      "iterations 4347 accuracy : 0.9315269901281243  loss : 0.34570282512782763\n",
      "iterations 4348 accuracy : 0.9315269901281243  loss : 0.3456886137784079\n",
      "iterations 4349 accuracy : 0.9315269901281243  loss : 0.34567439607908146\n",
      "iterations 4350 accuracy : 0.9317370300357067  loss : 0.3456596612103653\n",
      "iterations 4351 accuracy : 0.9317370300357067  loss : 0.3456448924887405\n",
      "iterations 4352 accuracy : 0.9317370300357067  loss : 0.3456303418064011\n",
      "iterations 4353 accuracy : 0.9317370300357067  loss : 0.3456158423761374\n",
      "iterations 4354 accuracy : 0.9315269901281243  loss : 0.3456013552968229\n",
      "iterations 4355 accuracy : 0.9317370300357067  loss : 0.34558771115229125\n",
      "iterations 4356 accuracy : 0.9317370300357067  loss : 0.3455730572323313\n",
      "iterations 4357 accuracy : 0.9317370300357067  loss : 0.34555984360498954\n",
      "iterations 4358 accuracy : 0.9315269901281243  loss : 0.34554522540260396\n",
      "iterations 4359 accuracy : 0.9315269901281243  loss : 0.3455314062039712\n",
      "iterations 4360 accuracy : 0.9315269901281243  loss : 0.345516932491777\n",
      "iterations 4361 accuracy : 0.9317370300357067  loss : 0.34550181788952045\n",
      "iterations 4362 accuracy : 0.9317370300357067  loss : 0.3454874640448374\n",
      "iterations 4363 accuracy : 0.9317370300357067  loss : 0.34547290549713555\n",
      "iterations 4364 accuracy : 0.9317370300357067  loss : 0.34545860865732647\n",
      "iterations 4365 accuracy : 0.9317370300357067  loss : 0.3454440745656065\n",
      "iterations 4366 accuracy : 0.9317370300357067  loss : 0.34542946281063314\n",
      "iterations 4367 accuracy : 0.9317370300357067  loss : 0.34541519513043645\n",
      "iterations 4368 accuracy : 0.9315269901281243  loss : 0.34540178946267225\n",
      "iterations 4369 accuracy : 0.9315269901281243  loss : 0.3453877564445264\n",
      "iterations 4370 accuracy : 0.9315269901281243  loss : 0.345373410956365\n",
      "iterations 4371 accuracy : 0.9317370300357067  loss : 0.3453582102450645\n",
      "iterations 4372 accuracy : 0.9317370300357067  loss : 0.34534373622184306\n",
      "iterations 4373 accuracy : 0.9317370300357067  loss : 0.34532962295185277\n",
      "iterations 4374 accuracy : 0.9317370300357067  loss : 0.3453147224881904\n",
      "iterations 4375 accuracy : 0.9317370300357067  loss : 0.34530110340065895\n",
      "iterations 4376 accuracy : 0.9315269901281243  loss : 0.34528726113349995\n",
      "iterations 4377 accuracy : 0.9315269901281243  loss : 0.34527285237821914\n",
      "iterations 4378 accuracy : 0.9315269901281243  loss : 0.3452586506345961\n",
      "iterations 4379 accuracy : 0.9317370300357067  loss : 0.34524400046301584\n",
      "iterations 4380 accuracy : 0.9317370300357067  loss : 0.345229283770059\n",
      "iterations 4381 accuracy : 0.9315269901281243  loss : 0.3452150579498048\n",
      "iterations 4382 accuracy : 0.9315269901281243  loss : 0.3452010540837204\n",
      "iterations 4383 accuracy : 0.9315269901281243  loss : 0.34518691180437594\n",
      "iterations 4384 accuracy : 0.9315269901281243  loss : 0.3451731806276181\n",
      "iterations 4385 accuracy : 0.9315269901281243  loss : 0.34515956601469755\n",
      "iterations 4386 accuracy : 0.9315269901281243  loss : 0.34514467805835924\n",
      "iterations 4387 accuracy : 0.9317370300357067  loss : 0.34513044711667384\n",
      "iterations 4388 accuracy : 0.9317370300357067  loss : 0.34511604587507916\n",
      "iterations 4389 accuracy : 0.9317370300357067  loss : 0.34510117267880375\n",
      "iterations 4390 accuracy : 0.931316950220542  loss : 0.34508588485195935\n",
      "iterations 4391 accuracy : 0.9315269901281243  loss : 0.3450722365868044\n",
      "iterations 4392 accuracy : 0.9317370300357067  loss : 0.34505825518659144\n",
      "iterations 4393 accuracy : 0.931316950220542  loss : 0.34504388321971385\n",
      "iterations 4394 accuracy : 0.931316950220542  loss : 0.3450293566251369\n",
      "iterations 4395 accuracy : 0.931316950220542  loss : 0.34501491729535105\n",
      "iterations 4396 accuracy : 0.931316950220542  loss : 0.345000830809228\n",
      "iterations 4397 accuracy : 0.9317370300357067  loss : 0.344987872778696\n",
      "iterations 4398 accuracy : 0.9315269901281243  loss : 0.34497301180778317\n",
      "iterations 4399 accuracy : 0.9315269901281243  loss : 0.3449582500148615\n",
      "iterations 4400 accuracy : 0.9317370300357067  loss : 0.3449442236945325\n",
      "iterations 4401 accuracy : 0.9315269901281243  loss : 0.34493003272856637\n",
      "iterations 4402 accuracy : 0.9315269901281243  loss : 0.3449156409517352\n",
      "iterations 4403 accuracy : 0.9317370300357067  loss : 0.3449022169531324\n",
      "iterations 4404 accuracy : 0.9317370300357067  loss : 0.3448878810267162\n",
      "iterations 4405 accuracy : 0.9317370300357067  loss : 0.34487419203116304\n",
      "iterations 4406 accuracy : 0.9317370300357067  loss : 0.3448595833455153\n",
      "iterations 4407 accuracy : 0.9317370300357067  loss : 0.3448452363498953\n",
      "iterations 4408 accuracy : 0.9317370300357067  loss : 0.34483156817617466\n",
      "iterations 4409 accuracy : 0.9315269901281243  loss : 0.34481833177415044\n",
      "iterations 4410 accuracy : 0.9315269901281243  loss : 0.34480417056383267\n",
      "iterations 4411 accuracy : 0.9315269901281243  loss : 0.3447902957973656\n",
      "iterations 4412 accuracy : 0.9315269901281243  loss : 0.34477673674020964\n",
      "iterations 4413 accuracy : 0.9315269901281243  loss : 0.3447624674089217\n",
      "iterations 4414 accuracy : 0.9317370300357067  loss : 0.34474856520355535\n",
      "iterations 4415 accuracy : 0.9317370300357067  loss : 0.34473472485795137\n",
      "iterations 4416 accuracy : 0.9317370300357067  loss : 0.34472027663529964\n",
      "iterations 4417 accuracy : 0.9315269901281243  loss : 0.34470754334423376\n",
      "iterations 4418 accuracy : 0.9317370300357067  loss : 0.34469364712650974\n",
      "iterations 4419 accuracy : 0.9315269901281243  loss : 0.3446793696157465\n",
      "iterations 4420 accuracy : 0.9315269901281243  loss : 0.34466570377115124\n",
      "iterations 4421 accuracy : 0.9315269901281243  loss : 0.3446515955439943\n",
      "iterations 4422 accuracy : 0.9315269901281243  loss : 0.34463745457816103\n",
      "iterations 4423 accuracy : 0.931316950220542  loss : 0.3446235242779937\n",
      "iterations 4424 accuracy : 0.931316950220542  loss : 0.344609605914271\n",
      "iterations 4425 accuracy : 0.9311069103129594  loss : 0.34459473029381626\n",
      "iterations 4426 accuracy : 0.931316950220542  loss : 0.34458098806520565\n",
      "iterations 4427 accuracy : 0.931316950220542  loss : 0.34456673654263936\n",
      "iterations 4428 accuracy : 0.931316950220542  loss : 0.34455261065905596\n",
      "iterations 4429 accuracy : 0.9311069103129594  loss : 0.34453769776290827\n",
      "iterations 4430 accuracy : 0.9311069103129594  loss : 0.3445235038768598\n",
      "iterations 4431 accuracy : 0.931316950220542  loss : 0.344510158196556\n",
      "iterations 4432 accuracy : 0.9311069103129594  loss : 0.344495200489332\n",
      "iterations 4433 accuracy : 0.930896870405377  loss : 0.3444805418341744\n",
      "iterations 4434 accuracy : 0.930896870405377  loss : 0.34446623045398206\n",
      "iterations 4435 accuracy : 0.9311069103129594  loss : 0.34445265455383406\n",
      "iterations 4436 accuracy : 0.930896870405377  loss : 0.34443745150649974\n",
      "iterations 4437 accuracy : 0.9311069103129594  loss : 0.34442412329416494\n",
      "iterations 4438 accuracy : 0.930896870405377  loss : 0.3444094866329396\n",
      "iterations 4439 accuracy : 0.930896870405377  loss : 0.3443953404149493\n",
      "iterations 4440 accuracy : 0.930896870405377  loss : 0.34438162027620045\n",
      "iterations 4441 accuracy : 0.930896870405377  loss : 0.344367450664697\n",
      "iterations 4442 accuracy : 0.930896870405377  loss : 0.3443535758443931\n",
      "iterations 4443 accuracy : 0.930896870405377  loss : 0.3443401446587823\n",
      "iterations 4444 accuracy : 0.930896870405377  loss : 0.3443258036268946\n",
      "iterations 4445 accuracy : 0.930896870405377  loss : 0.3443111478158182\n",
      "iterations 4446 accuracy : 0.930896870405377  loss : 0.34429719015200727\n",
      "iterations 4447 accuracy : 0.9306868304977945  loss : 0.3442827447301886\n",
      "iterations 4448 accuracy : 0.9306868304977945  loss : 0.34426898720024796\n",
      "iterations 4449 accuracy : 0.9306868304977945  loss : 0.34425516440113507\n",
      "iterations 4450 accuracy : 0.9306868304977945  loss : 0.34424137248876585\n",
      "iterations 4451 accuracy : 0.9306868304977945  loss : 0.3442270781759954\n",
      "iterations 4452 accuracy : 0.9306868304977945  loss : 0.3442131840109976\n",
      "iterations 4453 accuracy : 0.9306868304977945  loss : 0.34419961795278803\n",
      "iterations 4454 accuracy : 0.9306868304977945  loss : 0.34418583112370066\n",
      "iterations 4455 accuracy : 0.9306868304977945  loss : 0.3441716918099526\n",
      "iterations 4456 accuracy : 0.9306868304977945  loss : 0.3441571535145859\n",
      "iterations 4457 accuracy : 0.9306868304977945  loss : 0.3441432566217849\n",
      "iterations 4458 accuracy : 0.9304767905902122  loss : 0.34412906431664825\n",
      "iterations 4459 accuracy : 0.9304767905902122  loss : 0.3441154754067647\n",
      "iterations 4460 accuracy : 0.9304767905902122  loss : 0.3441014259623173\n",
      "iterations 4461 accuracy : 0.9304767905902122  loss : 0.3440874003502461\n",
      "iterations 4462 accuracy : 0.9304767905902122  loss : 0.34407344052241545\n",
      "iterations 4463 accuracy : 0.9304767905902122  loss : 0.3440595975175841\n",
      "iterations 4464 accuracy : 0.9304767905902122  loss : 0.3440451081055773\n",
      "iterations 4465 accuracy : 0.9304767905902122  loss : 0.34403049255881785\n",
      "iterations 4466 accuracy : 0.9304767905902122  loss : 0.3440164238429624\n",
      "iterations 4467 accuracy : 0.9304767905902122  loss : 0.3440029492589754\n",
      "iterations 4468 accuracy : 0.9304767905902122  loss : 0.3439892541624121\n",
      "iterations 4469 accuracy : 0.9304767905902122  loss : 0.3439756317632664\n",
      "iterations 4470 accuracy : 0.9304767905902122  loss : 0.34396193842866696\n",
      "iterations 4471 accuracy : 0.9304767905902122  loss : 0.3439482742838963\n",
      "iterations 4472 accuracy : 0.9304767905902122  loss : 0.3439353702007817\n",
      "iterations 4473 accuracy : 0.9304767905902122  loss : 0.3439219852755308\n",
      "iterations 4474 accuracy : 0.9304767905902122  loss : 0.3439083951401072\n",
      "iterations 4475 accuracy : 0.9304767905902122  loss : 0.3438950532844609\n",
      "iterations 4476 accuracy : 0.9304767905902122  loss : 0.34388118332145834\n",
      "iterations 4477 accuracy : 0.9304767905902122  loss : 0.34386771607229516\n",
      "iterations 4478 accuracy : 0.9304767905902122  loss : 0.34385332067759133\n",
      "iterations 4479 accuracy : 0.9304767905902122  loss : 0.3438392375784215\n",
      "iterations 4480 accuracy : 0.9304767905902122  loss : 0.3438259582550379\n",
      "iterations 4481 accuracy : 0.9304767905902122  loss : 0.3438118600892956\n",
      "iterations 4482 accuracy : 0.9304767905902122  loss : 0.3437975767762889\n",
      "iterations 4483 accuracy : 0.9304767905902122  loss : 0.34378392993091883\n",
      "iterations 4484 accuracy : 0.9304767905902122  loss : 0.3437708392868578\n",
      "iterations 4485 accuracy : 0.9304767905902122  loss : 0.3437563715019068\n",
      "iterations 4486 accuracy : 0.9304767905902122  loss : 0.3437424235011289\n",
      "iterations 4487 accuracy : 0.9304767905902122  loss : 0.3437281997511305\n",
      "iterations 4488 accuracy : 0.9304767905902122  loss : 0.3437142776080306\n",
      "iterations 4489 accuracy : 0.9304767905902122  loss : 0.3437010636014564\n",
      "iterations 4490 accuracy : 0.9304767905902122  loss : 0.3436875116292696\n",
      "iterations 4491 accuracy : 0.9304767905902122  loss : 0.343673399747905\n",
      "iterations 4492 accuracy : 0.9304767905902122  loss : 0.343659374757035\n",
      "iterations 4493 accuracy : 0.9304767905902122  loss : 0.3436458732110373\n",
      "iterations 4494 accuracy : 0.9306868304977945  loss : 0.34363230102097936\n",
      "iterations 4495 accuracy : 0.9306868304977945  loss : 0.3436189525622539\n",
      "iterations 4496 accuracy : 0.9304767905902122  loss : 0.343605938414009\n",
      "iterations 4497 accuracy : 0.9306868304977945  loss : 0.34359201286993507\n",
      "iterations 4498 accuracy : 0.9306868304977945  loss : 0.34357761009991034\n",
      "iterations 4499 accuracy : 0.9306868304977945  loss : 0.3435637056496561\n",
      "iterations 4500 accuracy : 0.930896870405377  loss : 0.34354950333611134\n",
      "iterations 4501 accuracy : 0.930896870405377  loss : 0.3435361557759859\n",
      "iterations 4502 accuracy : 0.930896870405377  loss : 0.3435224328328476\n",
      "iterations 4503 accuracy : 0.930896870405377  loss : 0.3435087094450504\n",
      "iterations 4504 accuracy : 0.930896870405377  loss : 0.34349447679990097\n",
      "iterations 4505 accuracy : 0.930896870405377  loss : 0.34348032687215296\n",
      "iterations 4506 accuracy : 0.930896870405377  loss : 0.3434665294700835\n",
      "iterations 4507 accuracy : 0.930896870405377  loss : 0.34345229079010353\n",
      "iterations 4508 accuracy : 0.930896870405377  loss : 0.34343802773059706\n",
      "iterations 4509 accuracy : 0.930896870405377  loss : 0.34342525068400265\n",
      "iterations 4510 accuracy : 0.930896870405377  loss : 0.3434113458977941\n",
      "iterations 4511 accuracy : 0.930896870405377  loss : 0.3433970597146239\n",
      "iterations 4512 accuracy : 0.930896870405377  loss : 0.3433834610490392\n",
      "iterations 4513 accuracy : 0.930896870405377  loss : 0.3433694594039992\n",
      "iterations 4514 accuracy : 0.930896870405377  loss : 0.3433559200400908\n",
      "iterations 4515 accuracy : 0.930896870405377  loss : 0.3433427173742656\n",
      "iterations 4516 accuracy : 0.930896870405377  loss : 0.34332927372354827\n",
      "iterations 4517 accuracy : 0.930896870405377  loss : 0.3433151380473619\n",
      "iterations 4518 accuracy : 0.930896870405377  loss : 0.3433020363348388\n",
      "iterations 4519 accuracy : 0.930896870405377  loss : 0.34328883757424644\n",
      "iterations 4520 accuracy : 0.930896870405377  loss : 0.34327556561745337\n",
      "iterations 4521 accuracy : 0.930896870405377  loss : 0.34326152998673315\n",
      "iterations 4522 accuracy : 0.930896870405377  loss : 0.3432485256496417\n",
      "iterations 4523 accuracy : 0.930896870405377  loss : 0.3432353690155613\n",
      "iterations 4524 accuracy : 0.930896870405377  loss : 0.34322135084642824\n",
      "iterations 4525 accuracy : 0.930896870405377  loss : 0.34320770895312064\n",
      "iterations 4526 accuracy : 0.930896870405377  loss : 0.34319417299561844\n",
      "iterations 4527 accuracy : 0.930896870405377  loss : 0.34318034819096355\n",
      "iterations 4528 accuracy : 0.930896870405377  loss : 0.3431676988060328\n",
      "iterations 4529 accuracy : 0.930896870405377  loss : 0.3431541838031013\n",
      "iterations 4530 accuracy : 0.930896870405377  loss : 0.34314084991493343\n",
      "iterations 4531 accuracy : 0.930896870405377  loss : 0.34312809012460166\n",
      "iterations 4532 accuracy : 0.930896870405377  loss : 0.34311324421446643\n",
      "iterations 4533 accuracy : 0.930896870405377  loss : 0.34309960788462385\n",
      "iterations 4534 accuracy : 0.930896870405377  loss : 0.34308628425349735\n",
      "iterations 4535 accuracy : 0.930896870405377  loss : 0.343072870143986\n",
      "iterations 4536 accuracy : 0.930896870405377  loss : 0.3430592951112165\n",
      "iterations 4537 accuracy : 0.930896870405377  loss : 0.3430445055040908\n",
      "iterations 4538 accuracy : 0.930896870405377  loss : 0.34303119321178616\n",
      "iterations 4539 accuracy : 0.930896870405377  loss : 0.3430175705894799\n",
      "iterations 4540 accuracy : 0.930896870405377  loss : 0.3430035937798809\n",
      "iterations 4541 accuracy : 0.930896870405377  loss : 0.3429906296191345\n",
      "iterations 4542 accuracy : 0.930896870405377  loss : 0.34297722320230956\n",
      "iterations 4543 accuracy : 0.930896870405377  loss : 0.3429630777268512\n",
      "iterations 4544 accuracy : 0.930896870405377  loss : 0.342948946248894\n",
      "iterations 4545 accuracy : 0.930896870405377  loss : 0.3429354176331587\n",
      "iterations 4546 accuracy : 0.930896870405377  loss : 0.3429221988543087\n",
      "iterations 4547 accuracy : 0.930896870405377  loss : 0.34290840374286663\n",
      "iterations 4548 accuracy : 0.930896870405377  loss : 0.34289527701294453\n",
      "iterations 4549 accuracy : 0.930896870405377  loss : 0.3428809859621796\n",
      "iterations 4550 accuracy : 0.930896870405377  loss : 0.3428672713940627\n",
      "iterations 4551 accuracy : 0.930896870405377  loss : 0.3428532170649374\n",
      "iterations 4552 accuracy : 0.930896870405377  loss : 0.3428406314505903\n",
      "iterations 4553 accuracy : 0.930896870405377  loss : 0.3428274468622528\n",
      "iterations 4554 accuracy : 0.930896870405377  loss : 0.3428136025977368\n",
      "iterations 4555 accuracy : 0.930896870405377  loss : 0.3427998388646163\n",
      "iterations 4556 accuracy : 0.930896870405377  loss : 0.34278566972308666\n",
      "iterations 4557 accuracy : 0.930896870405377  loss : 0.34277240106285045\n",
      "iterations 4558 accuracy : 0.930896870405377  loss : 0.34275943636952555\n",
      "iterations 4559 accuracy : 0.930896870405377  loss : 0.3427454530158095\n",
      "iterations 4560 accuracy : 0.930896870405377  loss : 0.34273219181611303\n",
      "iterations 4561 accuracy : 0.930896870405377  loss : 0.34271847262197824\n",
      "iterations 4562 accuracy : 0.930896870405377  loss : 0.34270574904921014\n",
      "iterations 4563 accuracy : 0.930896870405377  loss : 0.3426924540958144\n",
      "iterations 4564 accuracy : 0.930896870405377  loss : 0.3426781282587508\n",
      "iterations 4565 accuracy : 0.930896870405377  loss : 0.3426648210097739\n",
      "iterations 4566 accuracy : 0.930896870405377  loss : 0.3426518114256375\n",
      "iterations 4567 accuracy : 0.930896870405377  loss : 0.3426376559860358\n",
      "iterations 4568 accuracy : 0.930896870405377  loss : 0.3426234689933289\n",
      "iterations 4569 accuracy : 0.930896870405377  loss : 0.3426095212883446\n",
      "iterations 4570 accuracy : 0.930896870405377  loss : 0.34259682688749604\n",
      "iterations 4571 accuracy : 0.930896870405377  loss : 0.34258337046344844\n",
      "iterations 4572 accuracy : 0.930896870405377  loss : 0.3425695692096889\n",
      "iterations 4573 accuracy : 0.930896870405377  loss : 0.342556300145644\n",
      "iterations 4574 accuracy : 0.930896870405377  loss : 0.34254192314759835\n",
      "iterations 4575 accuracy : 0.930896870405377  loss : 0.34252877605938037\n",
      "iterations 4576 accuracy : 0.930896870405377  loss : 0.3425155877886048\n",
      "iterations 4577 accuracy : 0.930896870405377  loss : 0.342501993029072\n",
      "iterations 4578 accuracy : 0.930896870405377  loss : 0.34248860071778514\n",
      "iterations 4579 accuracy : 0.930896870405377  loss : 0.34247503080341607\n",
      "iterations 4580 accuracy : 0.930896870405377  loss : 0.34246158044923247\n",
      "iterations 4581 accuracy : 0.930896870405377  loss : 0.34244812268137037\n",
      "iterations 4582 accuracy : 0.930896870405377  loss : 0.3424353825892995\n",
      "iterations 4583 accuracy : 0.930896870405377  loss : 0.3424216724120058\n",
      "iterations 4584 accuracy : 0.930896870405377  loss : 0.34240801903853696\n",
      "iterations 4585 accuracy : 0.930896870405377  loss : 0.3423938829202987\n",
      "iterations 4586 accuracy : 0.930896870405377  loss : 0.34238072227533994\n",
      "iterations 4587 accuracy : 0.930896870405377  loss : 0.34236792706732116\n",
      "iterations 4588 accuracy : 0.930896870405377  loss : 0.3423553134787318\n",
      "iterations 4589 accuracy : 0.930896870405377  loss : 0.3423426830369947\n",
      "iterations 4590 accuracy : 0.930896870405377  loss : 0.34232888583746757\n",
      "iterations 4591 accuracy : 0.930896870405377  loss : 0.34231608468082364\n",
      "iterations 4592 accuracy : 0.930896870405377  loss : 0.34230220797443933\n",
      "iterations 4593 accuracy : 0.930896870405377  loss : 0.34228915280793737\n",
      "iterations 4594 accuracy : 0.930896870405377  loss : 0.3422767692228929\n",
      "iterations 4595 accuracy : 0.930896870405377  loss : 0.3422642582013817\n",
      "iterations 4596 accuracy : 0.930896870405377  loss : 0.3422506890637472\n",
      "iterations 4597 accuracy : 0.930896870405377  loss : 0.3422372841826438\n",
      "iterations 4598 accuracy : 0.930896870405377  loss : 0.34222389336420045\n",
      "iterations 4599 accuracy : 0.930896870405377  loss : 0.34221092839112194\n",
      "iterations 4600 accuracy : 0.930896870405377  loss : 0.34219740726407816\n",
      "iterations 4601 accuracy : 0.930896870405377  loss : 0.34218450309508985\n",
      "iterations 4602 accuracy : 0.930896870405377  loss : 0.3421714342648979\n",
      "iterations 4603 accuracy : 0.930896870405377  loss : 0.3421586095996981\n",
      "iterations 4604 accuracy : 0.930896870405377  loss : 0.34214520692490824\n",
      "iterations 4605 accuracy : 0.930896870405377  loss : 0.342131572320077\n",
      "iterations 4606 accuracy : 0.930896870405377  loss : 0.34211790037859235\n",
      "iterations 4607 accuracy : 0.930896870405377  loss : 0.3421052566106244\n",
      "iterations 4608 accuracy : 0.930896870405377  loss : 0.342092139746469\n",
      "iterations 4609 accuracy : 0.930896870405377  loss : 0.3420792581418935\n",
      "iterations 4610 accuracy : 0.930896870405377  loss : 0.34206574821989144\n",
      "iterations 4611 accuracy : 0.930896870405377  loss : 0.34205157385422424\n",
      "iterations 4612 accuracy : 0.930896870405377  loss : 0.34203783070534177\n",
      "iterations 4613 accuracy : 0.930896870405377  loss : 0.3420250119527807\n",
      "iterations 4614 accuracy : 0.930896870405377  loss : 0.34201174535291146\n",
      "iterations 4615 accuracy : 0.930896870405377  loss : 0.34199832773393635\n",
      "iterations 4616 accuracy : 0.930896870405377  loss : 0.3419846226686274\n",
      "iterations 4617 accuracy : 0.930896870405377  loss : 0.3419717350098013\n",
      "iterations 4618 accuracy : 0.930896870405377  loss : 0.34195843974470974\n",
      "iterations 4619 accuracy : 0.930896870405377  loss : 0.3419446163385934\n",
      "iterations 4620 accuracy : 0.930896870405377  loss : 0.341930960860319\n",
      "iterations 4621 accuracy : 0.930896870405377  loss : 0.3419175271194218\n",
      "iterations 4622 accuracy : 0.930896870405377  loss : 0.34190388995245424\n",
      "iterations 4623 accuracy : 0.930896870405377  loss : 0.34189085575521605\n",
      "iterations 4624 accuracy : 0.930896870405377  loss : 0.34187701393506564\n",
      "iterations 4625 accuracy : 0.930896870405377  loss : 0.34186376586933226\n",
      "iterations 4626 accuracy : 0.930896870405377  loss : 0.3418504632966186\n",
      "iterations 4627 accuracy : 0.930896870405377  loss : 0.341837875746243\n",
      "iterations 4628 accuracy : 0.930896870405377  loss : 0.34182499125663995\n",
      "iterations 4629 accuracy : 0.930896870405377  loss : 0.3418124478701203\n",
      "iterations 4630 accuracy : 0.930896870405377  loss : 0.34179939338557114\n",
      "iterations 4631 accuracy : 0.930896870405377  loss : 0.3417864523977849\n",
      "iterations 4632 accuracy : 0.930896870405377  loss : 0.34177352582287046\n",
      "iterations 4633 accuracy : 0.930896870405377  loss : 0.34176028155567734\n",
      "iterations 4634 accuracy : 0.930896870405377  loss : 0.34174748209778444\n",
      "iterations 4635 accuracy : 0.930896870405377  loss : 0.341734721436945\n",
      "iterations 4636 accuracy : 0.930896870405377  loss : 0.3417219075670952\n",
      "iterations 4637 accuracy : 0.930896870405377  loss : 0.34170821079039937\n",
      "iterations 4638 accuracy : 0.930896870405377  loss : 0.3416949292166494\n",
      "iterations 4639 accuracy : 0.930896870405377  loss : 0.3416819255912212\n",
      "iterations 4640 accuracy : 0.930896870405377  loss : 0.3416693488143245\n",
      "iterations 4641 accuracy : 0.930896870405377  loss : 0.34165648104171975\n",
      "iterations 4642 accuracy : 0.930896870405377  loss : 0.3416429479629351\n",
      "iterations 4643 accuracy : 0.930896870405377  loss : 0.3416295230038197\n",
      "iterations 4644 accuracy : 0.930896870405377  loss : 0.34161627624916957\n",
      "iterations 4645 accuracy : 0.930896870405377  loss : 0.3416030912030113\n",
      "iterations 4646 accuracy : 0.930896870405377  loss : 0.3415910825560305\n",
      "iterations 4647 accuracy : 0.930896870405377  loss : 0.3415781846085804\n",
      "iterations 4648 accuracy : 0.930896870405377  loss : 0.341565088228876\n",
      "iterations 4649 accuracy : 0.930896870405377  loss : 0.34155241883294507\n",
      "iterations 4650 accuracy : 0.930896870405377  loss : 0.3415386244568389\n",
      "iterations 4651 accuracy : 0.930896870405377  loss : 0.34152525329542477\n",
      "iterations 4652 accuracy : 0.930896870405377  loss : 0.34151171286794807\n",
      "iterations 4653 accuracy : 0.930896870405377  loss : 0.34149905848279716\n",
      "iterations 4654 accuracy : 0.930896870405377  loss : 0.34148639094982997\n",
      "iterations 4655 accuracy : 0.930896870405377  loss : 0.3414744162938654\n",
      "iterations 4656 accuracy : 0.930896870405377  loss : 0.34146117659137676\n",
      "iterations 4657 accuracy : 0.930896870405377  loss : 0.34144810321175734\n",
      "iterations 4658 accuracy : 0.930896870405377  loss : 0.3414350937680199\n",
      "iterations 4659 accuracy : 0.930896870405377  loss : 0.3414224324280103\n",
      "iterations 4660 accuracy : 0.930896870405377  loss : 0.3414093802277003\n",
      "iterations 4661 accuracy : 0.930896870405377  loss : 0.34139616779488013\n",
      "iterations 4662 accuracy : 0.930896870405377  loss : 0.3413827178364339\n",
      "iterations 4663 accuracy : 0.930896870405377  loss : 0.3413698249218311\n",
      "iterations 4664 accuracy : 0.930896870405377  loss : 0.3413564594160195\n",
      "iterations 4665 accuracy : 0.930896870405377  loss : 0.3413435300156291\n",
      "iterations 4666 accuracy : 0.930896870405377  loss : 0.34133012920003003\n",
      "iterations 4667 accuracy : 0.930896870405377  loss : 0.34131743868287856\n",
      "iterations 4668 accuracy : 0.930896870405377  loss : 0.3413050368337026\n",
      "iterations 4669 accuracy : 0.930896870405377  loss : 0.3412921824750426\n",
      "iterations 4670 accuracy : 0.930896870405377  loss : 0.34127932201954264\n",
      "iterations 4671 accuracy : 0.930896870405377  loss : 0.34126708750199997\n",
      "iterations 4672 accuracy : 0.930896870405377  loss : 0.34125421010291357\n",
      "iterations 4673 accuracy : 0.930896870405377  loss : 0.34124061889593815\n",
      "iterations 4674 accuracy : 0.930896870405377  loss : 0.34122768900522515\n",
      "iterations 4675 accuracy : 0.930896870405377  loss : 0.3412137500467742\n",
      "iterations 4676 accuracy : 0.930896870405377  loss : 0.34120067254696484\n",
      "iterations 4677 accuracy : 0.9311069103129594  loss : 0.34118769343598354\n",
      "iterations 4678 accuracy : 0.9311069103129594  loss : 0.34117465556270016\n",
      "iterations 4679 accuracy : 0.9311069103129594  loss : 0.34116131373937975\n",
      "iterations 4680 accuracy : 0.9311069103129594  loss : 0.3411487442224304\n",
      "iterations 4681 accuracy : 0.9311069103129594  loss : 0.34113664975136787\n",
      "iterations 4682 accuracy : 0.930896870405377  loss : 0.341124096178448\n",
      "iterations 4683 accuracy : 0.930896870405377  loss : 0.34111184439806824\n",
      "iterations 4684 accuracy : 0.930896870405377  loss : 0.34109871424586785\n",
      "iterations 4685 accuracy : 0.9311069103129594  loss : 0.3410854623243641\n",
      "iterations 4686 accuracy : 0.9311069103129594  loss : 0.34107234508339523\n",
      "iterations 4687 accuracy : 0.9311069103129594  loss : 0.34105994789900596\n",
      "iterations 4688 accuracy : 0.9311069103129594  loss : 0.34104658381351083\n",
      "iterations 4689 accuracy : 0.9311069103129594  loss : 0.34103322464403857\n",
      "iterations 4690 accuracy : 0.9311069103129594  loss : 0.3410199057013972\n",
      "iterations 4691 accuracy : 0.9311069103129594  loss : 0.341007058145238\n",
      "iterations 4692 accuracy : 0.9311069103129594  loss : 0.34099433002847385\n",
      "iterations 4693 accuracy : 0.9311069103129594  loss : 0.3409807618869505\n",
      "iterations 4694 accuracy : 0.9311069103129594  loss : 0.3409684238543438\n",
      "iterations 4695 accuracy : 0.9311069103129594  loss : 0.34095593762751125\n",
      "iterations 4696 accuracy : 0.9311069103129594  loss : 0.34094249256806375\n",
      "iterations 4697 accuracy : 0.9311069103129594  loss : 0.3409294172434195\n",
      "iterations 4698 accuracy : 0.9311069103129594  loss : 0.3409163306622569\n",
      "iterations 4699 accuracy : 0.9311069103129594  loss : 0.3409040051792151\n",
      "iterations 4700 accuracy : 0.9311069103129594  loss : 0.3408910232489049\n",
      "iterations 4701 accuracy : 0.9311069103129594  loss : 0.34087830515587875\n",
      "iterations 4702 accuracy : 0.9311069103129594  loss : 0.3408653764678815\n",
      "iterations 4703 accuracy : 0.9311069103129594  loss : 0.340852697726494\n",
      "iterations 4704 accuracy : 0.9311069103129594  loss : 0.3408399457608858\n",
      "iterations 4705 accuracy : 0.9311069103129594  loss : 0.3408270296306517\n",
      "iterations 4706 accuracy : 0.9311069103129594  loss : 0.3408141007169756\n",
      "iterations 4707 accuracy : 0.9311069103129594  loss : 0.3408011981747718\n",
      "iterations 4708 accuracy : 0.9311069103129594  loss : 0.3407889589888115\n",
      "iterations 4709 accuracy : 0.9311069103129594  loss : 0.3407763204227288\n",
      "iterations 4710 accuracy : 0.9311069103129594  loss : 0.3407632533100095\n",
      "iterations 4711 accuracy : 0.9311069103129594  loss : 0.34075073387137306\n",
      "iterations 4712 accuracy : 0.9311069103129594  loss : 0.34073773573856003\n",
      "iterations 4713 accuracy : 0.9311069103129594  loss : 0.34072544002458904\n",
      "iterations 4714 accuracy : 0.9311069103129594  loss : 0.34071261931960634\n",
      "iterations 4715 accuracy : 0.9311069103129594  loss : 0.34070021454800614\n",
      "iterations 4716 accuracy : 0.9311069103129594  loss : 0.3406873174719457\n",
      "iterations 4717 accuracy : 0.9311069103129594  loss : 0.3406742749233108\n",
      "iterations 4718 accuracy : 0.9311069103129594  loss : 0.3406608061780707\n",
      "iterations 4719 accuracy : 0.9311069103129594  loss : 0.34064784594473707\n",
      "iterations 4720 accuracy : 0.9311069103129594  loss : 0.3406351633366303\n",
      "iterations 4721 accuracy : 0.9311069103129594  loss : 0.34062253603612175\n",
      "iterations 4722 accuracy : 0.9311069103129594  loss : 0.34060977429447636\n",
      "iterations 4723 accuracy : 0.9311069103129594  loss : 0.34059676303830827\n",
      "iterations 4724 accuracy : 0.9311069103129594  loss : 0.3405840593799514\n",
      "iterations 4725 accuracy : 0.9311069103129594  loss : 0.3405717457642917\n",
      "iterations 4726 accuracy : 0.9311069103129594  loss : 0.3405589490709567\n",
      "iterations 4727 accuracy : 0.9311069103129594  loss : 0.34054653579691785\n",
      "iterations 4728 accuracy : 0.9311069103129594  loss : 0.3405334915655742\n",
      "iterations 4729 accuracy : 0.9311069103129594  loss : 0.34051984462126866\n",
      "iterations 4730 accuracy : 0.9311069103129594  loss : 0.3405075156076263\n",
      "iterations 4731 accuracy : 0.9311069103129594  loss : 0.3404944596442955\n",
      "iterations 4732 accuracy : 0.9311069103129594  loss : 0.34048068127520764\n",
      "iterations 4733 accuracy : 0.9311069103129594  loss : 0.3404683558974817\n",
      "iterations 4734 accuracy : 0.9311069103129594  loss : 0.3404560906440145\n",
      "iterations 4735 accuracy : 0.9311069103129594  loss : 0.34044408621974465\n",
      "iterations 4736 accuracy : 0.9311069103129594  loss : 0.34043186378458007\n",
      "iterations 4737 accuracy : 0.9311069103129594  loss : 0.3404192253960135\n",
      "iterations 4738 accuracy : 0.9311069103129594  loss : 0.34040635871628305\n",
      "iterations 4739 accuracy : 0.9311069103129594  loss : 0.3403937577900259\n",
      "iterations 4740 accuracy : 0.9311069103129594  loss : 0.34038121179429687\n",
      "iterations 4741 accuracy : 0.9311069103129594  loss : 0.340368696726643\n",
      "iterations 4742 accuracy : 0.9311069103129594  loss : 0.3403557005523774\n",
      "iterations 4743 accuracy : 0.9311069103129594  loss : 0.3403430675288189\n",
      "iterations 4744 accuracy : 0.9311069103129594  loss : 0.34033123411222216\n",
      "iterations 4745 accuracy : 0.9311069103129594  loss : 0.34031896160316033\n",
      "iterations 4746 accuracy : 0.9311069103129594  loss : 0.3403049711035159\n",
      "iterations 4747 accuracy : 0.9311069103129594  loss : 0.3402924033961544\n",
      "iterations 4748 accuracy : 0.9311069103129594  loss : 0.34027985295052043\n",
      "iterations 4749 accuracy : 0.9311069103129594  loss : 0.34026723435226663\n",
      "iterations 4750 accuracy : 0.9311069103129594  loss : 0.3402551041618698\n",
      "iterations 4751 accuracy : 0.9311069103129594  loss : 0.34024236163093496\n",
      "iterations 4752 accuracy : 0.9311069103129594  loss : 0.3402290793589563\n",
      "iterations 4753 accuracy : 0.9311069103129594  loss : 0.3402160439755675\n",
      "iterations 4754 accuracy : 0.9311069103129594  loss : 0.3402041376104232\n",
      "iterations 4755 accuracy : 0.9311069103129594  loss : 0.34019219820844865\n",
      "iterations 4756 accuracy : 0.9311069103129594  loss : 0.3401798091902216\n",
      "iterations 4757 accuracy : 0.9311069103129594  loss : 0.3401666074897846\n",
      "iterations 4758 accuracy : 0.931316950220542  loss : 0.3401542505405992\n",
      "iterations 4759 accuracy : 0.9311069103129594  loss : 0.3401413748318008\n",
      "iterations 4760 accuracy : 0.9311069103129594  loss : 0.3401287956950678\n",
      "iterations 4761 accuracy : 0.9311069103129594  loss : 0.3401162798517011\n",
      "iterations 4762 accuracy : 0.9311069103129594  loss : 0.3401035123051511\n",
      "iterations 4763 accuracy : 0.9311069103129594  loss : 0.3400908085790371\n",
      "iterations 4764 accuracy : 0.9311069103129594  loss : 0.34007791560609596\n",
      "iterations 4765 accuracy : 0.9311069103129594  loss : 0.3400658973024246\n",
      "iterations 4766 accuracy : 0.9311069103129594  loss : 0.3400533234553992\n",
      "iterations 4767 accuracy : 0.9311069103129594  loss : 0.3400406096527079\n",
      "iterations 4768 accuracy : 0.9311069103129594  loss : 0.3400273501982578\n",
      "iterations 4769 accuracy : 0.9311069103129594  loss : 0.3400145227351654\n",
      "iterations 4770 accuracy : 0.9311069103129594  loss : 0.3400018990252519\n",
      "iterations 4771 accuracy : 0.9311069103129594  loss : 0.3399891741715348\n",
      "iterations 4772 accuracy : 0.9311069103129594  loss : 0.3399766052703942\n",
      "iterations 4773 accuracy : 0.9311069103129594  loss : 0.33996417076805535\n",
      "iterations 4774 accuracy : 0.9311069103129594  loss : 0.3399518831923576\n",
      "iterations 4775 accuracy : 0.9311069103129594  loss : 0.33993981314398153\n",
      "iterations 4776 accuracy : 0.9311069103129594  loss : 0.339927720513791\n",
      "iterations 4777 accuracy : 0.9311069103129594  loss : 0.33991474355488543\n",
      "iterations 4778 accuracy : 0.9311069103129594  loss : 0.33990256986861517\n",
      "iterations 4779 accuracy : 0.9311069103129594  loss : 0.33988950602530704\n",
      "iterations 4780 accuracy : 0.9311069103129594  loss : 0.3398771993150538\n",
      "iterations 4781 accuracy : 0.9311069103129594  loss : 0.3398648426639057\n",
      "iterations 4782 accuracy : 0.9311069103129594  loss : 0.3398522865791542\n",
      "iterations 4783 accuracy : 0.9311069103129594  loss : 0.3398406207006898\n",
      "iterations 4784 accuracy : 0.9311069103129594  loss : 0.3398277084215502\n",
      "iterations 4785 accuracy : 0.9311069103129594  loss : 0.3398164446854286\n",
      "iterations 4786 accuracy : 0.9311069103129594  loss : 0.339804563327145\n",
      "iterations 4787 accuracy : 0.9311069103129594  loss : 0.3397922592768288\n",
      "iterations 4788 accuracy : 0.9311069103129594  loss : 0.33977976120013176\n",
      "iterations 4789 accuracy : 0.9311069103129594  loss : 0.33976654577893073\n",
      "iterations 4790 accuracy : 0.9311069103129594  loss : 0.33975370473891303\n",
      "iterations 4791 accuracy : 0.9311069103129594  loss : 0.3397416155354993\n",
      "iterations 4792 accuracy : 0.9311069103129594  loss : 0.33972940083082376\n",
      "iterations 4793 accuracy : 0.9311069103129594  loss : 0.339717292439263\n",
      "iterations 4794 accuracy : 0.9311069103129594  loss : 0.3397050634683364\n",
      "iterations 4795 accuracy : 0.9311069103129594  loss : 0.3396919589388896\n",
      "iterations 4796 accuracy : 0.9311069103129594  loss : 0.3396797187281011\n",
      "iterations 4797 accuracy : 0.9311069103129594  loss : 0.33966804112563653\n",
      "iterations 4798 accuracy : 0.9311069103129594  loss : 0.33965625809191297\n",
      "iterations 4799 accuracy : 0.9311069103129594  loss : 0.3396437584810526\n",
      "iterations 4800 accuracy : 0.9311069103129594  loss : 0.3396311914102062\n",
      "iterations 4801 accuracy : 0.9311069103129594  loss : 0.33961856108693195\n",
      "iterations 4802 accuracy : 0.9311069103129594  loss : 0.3396063493800757\n",
      "iterations 4803 accuracy : 0.9311069103129594  loss : 0.3395937647116271\n",
      "iterations 4804 accuracy : 0.9311069103129594  loss : 0.33958075813481625\n",
      "iterations 4805 accuracy : 0.9311069103129594  loss : 0.33956912024775443\n",
      "iterations 4806 accuracy : 0.9311069103129594  loss : 0.3395574495433227\n",
      "iterations 4807 accuracy : 0.9311069103129594  loss : 0.3395449717880898\n",
      "iterations 4808 accuracy : 0.9311069103129594  loss : 0.3395329066493225\n",
      "iterations 4809 accuracy : 0.9311069103129594  loss : 0.3395204878121455\n",
      "iterations 4810 accuracy : 0.9311069103129594  loss : 0.33950820601581405\n",
      "iterations 4811 accuracy : 0.9311069103129594  loss : 0.33949647714113307\n",
      "iterations 4812 accuracy : 0.9311069103129594  loss : 0.3394837925039178\n",
      "iterations 4813 accuracy : 0.9311069103129594  loss : 0.33947161257411573\n",
      "iterations 4814 accuracy : 0.9311069103129594  loss : 0.3394591835586729\n",
      "iterations 4815 accuracy : 0.9311069103129594  loss : 0.33944650017990907\n",
      "iterations 4816 accuracy : 0.9311069103129594  loss : 0.33943473106918626\n",
      "iterations 4817 accuracy : 0.9311069103129594  loss : 0.3394221716027721\n",
      "iterations 4818 accuracy : 0.9311069103129594  loss : 0.33940968161851426\n",
      "iterations 4819 accuracy : 0.9311069103129594  loss : 0.3393979184620478\n",
      "iterations 4820 accuracy : 0.9311069103129594  loss : 0.33938628853391867\n",
      "iterations 4821 accuracy : 0.9311069103129594  loss : 0.3393733828417556\n",
      "iterations 4822 accuracy : 0.9311069103129594  loss : 0.3393606199280844\n",
      "iterations 4823 accuracy : 0.9311069103129594  loss : 0.3393479973636509\n",
      "iterations 4824 accuracy : 0.9311069103129594  loss : 0.3393356078021737\n",
      "iterations 4825 accuracy : 0.9311069103129594  loss : 0.33932358964529746\n",
      "iterations 4826 accuracy : 0.9311069103129594  loss : 0.33931178597651857\n",
      "iterations 4827 accuracy : 0.9311069103129594  loss : 0.3392993681135219\n",
      "iterations 4828 accuracy : 0.9311069103129594  loss : 0.3392872845155534\n",
      "iterations 4829 accuracy : 0.9311069103129594  loss : 0.33927504633873135\n",
      "iterations 4830 accuracy : 0.9311069103129594  loss : 0.3392630979328736\n",
      "iterations 4831 accuracy : 0.9311069103129594  loss : 0.33925046045938134\n",
      "iterations 4832 accuracy : 0.9311069103129594  loss : 0.33923748992810376\n",
      "iterations 4833 accuracy : 0.9311069103129594  loss : 0.3392249805993475\n",
      "iterations 4834 accuracy : 0.9311069103129594  loss : 0.3392129284225834\n",
      "iterations 4835 accuracy : 0.9311069103129594  loss : 0.33920055538435334\n",
      "iterations 4836 accuracy : 0.9311069103129594  loss : 0.3391885417565349\n",
      "iterations 4837 accuracy : 0.9311069103129594  loss : 0.33917659064099737\n",
      "iterations 4838 accuracy : 0.9311069103129594  loss : 0.3391644652079486\n",
      "iterations 4839 accuracy : 0.9311069103129594  loss : 0.33915203756581724\n",
      "iterations 4840 accuracy : 0.9311069103129594  loss : 0.33913980852277853\n",
      "iterations 4841 accuracy : 0.9311069103129594  loss : 0.33912801506758095\n",
      "iterations 4842 accuracy : 0.9311069103129594  loss : 0.33911615079554075\n",
      "iterations 4843 accuracy : 0.9311069103129594  loss : 0.33910333615684635\n",
      "iterations 4844 accuracy : 0.9311069103129594  loss : 0.33909119731877563\n",
      "iterations 4845 accuracy : 0.9311069103129594  loss : 0.3390789805274187\n",
      "iterations 4846 accuracy : 0.9311069103129594  loss : 0.3390662067030515\n",
      "iterations 4847 accuracy : 0.9311069103129594  loss : 0.339053367637526\n",
      "iterations 4848 accuracy : 0.9311069103129594  loss : 0.3390407409426316\n",
      "iterations 4849 accuracy : 0.9311069103129594  loss : 0.3390286982792817\n",
      "iterations 4850 accuracy : 0.9311069103129594  loss : 0.33901628069940026\n",
      "iterations 4851 accuracy : 0.9311069103129594  loss : 0.3390039086690753\n",
      "iterations 4852 accuracy : 0.9311069103129594  loss : 0.3389927902845804\n",
      "iterations 4853 accuracy : 0.9311069103129594  loss : 0.3389803569681174\n",
      "iterations 4854 accuracy : 0.9311069103129594  loss : 0.33896800515169684\n",
      "iterations 4855 accuracy : 0.9311069103129594  loss : 0.33895657980536426\n",
      "iterations 4856 accuracy : 0.9311069103129594  loss : 0.3389442457864466\n",
      "iterations 4857 accuracy : 0.9311069103129594  loss : 0.3389322626326838\n",
      "iterations 4858 accuracy : 0.9311069103129594  loss : 0.33892017982918926\n",
      "iterations 4859 accuracy : 0.9311069103129594  loss : 0.3389078753857045\n",
      "iterations 4860 accuracy : 0.9311069103129594  loss : 0.33889570804614233\n",
      "iterations 4861 accuracy : 0.9311069103129594  loss : 0.338883106790948\n",
      "iterations 4862 accuracy : 0.9311069103129594  loss : 0.3388709209155112\n",
      "iterations 4863 accuracy : 0.9311069103129594  loss : 0.3388590657420069\n",
      "iterations 4864 accuracy : 0.9311069103129594  loss : 0.3388465487365289\n",
      "iterations 4865 accuracy : 0.9311069103129594  loss : 0.3388350092951855\n",
      "iterations 4866 accuracy : 0.9311069103129594  loss : 0.3388222752143982\n",
      "iterations 4867 accuracy : 0.9311069103129594  loss : 0.3388095721222909\n",
      "iterations 4868 accuracy : 0.9311069103129594  loss : 0.33879799252606496\n",
      "iterations 4869 accuracy : 0.9311069103129594  loss : 0.3387861675337621\n",
      "iterations 4870 accuracy : 0.9311069103129594  loss : 0.33877402308751703\n",
      "iterations 4871 accuracy : 0.9311069103129594  loss : 0.3387614450626829\n",
      "iterations 4872 accuracy : 0.9311069103129594  loss : 0.3387495959442609\n",
      "iterations 4873 accuracy : 0.9311069103129594  loss : 0.3387376583114039\n",
      "iterations 4874 accuracy : 0.9311069103129594  loss : 0.3387257331508737\n",
      "iterations 4875 accuracy : 0.9311069103129594  loss : 0.3387135288924987\n",
      "iterations 4876 accuracy : 0.9311069103129594  loss : 0.3387014741350429\n",
      "iterations 4877 accuracy : 0.9311069103129594  loss : 0.33868861764819175\n",
      "iterations 4878 accuracy : 0.9311069103129594  loss : 0.33867556647791186\n",
      "iterations 4879 accuracy : 0.9311069103129594  loss : 0.33866332768465374\n",
      "iterations 4880 accuracy : 0.9311069103129594  loss : 0.3386520815584916\n",
      "iterations 4881 accuracy : 0.9311069103129594  loss : 0.3386401046013989\n",
      "iterations 4882 accuracy : 0.9311069103129594  loss : 0.33862766193045557\n",
      "iterations 4883 accuracy : 0.9311069103129594  loss : 0.33861609932494546\n",
      "iterations 4884 accuracy : 0.9311069103129594  loss : 0.33860412865339207\n",
      "iterations 4885 accuracy : 0.930896870405377  loss : 0.33859160129729404\n",
      "iterations 4886 accuracy : 0.930896870405377  loss : 0.338579168792616\n",
      "iterations 4887 accuracy : 0.930896870405377  loss : 0.33856805783248517\n",
      "iterations 4888 accuracy : 0.930896870405377  loss : 0.3385555763890864\n",
      "iterations 4889 accuracy : 0.9311069103129594  loss : 0.3385435244015724\n",
      "iterations 4890 accuracy : 0.9311069103129594  loss : 0.33853151831197265\n",
      "iterations 4891 accuracy : 0.9311069103129594  loss : 0.33851937761681117\n",
      "iterations 4892 accuracy : 0.9311069103129594  loss : 0.33850715676004306\n",
      "iterations 4893 accuracy : 0.9311069103129594  loss : 0.33849499006305445\n",
      "iterations 4894 accuracy : 0.9311069103129594  loss : 0.33848242144589236\n",
      "iterations 4895 accuracy : 0.9311069103129594  loss : 0.3384709233768792\n",
      "iterations 4896 accuracy : 0.9311069103129594  loss : 0.3384589188809255\n",
      "iterations 4897 accuracy : 0.9311069103129594  loss : 0.33844740853449223\n",
      "iterations 4898 accuracy : 0.930896870405377  loss : 0.3384361915919177\n",
      "iterations 4899 accuracy : 0.930896870405377  loss : 0.33842451446633454\n",
      "iterations 4900 accuracy : 0.930896870405377  loss : 0.3384124546105451\n",
      "iterations 4901 accuracy : 0.930896870405377  loss : 0.3384011539412299\n",
      "iterations 4902 accuracy : 0.930896870405377  loss : 0.3383888398126215\n",
      "iterations 4903 accuracy : 0.930896870405377  loss : 0.3383769290375871\n",
      "iterations 4904 accuracy : 0.9311069103129594  loss : 0.33836449808007557\n",
      "iterations 4905 accuracy : 0.930896870405377  loss : 0.3383528664590247\n",
      "iterations 4906 accuracy : 0.930896870405377  loss : 0.3383406180426627\n",
      "iterations 4907 accuracy : 0.930896870405377  loss : 0.3383289794172848\n",
      "iterations 4908 accuracy : 0.9311069103129594  loss : 0.33831633542221967\n",
      "iterations 4909 accuracy : 0.9311069103129594  loss : 0.33830358915817355\n",
      "iterations 4910 accuracy : 0.9311069103129594  loss : 0.3382920220692476\n",
      "iterations 4911 accuracy : 0.930896870405377  loss : 0.33828039739918647\n",
      "iterations 4912 accuracy : 0.9311069103129594  loss : 0.3382678384567051\n",
      "iterations 4913 accuracy : 0.9311069103129594  loss : 0.3382555581771884\n",
      "iterations 4914 accuracy : 0.9311069103129594  loss : 0.3382435504573354\n",
      "iterations 4915 accuracy : 0.931316950220542  loss : 0.3382306925011958\n",
      "iterations 4916 accuracy : 0.931316950220542  loss : 0.3382180001757121\n",
      "iterations 4917 accuracy : 0.931316950220542  loss : 0.33820565842142153\n",
      "iterations 4918 accuracy : 0.9311069103129594  loss : 0.33819422234610597\n",
      "iterations 4919 accuracy : 0.931316950220542  loss : 0.33818207363349806\n",
      "iterations 4920 accuracy : 0.9311069103129594  loss : 0.33817052946538473\n",
      "iterations 4921 accuracy : 0.931316950220542  loss : 0.33815835928682675\n",
      "iterations 4922 accuracy : 0.931316950220542  loss : 0.33814694641895393\n",
      "iterations 4923 accuracy : 0.931316950220542  loss : 0.3381351056308154\n",
      "iterations 4924 accuracy : 0.931316950220542  loss : 0.3381232559683223\n",
      "iterations 4925 accuracy : 0.931316950220542  loss : 0.3381116989090253\n",
      "iterations 4926 accuracy : 0.931316950220542  loss : 0.3380996924034766\n",
      "iterations 4927 accuracy : 0.931316950220542  loss : 0.33808800247502135\n",
      "iterations 4928 accuracy : 0.931316950220542  loss : 0.3380765984911099\n",
      "iterations 4929 accuracy : 0.931316950220542  loss : 0.3380647223747696\n",
      "iterations 4930 accuracy : 0.931316950220542  loss : 0.3380524927718495\n",
      "iterations 4931 accuracy : 0.931316950220542  loss : 0.33804004032811336\n",
      "iterations 4932 accuracy : 0.931316950220542  loss : 0.33802732908525296\n",
      "iterations 4933 accuracy : 0.931316950220542  loss : 0.3380157760862198\n",
      "iterations 4934 accuracy : 0.931316950220542  loss : 0.3380036748012025\n",
      "iterations 4935 accuracy : 0.931316950220542  loss : 0.33799300869314636\n",
      "iterations 4936 accuracy : 0.931316950220542  loss : 0.3379814558637351\n",
      "iterations 4937 accuracy : 0.931316950220542  loss : 0.3379697450973484\n",
      "iterations 4938 accuracy : 0.931316950220542  loss : 0.33795806508862775\n",
      "iterations 4939 accuracy : 0.931316950220542  loss : 0.3379468712288499\n",
      "iterations 4940 accuracy : 0.931316950220542  loss : 0.3379354018291768\n",
      "iterations 4941 accuracy : 0.9311069103129594  loss : 0.3379238373067762\n",
      "iterations 4942 accuracy : 0.931316950220542  loss : 0.33791168001816746\n",
      "iterations 4943 accuracy : 0.931316950220542  loss : 0.33789968334958703\n",
      "iterations 4944 accuracy : 0.931316950220542  loss : 0.3378882833577025\n",
      "iterations 4945 accuracy : 0.931316950220542  loss : 0.33787652618976505\n",
      "iterations 4946 accuracy : 0.931316950220542  loss : 0.3378650082985646\n",
      "iterations 4947 accuracy : 0.931316950220542  loss : 0.33785273883209777\n",
      "iterations 4948 accuracy : 0.931316950220542  loss : 0.33784094396039793\n",
      "iterations 4949 accuracy : 0.931316950220542  loss : 0.3378295307199244\n",
      "iterations 4950 accuracy : 0.931316950220542  loss : 0.33781767555527037\n",
      "iterations 4951 accuracy : 0.931316950220542  loss : 0.33780569989918663\n",
      "iterations 4952 accuracy : 0.931316950220542  loss : 0.337794899623966\n",
      "iterations 4953 accuracy : 0.9311069103129594  loss : 0.3377834723787452\n",
      "iterations 4954 accuracy : 0.9311069103129594  loss : 0.3377720144470049\n",
      "iterations 4955 accuracy : 0.931316950220542  loss : 0.337760058256581\n",
      "iterations 4956 accuracy : 0.931316950220542  loss : 0.3377488558063628\n",
      "iterations 4957 accuracy : 0.9311069103129594  loss : 0.3377372411905052\n",
      "iterations 4958 accuracy : 0.931316950220542  loss : 0.3377255539506103\n",
      "iterations 4959 accuracy : 0.931316950220542  loss : 0.3377139176006713\n",
      "iterations 4960 accuracy : 0.931316950220542  loss : 0.3377020847609063\n",
      "iterations 4961 accuracy : 0.931316950220542  loss : 0.33769089855122825\n",
      "iterations 4962 accuracy : 0.931316950220542  loss : 0.33767910801339507\n",
      "iterations 4963 accuracy : 0.931316950220542  loss : 0.33766681788923736\n",
      "iterations 4964 accuracy : 0.9311069103129594  loss : 0.33765646842076574\n",
      "iterations 4965 accuracy : 0.931316950220542  loss : 0.33764458829919436\n",
      "iterations 4966 accuracy : 0.9311069103129594  loss : 0.33763281648489124\n",
      "iterations 4967 accuracy : 0.9311069103129594  loss : 0.33762087458510287\n",
      "iterations 4968 accuracy : 0.9311069103129594  loss : 0.3376092304289346\n",
      "iterations 4969 accuracy : 0.9311069103129594  loss : 0.33759790130524675\n",
      "iterations 4970 accuracy : 0.931316950220542  loss : 0.3375857578664839\n",
      "iterations 4971 accuracy : 0.931316950220542  loss : 0.3375736362555178\n",
      "iterations 4972 accuracy : 0.931316950220542  loss : 0.3375624735012343\n",
      "iterations 4973 accuracy : 0.9311069103129594  loss : 0.3375511961718253\n",
      "iterations 4974 accuracy : 0.9311069103129594  loss : 0.3375396510414896\n",
      "iterations 4975 accuracy : 0.9311069103129594  loss : 0.3375281871845223\n",
      "iterations 4976 accuracy : 0.931316950220542  loss : 0.33751613492833366\n",
      "iterations 4977 accuracy : 0.931316950220542  loss : 0.3375045281362281\n",
      "iterations 4978 accuracy : 0.9311069103129594  loss : 0.33749311183971503\n",
      "iterations 4979 accuracy : 0.931316950220542  loss : 0.3374811397716345\n",
      "iterations 4980 accuracy : 0.931316950220542  loss : 0.33746900053981976\n",
      "iterations 4981 accuracy : 0.9311069103129594  loss : 0.33745785421427854\n",
      "iterations 4982 accuracy : 0.9311069103129594  loss : 0.33744728456138273\n",
      "iterations 4983 accuracy : 0.931316950220542  loss : 0.33743475591168964\n",
      "iterations 4984 accuracy : 0.9311069103129594  loss : 0.33742372435321866\n",
      "iterations 4985 accuracy : 0.9311069103129594  loss : 0.33741203828453914\n",
      "iterations 4986 accuracy : 0.9311069103129594  loss : 0.33740074165368233\n",
      "iterations 4987 accuracy : 0.9311069103129594  loss : 0.3373890218346569\n",
      "iterations 4988 accuracy : 0.9311069103129594  loss : 0.337377924560135\n",
      "iterations 4989 accuracy : 0.9311069103129594  loss : 0.33736629613284874\n",
      "iterations 4990 accuracy : 0.9311069103129594  loss : 0.33735460607308887\n",
      "iterations 4991 accuracy : 0.9311069103129594  loss : 0.3373428447534465\n",
      "iterations 4992 accuracy : 0.9311069103129594  loss : 0.3373316807957634\n",
      "iterations 4993 accuracy : 0.9311069103129594  loss : 0.33731999287091907\n",
      "iterations 4994 accuracy : 0.9311069103129594  loss : 0.33730872086625796\n",
      "iterations 4995 accuracy : 0.9311069103129594  loss : 0.3372971508397665\n",
      "iterations 4996 accuracy : 0.9311069103129594  loss : 0.3372853372149226\n",
      "iterations 4997 accuracy : 0.931316950220542  loss : 0.3372729761825187\n",
      "iterations 4998 accuracy : 0.9311069103129594  loss : 0.33726186212581705\n",
      "iterations 4999 accuracy : 0.9311069103129594  loss : 0.33725046090210886\n",
      "iterations 5000 accuracy : 0.931316950220542  loss : 0.33723854485960064\n",
      "iterations 5001 accuracy : 0.931316950220542  loss : 0.33722634848656974\n",
      "iterations 5002 accuracy : 0.931316950220542  loss : 0.3372146239659832\n",
      "iterations 5003 accuracy : 0.931316950220542  loss : 0.3372028318953254\n",
      "iterations 5004 accuracy : 0.931316950220542  loss : 0.33719113631747144\n",
      "iterations 5005 accuracy : 0.931316950220542  loss : 0.3371787448473937\n",
      "iterations 5006 accuracy : 0.931316950220542  loss : 0.33716718082670305\n",
      "iterations 5007 accuracy : 0.931316950220542  loss : 0.3371558387174746\n",
      "iterations 5008 accuracy : 0.931316950220542  loss : 0.3371446480565623\n",
      "iterations 5009 accuracy : 0.931316950220542  loss : 0.33713302535244316\n",
      "iterations 5010 accuracy : 0.931316950220542  loss : 0.3371223325963505\n",
      "iterations 5011 accuracy : 0.931316950220542  loss : 0.33711059216954053\n",
      "iterations 5012 accuracy : 0.931316950220542  loss : 0.3370995525795893\n",
      "iterations 5013 accuracy : 0.931316950220542  loss : 0.3370889365437592\n",
      "iterations 5014 accuracy : 0.931316950220542  loss : 0.33707724885628115\n",
      "iterations 5015 accuracy : 0.931316950220542  loss : 0.33706516525676233\n",
      "iterations 5016 accuracy : 0.931316950220542  loss : 0.3370534881577699\n",
      "iterations 5017 accuracy : 0.931316950220542  loss : 0.33704235723461073\n",
      "iterations 5018 accuracy : 0.931316950220542  loss : 0.3370306133843122\n",
      "iterations 5019 accuracy : 0.931316950220542  loss : 0.33701918899218025\n",
      "iterations 5020 accuracy : 0.931316950220542  loss : 0.33700720575311155\n",
      "iterations 5021 accuracy : 0.931316950220542  loss : 0.336996038793494\n",
      "iterations 5022 accuracy : 0.931316950220542  loss : 0.33698470766629735\n",
      "iterations 5023 accuracy : 0.931316950220542  loss : 0.3369729336420997\n",
      "iterations 5024 accuracy : 0.931316950220542  loss : 0.3369612061754503\n",
      "iterations 5025 accuracy : 0.931316950220542  loss : 0.3369498149676132\n",
      "iterations 5026 accuracy : 0.931316950220542  loss : 0.33693781300685716\n",
      "iterations 5027 accuracy : 0.931316950220542  loss : 0.3369266490264109\n",
      "iterations 5028 accuracy : 0.931316950220542  loss : 0.3369154744055021\n",
      "iterations 5029 accuracy : 0.931316950220542  loss : 0.33690440487712614\n",
      "iterations 5030 accuracy : 0.931316950220542  loss : 0.3368921846431903\n",
      "iterations 5031 accuracy : 0.931316950220542  loss : 0.3368810420057615\n",
      "iterations 5032 accuracy : 0.931316950220542  loss : 0.33686933459316726\n",
      "iterations 5033 accuracy : 0.931316950220542  loss : 0.33685721622204673\n",
      "iterations 5034 accuracy : 0.931316950220542  loss : 0.33684515755971794\n",
      "iterations 5035 accuracy : 0.931316950220542  loss : 0.3368335164249612\n",
      "iterations 5036 accuracy : 0.931316950220542  loss : 0.3368215188159791\n",
      "iterations 5037 accuracy : 0.931316950220542  loss : 0.3368099814943961\n",
      "iterations 5038 accuracy : 0.9315269901281243  loss : 0.33679886439053036\n",
      "iterations 5039 accuracy : 0.9315269901281243  loss : 0.33678696291092297\n",
      "iterations 5040 accuracy : 0.9315269901281243  loss : 0.336774717433956\n",
      "iterations 5041 accuracy : 0.9315269901281243  loss : 0.33676339189613813\n",
      "iterations 5042 accuracy : 0.9317370300357067  loss : 0.33675130861481245\n",
      "iterations 5043 accuracy : 0.9315269901281243  loss : 0.3367401952353159\n",
      "iterations 5044 accuracy : 0.9315269901281243  loss : 0.33672916323996643\n",
      "iterations 5045 accuracy : 0.9315269901281243  loss : 0.33671818928134806\n",
      "iterations 5046 accuracy : 0.9315269901281243  loss : 0.33670686290607954\n",
      "iterations 5047 accuracy : 0.9315269901281243  loss : 0.33669578696036007\n",
      "iterations 5048 accuracy : 0.9315269901281243  loss : 0.33668392398833347\n",
      "iterations 5049 accuracy : 0.9315269901281243  loss : 0.33667275066742486\n",
      "iterations 5050 accuracy : 0.9315269901281243  loss : 0.33666094279993736\n",
      "iterations 5051 accuracy : 0.9315269901281243  loss : 0.3366494001043106\n",
      "iterations 5052 accuracy : 0.9317370300357067  loss : 0.3366377935944854\n",
      "iterations 5053 accuracy : 0.9319470699432892  loss : 0.33662576326899296\n",
      "iterations 5054 accuracy : 0.9319470699432892  loss : 0.3366146219567542\n",
      "iterations 5055 accuracy : 0.9319470699432892  loss : 0.33660389262469126\n",
      "iterations 5056 accuracy : 0.9319470699432892  loss : 0.3365918265330863\n",
      "iterations 5057 accuracy : 0.9319470699432892  loss : 0.33658025937759817\n",
      "iterations 5058 accuracy : 0.9319470699432892  loss : 0.3365686710464675\n",
      "iterations 5059 accuracy : 0.9319470699432892  loss : 0.3365573117488436\n",
      "iterations 5060 accuracy : 0.9319470699432892  loss : 0.33654625303500557\n",
      "iterations 5061 accuracy : 0.9319470699432892  loss : 0.33653487702877205\n",
      "iterations 5062 accuracy : 0.9319470699432892  loss : 0.3365239055579423\n",
      "iterations 5063 accuracy : 0.9319470699432892  loss : 0.3365128253499259\n",
      "iterations 5064 accuracy : 0.9319470699432892  loss : 0.33650057708208037\n",
      "iterations 5065 accuracy : 0.9319470699432892  loss : 0.33648997631213245\n",
      "iterations 5066 accuracy : 0.9319470699432892  loss : 0.33647900866180885\n",
      "iterations 5067 accuracy : 0.9319470699432892  loss : 0.33646765439698406\n",
      "iterations 5068 accuracy : 0.9319470699432892  loss : 0.3364561381870501\n",
      "iterations 5069 accuracy : 0.9321571098508716  loss : 0.33644437749133366\n",
      "iterations 5070 accuracy : 0.9319470699432892  loss : 0.3364341861119761\n",
      "iterations 5071 accuracy : 0.9319470699432892  loss : 0.3364220673475649\n",
      "iterations 5072 accuracy : 0.9321571098508716  loss : 0.33641050006445705\n",
      "iterations 5073 accuracy : 0.9321571098508716  loss : 0.33639903043803226\n",
      "iterations 5074 accuracy : 0.9321571098508716  loss : 0.33638752964229734\n",
      "iterations 5075 accuracy : 0.9321571098508716  loss : 0.33637622566241787\n",
      "iterations 5076 accuracy : 0.9321571098508716  loss : 0.3363655297045745\n",
      "iterations 5077 accuracy : 0.9319470699432892  loss : 0.33635422480329064\n",
      "iterations 5078 accuracy : 0.9321571098508716  loss : 0.3363427821855673\n",
      "iterations 5079 accuracy : 0.9319470699432892  loss : 0.33633184713377506\n",
      "iterations 5080 accuracy : 0.9321571098508716  loss : 0.33631988575727706\n",
      "iterations 5081 accuracy : 0.9321571098508716  loss : 0.336308345865575\n",
      "iterations 5082 accuracy : 0.9321571098508716  loss : 0.33629777555913626\n",
      "iterations 5083 accuracy : 0.9321571098508716  loss : 0.3362867386801852\n",
      "iterations 5084 accuracy : 0.9321571098508716  loss : 0.3362756307439259\n",
      "iterations 5085 accuracy : 0.9321571098508716  loss : 0.3362644839938345\n",
      "iterations 5086 accuracy : 0.9321571098508716  loss : 0.33625318525558784\n",
      "iterations 5087 accuracy : 0.9321571098508716  loss : 0.33624268108015104\n",
      "iterations 5088 accuracy : 0.9321571098508716  loss : 0.3362312996025282\n",
      "iterations 5089 accuracy : 0.9321571098508716  loss : 0.3362202479791938\n",
      "iterations 5090 accuracy : 0.9321571098508716  loss : 0.336207955421126\n",
      "iterations 5091 accuracy : 0.9321571098508716  loss : 0.3361960198361849\n",
      "iterations 5092 accuracy : 0.9321571098508716  loss : 0.3361854779917785\n",
      "iterations 5093 accuracy : 0.9321571098508716  loss : 0.33617458536380534\n",
      "iterations 5094 accuracy : 0.9321571098508716  loss : 0.33616361929199173\n",
      "iterations 5095 accuracy : 0.9319470699432892  loss : 0.33615309638191054\n",
      "iterations 5096 accuracy : 0.9321571098508716  loss : 0.3361419289979239\n",
      "iterations 5097 accuracy : 0.9321571098508716  loss : 0.33613049773231185\n",
      "iterations 5098 accuracy : 0.9321571098508716  loss : 0.3361182490125641\n",
      "iterations 5099 accuracy : 0.9321571098508716  loss : 0.33610729528813316\n",
      "iterations 5100 accuracy : 0.9321571098508716  loss : 0.33609642355076574\n",
      "iterations 5101 accuracy : 0.9321571098508716  loss : 0.3360847404701001\n",
      "iterations 5102 accuracy : 0.9321571098508716  loss : 0.3360736728683043\n",
      "iterations 5103 accuracy : 0.9321571098508716  loss : 0.33606295289794774\n",
      "iterations 5104 accuracy : 0.9321571098508716  loss : 0.33605186223442823\n",
      "iterations 5105 accuracy : 0.9321571098508716  loss : 0.33604054852587284\n",
      "iterations 5106 accuracy : 0.9321571098508716  loss : 0.33602914846856\n",
      "iterations 5107 accuracy : 0.9321571098508716  loss : 0.3360178588899616\n",
      "iterations 5108 accuracy : 0.9321571098508716  loss : 0.3360067371940354\n",
      "iterations 5109 accuracy : 0.9321571098508716  loss : 0.33599595421340533\n",
      "iterations 5110 accuracy : 0.9321571098508716  loss : 0.3359850240373906\n",
      "iterations 5111 accuracy : 0.9321571098508716  loss : 0.33597423827621087\n",
      "iterations 5112 accuracy : 0.9321571098508716  loss : 0.33596286876517606\n",
      "iterations 5113 accuracy : 0.9321571098508716  loss : 0.3359517628706106\n",
      "iterations 5114 accuracy : 0.9321571098508716  loss : 0.33594024820047647\n",
      "iterations 5115 accuracy : 0.9321571098508716  loss : 0.3359289857912704\n",
      "iterations 5116 accuracy : 0.9323671497584541  loss : 0.33591741542589676\n",
      "iterations 5117 accuracy : 0.9323671497584541  loss : 0.3359057353440423\n",
      "iterations 5118 accuracy : 0.9323671497584541  loss : 0.33589444258479756\n",
      "iterations 5119 accuracy : 0.9323671497584541  loss : 0.33588390004328933\n",
      "iterations 5120 accuracy : 0.9323671497584541  loss : 0.3358729926835593\n",
      "iterations 5121 accuracy : 0.9323671497584541  loss : 0.3358621731973401\n",
      "iterations 5122 accuracy : 0.9323671497584541  loss : 0.33585089945859536\n",
      "iterations 5123 accuracy : 0.9323671497584541  loss : 0.33583937903935196\n",
      "iterations 5124 accuracy : 0.9323671497584541  loss : 0.33582781189299843\n",
      "iterations 5125 accuracy : 0.9323671497584541  loss : 0.335817374582961\n",
      "iterations 5126 accuracy : 0.9323671497584541  loss : 0.33580619418416424\n",
      "iterations 5127 accuracy : 0.9323671497584541  loss : 0.3357948014800888\n",
      "iterations 5128 accuracy : 0.9323671497584541  loss : 0.33578356703070644\n",
      "iterations 5129 accuracy : 0.9323671497584541  loss : 0.3357729572764611\n",
      "iterations 5130 accuracy : 0.9323671497584541  loss : 0.33576144840216193\n",
      "iterations 5131 accuracy : 0.9323671497584541  loss : 0.3357502519916758\n",
      "iterations 5132 accuracy : 0.9323671497584541  loss : 0.33573861309031056\n",
      "iterations 5133 accuracy : 0.9323671497584541  loss : 0.33572714180153124\n",
      "iterations 5134 accuracy : 0.9323671497584541  loss : 0.3357154258887536\n",
      "iterations 5135 accuracy : 0.9323671497584541  loss : 0.33570415974110634\n",
      "iterations 5136 accuracy : 0.9323671497584541  loss : 0.33569263977122754\n",
      "iterations 5137 accuracy : 0.9323671497584541  loss : 0.3356814670658922\n",
      "iterations 5138 accuracy : 0.9323671497584541  loss : 0.33566985918787695\n",
      "iterations 5139 accuracy : 0.9323671497584541  loss : 0.33565863690395487\n",
      "iterations 5140 accuracy : 0.9323671497584541  loss : 0.33564713376154937\n",
      "iterations 5141 accuracy : 0.9323671497584541  loss : 0.33563610456568005\n",
      "iterations 5142 accuracy : 0.9323671497584541  loss : 0.3356250375755638\n",
      "iterations 5143 accuracy : 0.9323671497584541  loss : 0.335613219139443\n",
      "iterations 5144 accuracy : 0.9323671497584541  loss : 0.33560244800407174\n",
      "iterations 5145 accuracy : 0.9323671497584541  loss : 0.335591026426591\n",
      "iterations 5146 accuracy : 0.9323671497584541  loss : 0.3355796622116077\n",
      "iterations 5147 accuracy : 0.9323671497584541  loss : 0.33556878785563743\n",
      "iterations 5148 accuracy : 0.9323671497584541  loss : 0.33555812450381817\n",
      "iterations 5149 accuracy : 0.9323671497584541  loss : 0.33554722722078856\n",
      "iterations 5150 accuracy : 0.9323671497584541  loss : 0.335536027650327\n",
      "iterations 5151 accuracy : 0.9323671497584541  loss : 0.33552441636319763\n",
      "iterations 5152 accuracy : 0.9325771896660365  loss : 0.3355128911427554\n",
      "iterations 5153 accuracy : 0.9325771896660365  loss : 0.335501849381308\n",
      "iterations 5154 accuracy : 0.9325771896660365  loss : 0.33548967711640076\n",
      "iterations 5155 accuracy : 0.9325771896660365  loss : 0.33547822440972763\n",
      "iterations 5156 accuracy : 0.9325771896660365  loss : 0.3354672039485012\n",
      "iterations 5157 accuracy : 0.9325771896660365  loss : 0.3354563858597236\n",
      "iterations 5158 accuracy : 0.9325771896660365  loss : 0.33544539312563226\n",
      "iterations 5159 accuracy : 0.9325771896660365  loss : 0.335434533140718\n",
      "iterations 5160 accuracy : 0.9325771896660365  loss : 0.33542312094623056\n",
      "iterations 5161 accuracy : 0.9325771896660365  loss : 0.33541219908072456\n",
      "iterations 5162 accuracy : 0.9325771896660365  loss : 0.335401046883988\n",
      "iterations 5163 accuracy : 0.9325771896660365  loss : 0.3353905109858034\n",
      "iterations 5164 accuracy : 0.9325771896660365  loss : 0.33537980612305396\n",
      "iterations 5165 accuracy : 0.9325771896660365  loss : 0.3353690141097813\n",
      "iterations 5166 accuracy : 0.9325771896660365  loss : 0.3353586244725813\n",
      "iterations 5167 accuracy : 0.9325771896660365  loss : 0.3353475890397759\n",
      "iterations 5168 accuracy : 0.9325771896660365  loss : 0.3353370927655545\n",
      "iterations 5169 accuracy : 0.9323671497584541  loss : 0.33532640449645007\n",
      "iterations 5170 accuracy : 0.9323671497584541  loss : 0.33531554029652466\n",
      "iterations 5171 accuracy : 0.9323671497584541  loss : 0.33530463801691784\n",
      "iterations 5172 accuracy : 0.9323671497584541  loss : 0.33529388423379647\n",
      "iterations 5173 accuracy : 0.9323671497584541  loss : 0.3352829269320719\n",
      "iterations 5174 accuracy : 0.9323671497584541  loss : 0.3352721934669013\n",
      "iterations 5175 accuracy : 0.9323671497584541  loss : 0.33526085388229654\n",
      "iterations 5176 accuracy : 0.9323671497584541  loss : 0.3352497917534017\n",
      "iterations 5177 accuracy : 0.9323671497584541  loss : 0.33523908414538245\n",
      "iterations 5178 accuracy : 0.9325771896660365  loss : 0.3352274924516532\n",
      "iterations 5179 accuracy : 0.9325771896660365  loss : 0.3352163059157379\n",
      "iterations 5180 accuracy : 0.9325771896660365  loss : 0.33520527882892415\n",
      "iterations 5181 accuracy : 0.9325771896660365  loss : 0.33519405700016247\n",
      "iterations 5182 accuracy : 0.9325771896660365  loss : 0.33518374782453686\n",
      "iterations 5183 accuracy : 0.9325771896660365  loss : 0.3351723956237976\n",
      "iterations 5184 accuracy : 0.9325771896660365  loss : 0.3351610337106736\n",
      "iterations 5185 accuracy : 0.9325771896660365  loss : 0.33515090798946656\n",
      "iterations 5186 accuracy : 0.9325771896660365  loss : 0.3351395507288537\n",
      "iterations 5187 accuracy : 0.9325771896660365  loss : 0.3351285495222013\n",
      "iterations 5188 accuracy : 0.9325771896660365  loss : 0.3351173997007757\n",
      "iterations 5189 accuracy : 0.9325771896660365  loss : 0.33510634619292023\n",
      "iterations 5190 accuracy : 0.9323671497584541  loss : 0.3350952666957283\n",
      "iterations 5191 accuracy : 0.9325771896660365  loss : 0.33508451080196366\n",
      "iterations 5192 accuracy : 0.9323671497584541  loss : 0.3350733906312062\n",
      "iterations 5193 accuracy : 0.9325771896660365  loss : 0.3350624371997245\n",
      "iterations 5194 accuracy : 0.9321571098508716  loss : 0.3350510479168517\n",
      "iterations 5195 accuracy : 0.9321571098508716  loss : 0.3350398952062599\n",
      "iterations 5196 accuracy : 0.9321571098508716  loss : 0.33502871529707023\n",
      "iterations 5197 accuracy : 0.9321571098508716  loss : 0.3350182006345089\n",
      "iterations 5198 accuracy : 0.9319470699432892  loss : 0.3350070076095516\n",
      "iterations 5199 accuracy : 0.9319470699432892  loss : 0.3349958801629387\n",
      "iterations 5200 accuracy : 0.9319470699432892  loss : 0.3349847849874437\n",
      "iterations 5201 accuracy : 0.9319470699432892  loss : 0.33497350003605403\n",
      "iterations 5202 accuracy : 0.9321571098508716  loss : 0.33496300133297463\n",
      "iterations 5203 accuracy : 0.9321571098508716  loss : 0.33495219581164937\n",
      "iterations 5204 accuracy : 0.9323671497584541  loss : 0.33494164028307244\n",
      "iterations 5205 accuracy : 0.9319470699432892  loss : 0.3349300804158022\n",
      "iterations 5206 accuracy : 0.9321571098508716  loss : 0.33491965846934285\n",
      "iterations 5207 accuracy : 0.9325771896660365  loss : 0.3349092796082953\n",
      "iterations 5208 accuracy : 0.9325771896660365  loss : 0.33489880211511125\n",
      "iterations 5209 accuracy : 0.9325771896660365  loss : 0.33488826658498555\n",
      "iterations 5210 accuracy : 0.9325771896660365  loss : 0.3348779208655984\n",
      "iterations 5211 accuracy : 0.9325771896660365  loss : 0.33486732104912714\n",
      "iterations 5212 accuracy : 0.9325771896660365  loss : 0.33485643191189673\n",
      "iterations 5213 accuracy : 0.9325771896660365  loss : 0.3348448057200533\n",
      "iterations 5214 accuracy : 0.9325771896660365  loss : 0.3348338528614449\n",
      "iterations 5215 accuracy : 0.9325771896660365  loss : 0.3348233119439821\n",
      "iterations 5216 accuracy : 0.9325771896660365  loss : 0.334812790888502\n",
      "iterations 5217 accuracy : 0.9325771896660365  loss : 0.33480232789004266\n",
      "iterations 5218 accuracy : 0.9325771896660365  loss : 0.33479106078764853\n",
      "iterations 5219 accuracy : 0.9325771896660365  loss : 0.3347796561400791\n",
      "iterations 5220 accuracy : 0.9325771896660365  loss : 0.3347686723449436\n",
      "iterations 5221 accuracy : 0.9325771896660365  loss : 0.33475752590019126\n",
      "iterations 5222 accuracy : 0.9325771896660365  loss : 0.33474655762178157\n",
      "iterations 5223 accuracy : 0.9323671497584541  loss : 0.3347357424274084\n",
      "iterations 5224 accuracy : 0.9323671497584541  loss : 0.33472455324151457\n",
      "iterations 5225 accuracy : 0.9325771896660365  loss : 0.33471441824908965\n",
      "iterations 5226 accuracy : 0.9325771896660365  loss : 0.3347036636404082\n",
      "iterations 5227 accuracy : 0.9325771896660365  loss : 0.3346928493476759\n",
      "iterations 5228 accuracy : 0.9325771896660365  loss : 0.33468177812220146\n",
      "iterations 5229 accuracy : 0.9325771896660365  loss : 0.33467148766766036\n",
      "iterations 5230 accuracy : 0.9325771896660365  loss : 0.33466031457796624\n",
      "iterations 5231 accuracy : 0.9325771896660365  loss : 0.3346496922195112\n",
      "iterations 5232 accuracy : 0.9325771896660365  loss : 0.3346394167536074\n",
      "iterations 5233 accuracy : 0.9325771896660365  loss : 0.3346286309928294\n",
      "iterations 5234 accuracy : 0.9325771896660365  loss : 0.33461793419912916\n",
      "iterations 5235 accuracy : 0.9325771896660365  loss : 0.3346072454986643\n",
      "iterations 5236 accuracy : 0.9325771896660365  loss : 0.33459694653062055\n",
      "iterations 5237 accuracy : 0.9325771896660365  loss : 0.33458623764779793\n",
      "iterations 5238 accuracy : 0.9325771896660365  loss : 0.3345755441046321\n",
      "iterations 5239 accuracy : 0.9325771896660365  loss : 0.33456421887418886\n",
      "iterations 5240 accuracy : 0.9323671497584541  loss : 0.33455310930837656\n",
      "iterations 5241 accuracy : 0.9321571098508716  loss : 0.334541283554072\n",
      "iterations 5242 accuracy : 0.9319470699432892  loss : 0.3345302917911023\n",
      "iterations 5243 accuracy : 0.9317370300357067  loss : 0.3345195100788712\n",
      "iterations 5244 accuracy : 0.9317370300357067  loss : 0.33450842767917816\n",
      "iterations 5245 accuracy : 0.931316950220542  loss : 0.33449702213911264\n",
      "iterations 5246 accuracy : 0.931316950220542  loss : 0.3344871339736544\n",
      "iterations 5247 accuracy : 0.9315269901281243  loss : 0.33447650452584715\n",
      "iterations 5248 accuracy : 0.9315269901281243  loss : 0.33446575823045455\n",
      "iterations 5249 accuracy : 0.931316950220542  loss : 0.33445466392764284\n",
      "iterations 5250 accuracy : 0.931316950220542  loss : 0.33444408513490986\n",
      "iterations 5251 accuracy : 0.931316950220542  loss : 0.33443325920321604\n",
      "iterations 5252 accuracy : 0.931316950220542  loss : 0.33442201779486247\n",
      "iterations 5253 accuracy : 0.931316950220542  loss : 0.3344113347768341\n",
      "iterations 5254 accuracy : 0.931316950220542  loss : 0.334400813789598\n",
      "iterations 5255 accuracy : 0.931316950220542  loss : 0.3343903656631731\n",
      "iterations 5256 accuracy : 0.931316950220542  loss : 0.3343797468857646\n",
      "iterations 5257 accuracy : 0.931316950220542  loss : 0.33436901432656846\n",
      "iterations 5258 accuracy : 0.931316950220542  loss : 0.3343584532200441\n",
      "iterations 5259 accuracy : 0.931316950220542  loss : 0.33434737634689166\n",
      "iterations 5260 accuracy : 0.931316950220542  loss : 0.33433687843552146\n",
      "iterations 5261 accuracy : 0.931316950220542  loss : 0.3343259388855674\n",
      "iterations 5262 accuracy : 0.931316950220542  loss : 0.3343147848783718\n",
      "iterations 5263 accuracy : 0.931316950220542  loss : 0.33430428669263995\n",
      "iterations 5264 accuracy : 0.931316950220542  loss : 0.33429372005167013\n",
      "iterations 5265 accuracy : 0.931316950220542  loss : 0.33428336560195826\n",
      "iterations 5266 accuracy : 0.931316950220542  loss : 0.33427293661366386\n",
      "iterations 5267 accuracy : 0.931316950220542  loss : 0.33426161363581\n",
      "iterations 5268 accuracy : 0.931316950220542  loss : 0.3342511204603548\n",
      "iterations 5269 accuracy : 0.931316950220542  loss : 0.334240458777943\n",
      "iterations 5270 accuracy : 0.931316950220542  loss : 0.33422952187759575\n",
      "iterations 5271 accuracy : 0.931316950220542  loss : 0.33421930569484604\n",
      "iterations 5272 accuracy : 0.931316950220542  loss : 0.33420937781845006\n",
      "iterations 5273 accuracy : 0.931316950220542  loss : 0.3341991189917588\n",
      "iterations 5274 accuracy : 0.931316950220542  loss : 0.33418850417415424\n",
      "iterations 5275 accuracy : 0.931316950220542  loss : 0.3341774284518618\n",
      "iterations 5276 accuracy : 0.9315269901281243  loss : 0.33416637088046974\n",
      "iterations 5277 accuracy : 0.9315269901281243  loss : 0.33415541203536414\n",
      "iterations 5278 accuracy : 0.9315269901281243  loss : 0.3341448634918308\n",
      "iterations 5279 accuracy : 0.9315269901281243  loss : 0.3341345357585772\n",
      "iterations 5280 accuracy : 0.931316950220542  loss : 0.3341234848282598\n",
      "iterations 5281 accuracy : 0.931316950220542  loss : 0.3341124542493991\n",
      "iterations 5282 accuracy : 0.931316950220542  loss : 0.3341020537630978\n",
      "iterations 5283 accuracy : 0.931316950220542  loss : 0.33409142925371355\n",
      "iterations 5284 accuracy : 0.931316950220542  loss : 0.3340809119472225\n",
      "iterations 5285 accuracy : 0.931316950220542  loss : 0.3340699843553657\n",
      "iterations 5286 accuracy : 0.931316950220542  loss : 0.33405924035610535\n",
      "iterations 5287 accuracy : 0.931316950220542  loss : 0.3340483540552175\n",
      "iterations 5288 accuracy : 0.931316950220542  loss : 0.334037417349092\n",
      "iterations 5289 accuracy : 0.931316950220542  loss : 0.33402610309559866\n",
      "iterations 5290 accuracy : 0.931316950220542  loss : 0.33401551242068866\n",
      "iterations 5291 accuracy : 0.931316950220542  loss : 0.3340050763851887\n",
      "iterations 5292 accuracy : 0.9311069103129594  loss : 0.3339943422475986\n",
      "iterations 5293 accuracy : 0.9311069103129594  loss : 0.3339836385228295\n",
      "iterations 5294 accuracy : 0.9311069103129594  loss : 0.33397257972170347\n",
      "iterations 5295 accuracy : 0.9311069103129594  loss : 0.33396285490981265\n",
      "iterations 5296 accuracy : 0.9311069103129594  loss : 0.33395228978436664\n",
      "iterations 5297 accuracy : 0.9311069103129594  loss : 0.33394110039707725\n",
      "iterations 5298 accuracy : 0.9311069103129594  loss : 0.333930404763308\n",
      "iterations 5299 accuracy : 0.9311069103129594  loss : 0.333919659853234\n",
      "iterations 5300 accuracy : 0.9311069103129594  loss : 0.3339092213284841\n",
      "iterations 5301 accuracy : 0.9311069103129594  loss : 0.33389797006391075\n",
      "iterations 5302 accuracy : 0.9311069103129594  loss : 0.3338881081050406\n",
      "iterations 5303 accuracy : 0.9311069103129594  loss : 0.33387729689986373\n",
      "iterations 5304 accuracy : 0.9311069103129594  loss : 0.3338668357207448\n",
      "iterations 5305 accuracy : 0.9311069103129594  loss : 0.33385608085606483\n",
      "iterations 5306 accuracy : 0.9311069103129594  loss : 0.33384559180799056\n",
      "iterations 5307 accuracy : 0.931316950220542  loss : 0.33383488768856295\n",
      "iterations 5308 accuracy : 0.931316950220542  loss : 0.3338243591703345\n",
      "iterations 5309 accuracy : 0.931316950220542  loss : 0.33381341739182163\n",
      "iterations 5310 accuracy : 0.931316950220542  loss : 0.33380311613672226\n",
      "iterations 5311 accuracy : 0.931316950220542  loss : 0.33379257340340185\n",
      "iterations 5312 accuracy : 0.931316950220542  loss : 0.3337819973648192\n",
      "iterations 5313 accuracy : 0.931316950220542  loss : 0.33377188735297575\n",
      "iterations 5314 accuracy : 0.931316950220542  loss : 0.3337608702001674\n",
      "iterations 5315 accuracy : 0.931316950220542  loss : 0.33375086852237285\n",
      "iterations 5316 accuracy : 0.931316950220542  loss : 0.333740339850739\n",
      "iterations 5317 accuracy : 0.931316950220542  loss : 0.333729736163974\n",
      "iterations 5318 accuracy : 0.931316950220542  loss : 0.3337188611637285\n",
      "iterations 5319 accuracy : 0.931316950220542  loss : 0.333708176147938\n",
      "iterations 5320 accuracy : 0.931316950220542  loss : 0.3336973921659777\n",
      "iterations 5321 accuracy : 0.931316950220542  loss : 0.3336868715583091\n",
      "iterations 5322 accuracy : 0.931316950220542  loss : 0.3336768407175841\n",
      "iterations 5323 accuracy : 0.931316950220542  loss : 0.33366590749181463\n",
      "iterations 5324 accuracy : 0.931316950220542  loss : 0.3336556668328804\n",
      "iterations 5325 accuracy : 0.931316950220542  loss : 0.3336448245672868\n",
      "iterations 5326 accuracy : 0.9311069103129594  loss : 0.33363354212798024\n",
      "iterations 5327 accuracy : 0.931316950220542  loss : 0.33362320411632435\n",
      "iterations 5328 accuracy : 0.9311069103129594  loss : 0.33361259098450885\n",
      "iterations 5329 accuracy : 0.9311069103129594  loss : 0.3336021169587301\n",
      "iterations 5330 accuracy : 0.9311069103129594  loss : 0.33359147319276683\n",
      "iterations 5331 accuracy : 0.9311069103129594  loss : 0.33358088393618124\n",
      "iterations 5332 accuracy : 0.931316950220542  loss : 0.33357053116352897\n",
      "iterations 5333 accuracy : 0.9311069103129594  loss : 0.33356020795534436\n",
      "iterations 5334 accuracy : 0.9311069103129594  loss : 0.33354959301575127\n",
      "iterations 5335 accuracy : 0.9311069103129594  loss : 0.3335392205016029\n",
      "iterations 5336 accuracy : 0.9311069103129594  loss : 0.3335286193970446\n",
      "iterations 5337 accuracy : 0.9311069103129594  loss : 0.33351743650804133\n",
      "iterations 5338 accuracy : 0.9311069103129594  loss : 0.3335074799727812\n",
      "iterations 5339 accuracy : 0.9311069103129594  loss : 0.33349701488243355\n",
      "iterations 5340 accuracy : 0.9311069103129594  loss : 0.33348696453499305\n",
      "iterations 5341 accuracy : 0.931316950220542  loss : 0.33347705512856307\n",
      "iterations 5342 accuracy : 0.931316950220542  loss : 0.3334665117019424\n",
      "iterations 5343 accuracy : 0.931316950220542  loss : 0.33345611289786703\n",
      "iterations 5344 accuracy : 0.931316950220542  loss : 0.333446546326369\n",
      "iterations 5345 accuracy : 0.931316950220542  loss : 0.333435728121219\n",
      "iterations 5346 accuracy : 0.9311069103129594  loss : 0.33342517719309056\n",
      "iterations 5347 accuracy : 0.9311069103129594  loss : 0.3334148165855195\n",
      "iterations 5348 accuracy : 0.931316950220542  loss : 0.33340449644216263\n",
      "iterations 5349 accuracy : 0.9311069103129594  loss : 0.3333942249946238\n",
      "iterations 5350 accuracy : 0.931316950220542  loss : 0.33338424416237633\n",
      "iterations 5351 accuracy : 0.931316950220542  loss : 0.33337415468767967\n",
      "iterations 5352 accuracy : 0.931316950220542  loss : 0.33336329149550536\n",
      "iterations 5353 accuracy : 0.931316950220542  loss : 0.3333524891878913\n",
      "iterations 5354 accuracy : 0.931316950220542  loss : 0.3333426151726392\n",
      "iterations 5355 accuracy : 0.931316950220542  loss : 0.33333229531207675\n",
      "iterations 5356 accuracy : 0.931316950220542  loss : 0.3333223039710019\n",
      "iterations 5357 accuracy : 0.931316950220542  loss : 0.3333118715542541\n",
      "iterations 5358 accuracy : 0.931316950220542  loss : 0.333302262883533\n",
      "iterations 5359 accuracy : 0.931316950220542  loss : 0.33329246188547157\n",
      "iterations 5360 accuracy : 0.9311069103129594  loss : 0.33328241498620603\n",
      "iterations 5361 accuracy : 0.9311069103129594  loss : 0.3332719995919665\n",
      "iterations 5362 accuracy : 0.9311069103129594  loss : 0.33326153518768675\n",
      "iterations 5363 accuracy : 0.9311069103129594  loss : 0.33325140245399176\n",
      "iterations 5364 accuracy : 0.9311069103129594  loss : 0.3332409803145472\n",
      "iterations 5365 accuracy : 0.9311069103129594  loss : 0.3332308186244453\n",
      "iterations 5366 accuracy : 0.9311069103129594  loss : 0.33322075796101597\n",
      "iterations 5367 accuracy : 0.9311069103129594  loss : 0.3332102339431898\n",
      "iterations 5368 accuracy : 0.9311069103129594  loss : 0.33319990436160657\n",
      "iterations 5369 accuracy : 0.9311069103129594  loss : 0.33318949954353655\n",
      "iterations 5370 accuracy : 0.931316950220542  loss : 0.33317868290002667\n",
      "iterations 5371 accuracy : 0.9311069103129594  loss : 0.3331686119069537\n",
      "iterations 5372 accuracy : 0.931316950220542  loss : 0.3331578422890009\n",
      "iterations 5373 accuracy : 0.9311069103129594  loss : 0.3331480626038389\n",
      "iterations 5374 accuracy : 0.931316950220542  loss : 0.33313747937534743\n",
      "iterations 5375 accuracy : 0.9311069103129594  loss : 0.3331275274199537\n",
      "iterations 5376 accuracy : 0.9311069103129594  loss : 0.3331175352909648\n",
      "iterations 5377 accuracy : 0.9311069103129594  loss : 0.33310755203846987\n",
      "iterations 5378 accuracy : 0.9311069103129594  loss : 0.3330971836415214\n",
      "iterations 5379 accuracy : 0.9311069103129594  loss : 0.33308680901598164\n",
      "iterations 5380 accuracy : 0.9311069103129594  loss : 0.3330763371745709\n",
      "iterations 5381 accuracy : 0.931316950220542  loss : 0.333065928231424\n",
      "iterations 5382 accuracy : 0.931316950220542  loss : 0.3330554100295473\n",
      "iterations 5383 accuracy : 0.931316950220542  loss : 0.33304495204220796\n",
      "iterations 5384 accuracy : 0.931316950220542  loss : 0.33303430986730426\n",
      "iterations 5385 accuracy : 0.931316950220542  loss : 0.33302412197365444\n",
      "iterations 5386 accuracy : 0.931316950220542  loss : 0.3330138540775092\n",
      "iterations 5387 accuracy : 0.9311069103129594  loss : 0.33300296386842565\n",
      "iterations 5388 accuracy : 0.9311069103129594  loss : 0.33299282080099335\n",
      "iterations 5389 accuracy : 0.9311069103129594  loss : 0.3329824430880774\n",
      "iterations 5390 accuracy : 0.931316950220542  loss : 0.3329720795168091\n",
      "iterations 5391 accuracy : 0.9311069103129594  loss : 0.3329612331630532\n",
      "iterations 5392 accuracy : 0.931316950220542  loss : 0.33295124247817587\n",
      "iterations 5393 accuracy : 0.931316950220542  loss : 0.33294153409529803\n",
      "iterations 5394 accuracy : 0.931316950220542  loss : 0.3329312912821708\n",
      "iterations 5395 accuracy : 0.931316950220542  loss : 0.3329210296425413\n",
      "iterations 5396 accuracy : 0.931316950220542  loss : 0.3329109425761965\n",
      "iterations 5397 accuracy : 0.9311069103129594  loss : 0.3329003827464326\n",
      "iterations 5398 accuracy : 0.931316950220542  loss : 0.33289079517175807\n",
      "iterations 5399 accuracy : 0.931316950220542  loss : 0.3328806982155661\n",
      "iterations 5400 accuracy : 0.9311069103129594  loss : 0.33287110345696064\n",
      "iterations 5401 accuracy : 0.9311069103129594  loss : 0.332860561619152\n",
      "iterations 5402 accuracy : 0.931316950220542  loss : 0.3328502435645742\n",
      "iterations 5403 accuracy : 0.931316950220542  loss : 0.3328399574365486\n",
      "iterations 5404 accuracy : 0.9311069103129594  loss : 0.3328303201764553\n",
      "iterations 5405 accuracy : 0.9311069103129594  loss : 0.332820338113149\n",
      "iterations 5406 accuracy : 0.9311069103129594  loss : 0.33281006712817535\n",
      "iterations 5407 accuracy : 0.9311069103129594  loss : 0.33280005690899594\n",
      "iterations 5408 accuracy : 0.9311069103129594  loss : 0.33278995778943987\n",
      "iterations 5409 accuracy : 0.9311069103129594  loss : 0.33277980952658603\n",
      "iterations 5410 accuracy : 0.9311069103129594  loss : 0.3327696018263882\n",
      "iterations 5411 accuracy : 0.9311069103129594  loss : 0.3327587541125395\n",
      "iterations 5412 accuracy : 0.9311069103129594  loss : 0.332748596809779\n",
      "iterations 5413 accuracy : 0.931316950220542  loss : 0.3327380513439813\n",
      "iterations 5414 accuracy : 0.931316950220542  loss : 0.33272738134574137\n",
      "iterations 5415 accuracy : 0.931316950220542  loss : 0.33271699574001995\n",
      "iterations 5416 accuracy : 0.931316950220542  loss : 0.3327068482168883\n",
      "iterations 5417 accuracy : 0.931316950220542  loss : 0.33269665628365047\n",
      "iterations 5418 accuracy : 0.931316950220542  loss : 0.3326864551451478\n",
      "iterations 5419 accuracy : 0.931316950220542  loss : 0.33267670539536875\n",
      "iterations 5420 accuracy : 0.931316950220542  loss : 0.33266626727875936\n",
      "iterations 5421 accuracy : 0.931316950220542  loss : 0.3326559596932924\n",
      "iterations 5422 accuracy : 0.931316950220542  loss : 0.3326462221092019\n",
      "iterations 5423 accuracy : 0.931316950220542  loss : 0.33263574982735844\n",
      "iterations 5424 accuracy : 0.931316950220542  loss : 0.3326262950619181\n",
      "iterations 5425 accuracy : 0.931316950220542  loss : 0.3326159899423362\n",
      "iterations 5426 accuracy : 0.931316950220542  loss : 0.33260553545609095\n",
      "iterations 5427 accuracy : 0.931316950220542  loss : 0.3325955264461075\n",
      "iterations 5428 accuracy : 0.931316950220542  loss : 0.332585052289269\n",
      "iterations 5429 accuracy : 0.931316950220542  loss : 0.3325750912019662\n",
      "iterations 5430 accuracy : 0.9311069103129594  loss : 0.3325660562696153\n",
      "iterations 5431 accuracy : 0.9311069103129594  loss : 0.33255588305497186\n",
      "iterations 5432 accuracy : 0.931316950220542  loss : 0.3325461000923839\n",
      "iterations 5433 accuracy : 0.931316950220542  loss : 0.3325353688369143\n",
      "iterations 5434 accuracy : 0.931316950220542  loss : 0.3325253397874731\n",
      "iterations 5435 accuracy : 0.9311069103129594  loss : 0.3325148752775401\n",
      "iterations 5436 accuracy : 0.9311069103129594  loss : 0.3325044553373644\n",
      "iterations 5437 accuracy : 0.9311069103129594  loss : 0.33249461228538224\n",
      "iterations 5438 accuracy : 0.9311069103129594  loss : 0.3324848876164379\n",
      "iterations 5439 accuracy : 0.9311069103129594  loss : 0.3324747692147851\n",
      "iterations 5440 accuracy : 0.9311069103129594  loss : 0.3324650730544477\n",
      "iterations 5441 accuracy : 0.931316950220542  loss : 0.3324560531827871\n",
      "iterations 5442 accuracy : 0.931316950220542  loss : 0.33244618403309767\n",
      "iterations 5443 accuracy : 0.931316950220542  loss : 0.3324355869492809\n",
      "iterations 5444 accuracy : 0.931316950220542  loss : 0.33242551384783114\n",
      "iterations 5445 accuracy : 0.9311069103129594  loss : 0.33241523360015035\n",
      "iterations 5446 accuracy : 0.9311069103129594  loss : 0.3324051619158611\n",
      "iterations 5447 accuracy : 0.931316950220542  loss : 0.33239548255982554\n",
      "iterations 5448 accuracy : 0.931316950220542  loss : 0.33238520466508653\n",
      "iterations 5449 accuracy : 0.931316950220542  loss : 0.3323751431890994\n",
      "iterations 5450 accuracy : 0.931316950220542  loss : 0.33236462134396644\n",
      "iterations 5451 accuracy : 0.9311069103129594  loss : 0.33235443175546725\n",
      "iterations 5452 accuracy : 0.931316950220542  loss : 0.3323449270387047\n",
      "iterations 5453 accuracy : 0.9311069103129594  loss : 0.3323344908240393\n",
      "iterations 5454 accuracy : 0.931316950220542  loss : 0.3323244386193067\n",
      "iterations 5455 accuracy : 0.931316950220542  loss : 0.33231467611512194\n",
      "iterations 5456 accuracy : 0.931316950220542  loss : 0.33230463036864166\n",
      "iterations 5457 accuracy : 0.9311069103129594  loss : 0.3322938948245147\n",
      "iterations 5458 accuracy : 0.9311069103129594  loss : 0.3322839427334862\n",
      "iterations 5459 accuracy : 0.9311069103129594  loss : 0.3322733957023989\n",
      "iterations 5460 accuracy : 0.930896870405377  loss : 0.33226272917015837\n",
      "iterations 5461 accuracy : 0.930896870405377  loss : 0.33225297427398376\n",
      "iterations 5462 accuracy : 0.930896870405377  loss : 0.33224290662459555\n",
      "iterations 5463 accuracy : 0.930896870405377  loss : 0.3322327139506158\n",
      "iterations 5464 accuracy : 0.930896870405377  loss : 0.33222257152075596\n",
      "iterations 5465 accuracy : 0.930896870405377  loss : 0.3322132178960876\n",
      "iterations 5466 accuracy : 0.9311069103129594  loss : 0.33220391080504363\n",
      "iterations 5467 accuracy : 0.9311069103129594  loss : 0.3321937829078613\n",
      "iterations 5468 accuracy : 0.9311069103129594  loss : 0.3321836362859716\n",
      "iterations 5469 accuracy : 0.9311069103129594  loss : 0.33217346361325484\n",
      "iterations 5470 accuracy : 0.931316950220542  loss : 0.3321640130323049\n",
      "iterations 5471 accuracy : 0.931316950220542  loss : 0.33215447312992774\n",
      "iterations 5472 accuracy : 0.9311069103129594  loss : 0.33214421608673933\n",
      "iterations 5473 accuracy : 0.9311069103129594  loss : 0.33213446970725796\n",
      "iterations 5474 accuracy : 0.931316950220542  loss : 0.33212499347599517\n",
      "iterations 5475 accuracy : 0.931316950220542  loss : 0.3321143220720642\n",
      "iterations 5476 accuracy : 0.931316950220542  loss : 0.3321047715199042\n",
      "iterations 5477 accuracy : 0.931316950220542  loss : 0.33209502456568585\n",
      "iterations 5478 accuracy : 0.9311069103129594  loss : 0.332085494488282\n",
      "iterations 5479 accuracy : 0.931316950220542  loss : 0.33207522186200394\n",
      "iterations 5480 accuracy : 0.931316950220542  loss : 0.332065055689988\n",
      "iterations 5481 accuracy : 0.931316950220542  loss : 0.33205509465724486\n",
      "iterations 5482 accuracy : 0.931316950220542  loss : 0.3320447333510554\n",
      "iterations 5483 accuracy : 0.931316950220542  loss : 0.3320348795475747\n",
      "iterations 5484 accuracy : 0.931316950220542  loss : 0.3320246865509484\n",
      "iterations 5485 accuracy : 0.931316950220542  loss : 0.3320144941213564\n",
      "iterations 5486 accuracy : 0.931316950220542  loss : 0.3320044142901068\n",
      "iterations 5487 accuracy : 0.931316950220542  loss : 0.33199418126301716\n",
      "iterations 5488 accuracy : 0.931316950220542  loss : 0.3319841840220608\n",
      "iterations 5489 accuracy : 0.931316950220542  loss : 0.33197485272309263\n",
      "iterations 5490 accuracy : 0.931316950220542  loss : 0.33196497179748674\n",
      "iterations 5491 accuracy : 0.931316950220542  loss : 0.33195425155957503\n",
      "iterations 5492 accuracy : 0.9311069103129594  loss : 0.3319444994796302\n",
      "iterations 5493 accuracy : 0.931316950220542  loss : 0.33193569025471503\n",
      "iterations 5494 accuracy : 0.931316950220542  loss : 0.33192520730342306\n",
      "iterations 5495 accuracy : 0.931316950220542  loss : 0.3319150990783812\n",
      "iterations 5496 accuracy : 0.931316950220542  loss : 0.33190513358477985\n",
      "iterations 5497 accuracy : 0.931316950220542  loss : 0.3318951464002647\n",
      "iterations 5498 accuracy : 0.931316950220542  loss : 0.3318849012366422\n",
      "iterations 5499 accuracy : 0.931316950220542  loss : 0.33187519630717394\n",
      "iterations 5500 accuracy : 0.931316950220542  loss : 0.33186538919220926\n",
      "iterations 5501 accuracy : 0.931316950220542  loss : 0.3318552896497048\n",
      "iterations 5502 accuracy : 0.930896870405377  loss : 0.3318443240174397\n",
      "iterations 5503 accuracy : 0.930896870405377  loss : 0.33183426846553943\n",
      "iterations 5504 accuracy : 0.930896870405377  loss : 0.33182521712828195\n",
      "iterations 5505 accuracy : 0.9311069103129594  loss : 0.3318159082919332\n",
      "iterations 5506 accuracy : 0.9311069103129594  loss : 0.3318060492125763\n",
      "iterations 5507 accuracy : 0.9311069103129594  loss : 0.3317962715370262\n",
      "iterations 5508 accuracy : 0.9311069103129594  loss : 0.3317865954830084\n",
      "iterations 5509 accuracy : 0.9311069103129594  loss : 0.3317764688502653\n",
      "iterations 5510 accuracy : 0.931316950220542  loss : 0.33176729297809454\n",
      "iterations 5511 accuracy : 0.931316950220542  loss : 0.3317573383084595\n",
      "iterations 5512 accuracy : 0.931316950220542  loss : 0.33174767144209416\n",
      "iterations 5513 accuracy : 0.931316950220542  loss : 0.33173755048234627\n",
      "iterations 5514 accuracy : 0.931316950220542  loss : 0.33172735802714776\n",
      "iterations 5515 accuracy : 0.931316950220542  loss : 0.3317174369443822\n",
      "iterations 5516 accuracy : 0.930896870405377  loss : 0.3317073816702866\n",
      "iterations 5517 accuracy : 0.9311069103129594  loss : 0.3316977994980093\n",
      "iterations 5518 accuracy : 0.930896870405377  loss : 0.33168766894811097\n",
      "iterations 5519 accuracy : 0.930896870405377  loss : 0.33167844864588747\n",
      "iterations 5520 accuracy : 0.930896870405377  loss : 0.33166870323345854\n",
      "iterations 5521 accuracy : 0.930896870405377  loss : 0.33165866750541156\n",
      "iterations 5522 accuracy : 0.930896870405377  loss : 0.33164840054587114\n",
      "iterations 5523 accuracy : 0.930896870405377  loss : 0.33163844900800954\n",
      "iterations 5524 accuracy : 0.9306868304977945  loss : 0.33162802305062467\n",
      "iterations 5525 accuracy : 0.930896870405377  loss : 0.3316188937936164\n",
      "iterations 5526 accuracy : 0.930896870405377  loss : 0.33160896585332816\n",
      "iterations 5527 accuracy : 0.930896870405377  loss : 0.3315996804149386\n",
      "iterations 5528 accuracy : 0.930896870405377  loss : 0.33159004925140545\n",
      "iterations 5529 accuracy : 0.930896870405377  loss : 0.33157996162662146\n",
      "iterations 5530 accuracy : 0.930896870405377  loss : 0.33157074219009064\n",
      "iterations 5531 accuracy : 0.930896870405377  loss : 0.3315607101830315\n",
      "iterations 5532 accuracy : 0.930896870405377  loss : 0.331551286817009\n",
      "iterations 5533 accuracy : 0.930896870405377  loss : 0.33154155454227396\n",
      "iterations 5534 accuracy : 0.930896870405377  loss : 0.33153146442195164\n",
      "iterations 5535 accuracy : 0.930896870405377  loss : 0.331521745826153\n",
      "iterations 5536 accuracy : 0.930896870405377  loss : 0.33151189863659764\n",
      "iterations 5537 accuracy : 0.930896870405377  loss : 0.33150236540526323\n",
      "iterations 5538 accuracy : 0.930896870405377  loss : 0.33149249713282186\n",
      "iterations 5539 accuracy : 0.930896870405377  loss : 0.3314826788603653\n",
      "iterations 5540 accuracy : 0.930896870405377  loss : 0.33147299238899375\n",
      "iterations 5541 accuracy : 0.930896870405377  loss : 0.3314628636715208\n",
      "iterations 5542 accuracy : 0.930896870405377  loss : 0.33145302891220596\n",
      "iterations 5543 accuracy : 0.930896870405377  loss : 0.3314434063859679\n",
      "iterations 5544 accuracy : 0.930896870405377  loss : 0.331433428804818\n",
      "iterations 5545 accuracy : 0.930896870405377  loss : 0.33142414843068657\n",
      "iterations 5546 accuracy : 0.930896870405377  loss : 0.33141456527685115\n",
      "iterations 5547 accuracy : 0.930896870405377  loss : 0.33140520259289713\n",
      "iterations 5548 accuracy : 0.930896870405377  loss : 0.33139546665222797\n",
      "iterations 5549 accuracy : 0.930896870405377  loss : 0.33138548502648535\n",
      "iterations 5550 accuracy : 0.930896870405377  loss : 0.33137540775324337\n",
      "iterations 5551 accuracy : 0.930896870405377  loss : 0.3313659478284524\n",
      "iterations 5552 accuracy : 0.930896870405377  loss : 0.33135629669963684\n",
      "iterations 5553 accuracy : 0.930896870405377  loss : 0.331346632706086\n",
      "iterations 5554 accuracy : 0.930896870405377  loss : 0.3313371232293084\n",
      "iterations 5555 accuracy : 0.930896870405377  loss : 0.33132727733834566\n",
      "iterations 5556 accuracy : 0.930896870405377  loss : 0.3313171293878486\n",
      "iterations 5557 accuracy : 0.930896870405377  loss : 0.33130737909691954\n",
      "iterations 5558 accuracy : 0.930896870405377  loss : 0.33129779314342755\n",
      "iterations 5559 accuracy : 0.930896870405377  loss : 0.33128758177504286\n",
      "iterations 5560 accuracy : 0.930896870405377  loss : 0.3312779965213336\n",
      "iterations 5561 accuracy : 0.930896870405377  loss : 0.33126806244109214\n",
      "iterations 5562 accuracy : 0.930896870405377  loss : 0.33125869468553976\n",
      "iterations 5563 accuracy : 0.930896870405377  loss : 0.33124954290794384\n",
      "iterations 5564 accuracy : 0.930896870405377  loss : 0.3312396923083116\n",
      "iterations 5565 accuracy : 0.930896870405377  loss : 0.3312289029932892\n",
      "iterations 5566 accuracy : 0.9306868304977945  loss : 0.3312182351402984\n",
      "iterations 5567 accuracy : 0.930896870405377  loss : 0.3312093346124736\n",
      "iterations 5568 accuracy : 0.930896870405377  loss : 0.33119975625831527\n",
      "iterations 5569 accuracy : 0.930896870405377  loss : 0.331190442223034\n",
      "iterations 5570 accuracy : 0.930896870405377  loss : 0.3311809497509387\n",
      "iterations 5571 accuracy : 0.930896870405377  loss : 0.3311716666455845\n",
      "iterations 5572 accuracy : 0.930896870405377  loss : 0.3311616870470589\n",
      "iterations 5573 accuracy : 0.930896870405377  loss : 0.33115168109927384\n",
      "iterations 5574 accuracy : 0.930896870405377  loss : 0.33114197087496805\n",
      "iterations 5575 accuracy : 0.930896870405377  loss : 0.33113175375444637\n",
      "iterations 5576 accuracy : 0.9306868304977945  loss : 0.33112156337905585\n",
      "iterations 5577 accuracy : 0.9306868304977945  loss : 0.33111161378232656\n",
      "iterations 5578 accuracy : 0.9306868304977945  loss : 0.3311019712108221\n",
      "iterations 5579 accuracy : 0.9306868304977945  loss : 0.33109174133238073\n",
      "iterations 5580 accuracy : 0.9306868304977945  loss : 0.3310817279856822\n",
      "iterations 5581 accuracy : 0.9306868304977945  loss : 0.33107250447492426\n",
      "iterations 5582 accuracy : 0.9306868304977945  loss : 0.3310629275127449\n",
      "iterations 5583 accuracy : 0.9306868304977945  loss : 0.33105361299712743\n",
      "iterations 5584 accuracy : 0.9306868304977945  loss : 0.3310441456468443\n",
      "iterations 5585 accuracy : 0.9306868304977945  loss : 0.33103424902148415\n",
      "iterations 5586 accuracy : 0.9306868304977945  loss : 0.3310243707109119\n",
      "iterations 5587 accuracy : 0.930896870405377  loss : 0.33101534360442814\n",
      "iterations 5588 accuracy : 0.9306868304977945  loss : 0.3310056497940022\n",
      "iterations 5589 accuracy : 0.9306868304977945  loss : 0.33099605586164565\n",
      "iterations 5590 accuracy : 0.9306868304977945  loss : 0.33098627832659305\n",
      "iterations 5591 accuracy : 0.9306868304977945  loss : 0.3309764546749985\n",
      "iterations 5592 accuracy : 0.9306868304977945  loss : 0.33096743615524743\n",
      "iterations 5593 accuracy : 0.9306868304977945  loss : 0.3309574940351412\n",
      "iterations 5594 accuracy : 0.9306868304977945  loss : 0.33094795783949804\n",
      "iterations 5595 accuracy : 0.9306868304977945  loss : 0.33093812022056396\n",
      "iterations 5596 accuracy : 0.9306868304977945  loss : 0.3309290763131763\n",
      "iterations 5597 accuracy : 0.9306868304977945  loss : 0.3309186629416349\n",
      "iterations 5598 accuracy : 0.9306868304977945  loss : 0.3309088605311504\n",
      "iterations 5599 accuracy : 0.9306868304977945  loss : 0.3308993561839447\n",
      "iterations 5600 accuracy : 0.9306868304977945  loss : 0.33088978025353555\n",
      "iterations 5601 accuracy : 0.9306868304977945  loss : 0.3308796371296453\n",
      "iterations 5602 accuracy : 0.9306868304977945  loss : 0.3308697160430271\n",
      "iterations 5603 accuracy : 0.9306868304977945  loss : 0.3308603028223408\n",
      "iterations 5604 accuracy : 0.9306868304977945  loss : 0.3308511569496563\n",
      "iterations 5605 accuracy : 0.9306868304977945  loss : 0.33084159794032897\n",
      "iterations 5606 accuracy : 0.9306868304977945  loss : 0.3308327661071351\n",
      "iterations 5607 accuracy : 0.9306868304977945  loss : 0.330823432995423\n",
      "iterations 5608 accuracy : 0.9306868304977945  loss : 0.3308133508828834\n",
      "iterations 5609 accuracy : 0.9306868304977945  loss : 0.33080353260343764\n",
      "iterations 5610 accuracy : 0.9306868304977945  loss : 0.3307939512444727\n",
      "iterations 5611 accuracy : 0.9306868304977945  loss : 0.330784312832599\n",
      "iterations 5612 accuracy : 0.9306868304977945  loss : 0.33077528854116056\n",
      "iterations 5613 accuracy : 0.9306868304977945  loss : 0.3307658235516161\n",
      "iterations 5614 accuracy : 0.9306868304977945  loss : 0.3307569611223688\n",
      "iterations 5615 accuracy : 0.9306868304977945  loss : 0.3307467004894483\n",
      "iterations 5616 accuracy : 0.9306868304977945  loss : 0.33073798459851045\n",
      "iterations 5617 accuracy : 0.9306868304977945  loss : 0.33072830804565506\n",
      "iterations 5618 accuracy : 0.930896870405377  loss : 0.33071987801029845\n",
      "iterations 5619 accuracy : 0.930896870405377  loss : 0.3307103497345536\n",
      "iterations 5620 accuracy : 0.930896870405377  loss : 0.33070038116660827\n",
      "iterations 5621 accuracy : 0.930896870405377  loss : 0.33069043806572485\n",
      "iterations 5622 accuracy : 0.9306868304977945  loss : 0.33068086830633886\n",
      "iterations 5623 accuracy : 0.9306868304977945  loss : 0.3306710278843758\n",
      "iterations 5624 accuracy : 0.9306868304977945  loss : 0.33066163968825063\n",
      "iterations 5625 accuracy : 0.9306868304977945  loss : 0.33065197822449893\n",
      "iterations 5626 accuracy : 0.9306868304977945  loss : 0.3306429518119408\n",
      "iterations 5627 accuracy : 0.930896870405377  loss : 0.33063429431209046\n",
      "iterations 5628 accuracy : 0.9306868304977945  loss : 0.33062437746659756\n",
      "iterations 5629 accuracy : 0.9306868304977945  loss : 0.33061428746635246\n",
      "iterations 5630 accuracy : 0.9306868304977945  loss : 0.33060522451159496\n",
      "iterations 5631 accuracy : 0.9306868304977945  loss : 0.3305956050125006\n",
      "iterations 5632 accuracy : 0.9306868304977945  loss : 0.3305862153056152\n",
      "iterations 5633 accuracy : 0.9306868304977945  loss : 0.33057691913336795\n",
      "iterations 5634 accuracy : 0.9306868304977945  loss : 0.33056731594746547\n",
      "iterations 5635 accuracy : 0.9306868304977945  loss : 0.3305573425216372\n",
      "iterations 5636 accuracy : 0.9306868304977945  loss : 0.3305477898783463\n",
      "iterations 5637 accuracy : 0.9306868304977945  loss : 0.3305388357603754\n",
      "iterations 5638 accuracy : 0.9306868304977945  loss : 0.3305291540058509\n",
      "iterations 5639 accuracy : 0.9306868304977945  loss : 0.3305191538622773\n",
      "iterations 5640 accuracy : 0.9306868304977945  loss : 0.3305094996507221\n",
      "iterations 5641 accuracy : 0.9306868304977945  loss : 0.33050014216717866\n",
      "iterations 5642 accuracy : 0.9306868304977945  loss : 0.33049074229038516\n",
      "iterations 5643 accuracy : 0.9306868304977945  loss : 0.3304819582562544\n",
      "iterations 5644 accuracy : 0.9306868304977945  loss : 0.33047231818464357\n",
      "iterations 5645 accuracy : 0.9306868304977945  loss : 0.3304628704714848\n",
      "iterations 5646 accuracy : 0.9306868304977945  loss : 0.33045422984911155\n",
      "iterations 5647 accuracy : 0.9306868304977945  loss : 0.33044519910897163\n",
      "iterations 5648 accuracy : 0.9306868304977945  loss : 0.330435666085775\n",
      "iterations 5649 accuracy : 0.9306868304977945  loss : 0.33042511067812597\n",
      "iterations 5650 accuracy : 0.9306868304977945  loss : 0.33041508558613136\n",
      "iterations 5651 accuracy : 0.9306868304977945  loss : 0.3304063838843529\n",
      "iterations 5652 accuracy : 0.9306868304977945  loss : 0.33039702448307956\n",
      "iterations 5653 accuracy : 0.9306868304977945  loss : 0.3303875020570519\n",
      "iterations 5654 accuracy : 0.9306868304977945  loss : 0.3303777612343775\n",
      "iterations 5655 accuracy : 0.9306868304977945  loss : 0.3303680242811001\n",
      "iterations 5656 accuracy : 0.9306868304977945  loss : 0.33035925648265635\n",
      "iterations 5657 accuracy : 0.9306868304977945  loss : 0.3303498187913051\n",
      "iterations 5658 accuracy : 0.9306868304977945  loss : 0.33034002847067817\n",
      "iterations 5659 accuracy : 0.9306868304977945  loss : 0.3303303157991188\n",
      "iterations 5660 accuracy : 0.9306868304977945  loss : 0.3303199874924808\n",
      "iterations 5661 accuracy : 0.9306868304977945  loss : 0.3303109036559102\n",
      "iterations 5662 accuracy : 0.9306868304977945  loss : 0.3303012068109623\n",
      "iterations 5663 accuracy : 0.9306868304977945  loss : 0.3302916508877594\n",
      "iterations 5664 accuracy : 0.9306868304977945  loss : 0.3302812069272842\n",
      "iterations 5665 accuracy : 0.9306868304977945  loss : 0.330272230180588\n",
      "iterations 5666 accuracy : 0.9306868304977945  loss : 0.3302628456696563\n",
      "iterations 5667 accuracy : 0.9306868304977945  loss : 0.33025286145637794\n",
      "iterations 5668 accuracy : 0.9306868304977945  loss : 0.33024374854064165\n",
      "iterations 5669 accuracy : 0.9306868304977945  loss : 0.330234260268753\n",
      "iterations 5670 accuracy : 0.9306868304977945  loss : 0.3302246496585206\n",
      "iterations 5671 accuracy : 0.9306868304977945  loss : 0.33021595188904845\n",
      "iterations 5672 accuracy : 0.9306868304977945  loss : 0.33020658346599013\n",
      "iterations 5673 accuracy : 0.9306868304977945  loss : 0.3301966471951081\n",
      "iterations 5674 accuracy : 0.9306868304977945  loss : 0.3301876844527985\n",
      "iterations 5675 accuracy : 0.9306868304977945  loss : 0.3301780571090517\n",
      "iterations 5676 accuracy : 0.9306868304977945  loss : 0.33016928452052763\n",
      "iterations 5677 accuracy : 0.9306868304977945  loss : 0.3301598471401518\n",
      "iterations 5678 accuracy : 0.9306868304977945  loss : 0.33015037538941505\n",
      "iterations 5679 accuracy : 0.9306868304977945  loss : 0.3301409261885703\n",
      "iterations 5680 accuracy : 0.9306868304977945  loss : 0.33013157489484163\n",
      "iterations 5681 accuracy : 0.9306868304977945  loss : 0.33012139561204357\n",
      "iterations 5682 accuracy : 0.9306868304977945  loss : 0.33011245271061107\n",
      "iterations 5683 accuracy : 0.9306868304977945  loss : 0.33010373038239293\n",
      "iterations 5684 accuracy : 0.9306868304977945  loss : 0.330094361322476\n",
      "iterations 5685 accuracy : 0.9306868304977945  loss : 0.3300842011642036\n",
      "iterations 5686 accuracy : 0.9306868304977945  loss : 0.330074795630459\n",
      "iterations 5687 accuracy : 0.9306868304977945  loss : 0.330065005153822\n",
      "iterations 5688 accuracy : 0.9306868304977945  loss : 0.33005565629140565\n",
      "iterations 5689 accuracy : 0.9306868304977945  loss : 0.3300463418036261\n",
      "iterations 5690 accuracy : 0.9306868304977945  loss : 0.33003626710063394\n",
      "iterations 5691 accuracy : 0.9306868304977945  loss : 0.3300261682602327\n",
      "iterations 5692 accuracy : 0.9306868304977945  loss : 0.3300169176925242\n",
      "iterations 5693 accuracy : 0.9306868304977945  loss : 0.3300074914350589\n",
      "iterations 5694 accuracy : 0.9306868304977945  loss : 0.3299981054863346\n",
      "iterations 5695 accuracy : 0.9306868304977945  loss : 0.329988611327748\n",
      "iterations 5696 accuracy : 0.9306868304977945  loss : 0.3299800132325575\n",
      "iterations 5697 accuracy : 0.9306868304977945  loss : 0.32997075745081783\n",
      "iterations 5698 accuracy : 0.9306868304977945  loss : 0.3299614380330922\n",
      "iterations 5699 accuracy : 0.9306868304977945  loss : 0.3299516439055642\n",
      "iterations 5700 accuracy : 0.9306868304977945  loss : 0.32994215905858326\n",
      "iterations 5701 accuracy : 0.9306868304977945  loss : 0.32993272508931437\n",
      "iterations 5702 accuracy : 0.9306868304977945  loss : 0.3299235746211969\n",
      "iterations 5703 accuracy : 0.9306868304977945  loss : 0.3299138264762604\n",
      "iterations 5704 accuracy : 0.9306868304977945  loss : 0.32990477676805097\n",
      "iterations 5705 accuracy : 0.9306868304977945  loss : 0.32989503414753235\n",
      "iterations 5706 accuracy : 0.9306868304977945  loss : 0.3298854589260077\n",
      "iterations 5707 accuracy : 0.9306868304977945  loss : 0.32987590669174305\n",
      "iterations 5708 accuracy : 0.9306868304977945  loss : 0.3298669531616938\n",
      "iterations 5709 accuracy : 0.9306868304977945  loss : 0.32985771927841084\n",
      "iterations 5710 accuracy : 0.9306868304977945  loss : 0.32984798195089043\n",
      "iterations 5711 accuracy : 0.9306868304977945  loss : 0.3298393153677305\n",
      "iterations 5712 accuracy : 0.9306868304977945  loss : 0.3298296795074025\n",
      "iterations 5713 accuracy : 0.9306868304977945  loss : 0.3298209259566497\n",
      "iterations 5714 accuracy : 0.9306868304977945  loss : 0.32981196946304386\n",
      "iterations 5715 accuracy : 0.9306868304977945  loss : 0.3298026518239284\n",
      "iterations 5716 accuracy : 0.9306868304977945  loss : 0.32979397697771745\n",
      "iterations 5717 accuracy : 0.9306868304977945  loss : 0.3297845309417393\n",
      "iterations 5718 accuracy : 0.9306868304977945  loss : 0.3297747009411049\n",
      "iterations 5719 accuracy : 0.9306868304977945  loss : 0.32976549757334517\n",
      "iterations 5720 accuracy : 0.9306868304977945  loss : 0.3297559754268553\n",
      "iterations 5721 accuracy : 0.9306868304977945  loss : 0.32974665063543845\n",
      "iterations 5722 accuracy : 0.9306868304977945  loss : 0.32973666171417393\n",
      "iterations 5723 accuracy : 0.9306868304977945  loss : 0.3297276983846386\n",
      "iterations 5724 accuracy : 0.9306868304977945  loss : 0.329718618592747\n",
      "iterations 5725 accuracy : 0.9306868304977945  loss : 0.32970942666878594\n",
      "iterations 5726 accuracy : 0.9306868304977945  loss : 0.32970002644370705\n",
      "iterations 5727 accuracy : 0.9306868304977945  loss : 0.3296907503148485\n",
      "iterations 5728 accuracy : 0.9306868304977945  loss : 0.32968049979554626\n",
      "iterations 5729 accuracy : 0.9306868304977945  loss : 0.3296709629998651\n",
      "iterations 5730 accuracy : 0.9306868304977945  loss : 0.32966239794069135\n",
      "iterations 5731 accuracy : 0.9306868304977945  loss : 0.32965370292303675\n",
      "iterations 5732 accuracy : 0.9306868304977945  loss : 0.32964413800427256\n",
      "iterations 5733 accuracy : 0.9306868304977945  loss : 0.32963447814779695\n",
      "iterations 5734 accuracy : 0.9306868304977945  loss : 0.3296251323727138\n",
      "iterations 5735 accuracy : 0.9306868304977945  loss : 0.32961591674033247\n",
      "iterations 5736 accuracy : 0.9306868304977945  loss : 0.3296067022366232\n",
      "iterations 5737 accuracy : 0.9306868304977945  loss : 0.32959724283786423\n",
      "iterations 5738 accuracy : 0.9306868304977945  loss : 0.3295876098209718\n",
      "iterations 5739 accuracy : 0.9306868304977945  loss : 0.3295780635676187\n",
      "iterations 5740 accuracy : 0.9306868304977945  loss : 0.3295686914375107\n",
      "iterations 5741 accuracy : 0.930896870405377  loss : 0.32955857258400595\n",
      "iterations 5742 accuracy : 0.930896870405377  loss : 0.32954964488600774\n",
      "iterations 5743 accuracy : 0.930896870405377  loss : 0.3295404013117491\n",
      "iterations 5744 accuracy : 0.930896870405377  loss : 0.3295309363154661\n",
      "iterations 5745 accuracy : 0.930896870405377  loss : 0.3295221080670484\n",
      "iterations 5746 accuracy : 0.930896870405377  loss : 0.32951319125984285\n",
      "iterations 5747 accuracy : 0.930896870405377  loss : 0.3295038289802788\n",
      "iterations 5748 accuracy : 0.930896870405377  loss : 0.3294943115119764\n",
      "iterations 5749 accuracy : 0.930896870405377  loss : 0.3294846169470946\n",
      "iterations 5750 accuracy : 0.930896870405377  loss : 0.3294748192485074\n",
      "iterations 5751 accuracy : 0.930896870405377  loss : 0.3294657146050742\n",
      "iterations 5752 accuracy : 0.930896870405377  loss : 0.32945653079447257\n",
      "iterations 5753 accuracy : 0.930896870405377  loss : 0.32944748469056595\n",
      "iterations 5754 accuracy : 0.930896870405377  loss : 0.3294383920280279\n",
      "iterations 5755 accuracy : 0.930896870405377  loss : 0.32942933122465723\n",
      "iterations 5756 accuracy : 0.930896870405377  loss : 0.3294207103670029\n",
      "iterations 5757 accuracy : 0.930896870405377  loss : 0.3294112680883247\n",
      "iterations 5758 accuracy : 0.930896870405377  loss : 0.3294026845670167\n",
      "iterations 5759 accuracy : 0.930896870405377  loss : 0.32939388841936074\n",
      "iterations 5760 accuracy : 0.930896870405377  loss : 0.32938477438908426\n",
      "iterations 5761 accuracy : 0.930896870405377  loss : 0.32937532144104303\n",
      "iterations 5762 accuracy : 0.930896870405377  loss : 0.32936701076146574\n",
      "iterations 5763 accuracy : 0.930896870405377  loss : 0.3293580430478471\n",
      "iterations 5764 accuracy : 0.930896870405377  loss : 0.32934885885159687\n",
      "iterations 5765 accuracy : 0.930896870405377  loss : 0.3293397377499778\n",
      "iterations 5766 accuracy : 0.930896870405377  loss : 0.3293309800448268\n",
      "iterations 5767 accuracy : 0.930896870405377  loss : 0.32932064425189916\n",
      "iterations 5768 accuracy : 0.930896870405377  loss : 0.3293118703229546\n",
      "iterations 5769 accuracy : 0.930896870405377  loss : 0.32930237594457495\n",
      "iterations 5770 accuracy : 0.930896870405377  loss : 0.32929382741518753\n",
      "iterations 5771 accuracy : 0.930896870405377  loss : 0.3292844614064084\n",
      "iterations 5772 accuracy : 0.930896870405377  loss : 0.32927466080588075\n",
      "iterations 5773 accuracy : 0.930896870405377  loss : 0.32926522138813064\n",
      "iterations 5774 accuracy : 0.930896870405377  loss : 0.32925608417060687\n",
      "iterations 5775 accuracy : 0.930896870405377  loss : 0.32924735712390457\n",
      "iterations 5776 accuracy : 0.930896870405377  loss : 0.3292384089499139\n",
      "iterations 5777 accuracy : 0.930896870405377  loss : 0.3292293482723946\n",
      "iterations 5778 accuracy : 0.930896870405377  loss : 0.32922011628612724\n",
      "iterations 5779 accuracy : 0.930896870405377  loss : 0.3292101972631409\n",
      "iterations 5780 accuracy : 0.930896870405377  loss : 0.3292014451910199\n",
      "iterations 5781 accuracy : 0.930896870405377  loss : 0.3291925111362473\n",
      "iterations 5782 accuracy : 0.930896870405377  loss : 0.32918339221287773\n",
      "iterations 5783 accuracy : 0.930896870405377  loss : 0.32917393162702024\n",
      "iterations 5784 accuracy : 0.930896870405377  loss : 0.3291652732661664\n",
      "iterations 5785 accuracy : 0.930896870405377  loss : 0.3291553830034579\n",
      "iterations 5786 accuracy : 0.930896870405377  loss : 0.32914592753015437\n",
      "iterations 5787 accuracy : 0.930896870405377  loss : 0.32913681930152794\n",
      "iterations 5788 accuracy : 0.930896870405377  loss : 0.329127908512427\n",
      "iterations 5789 accuracy : 0.930896870405377  loss : 0.3291186568159308\n",
      "iterations 5790 accuracy : 0.930896870405377  loss : 0.3291095476430077\n",
      "iterations 5791 accuracy : 0.930896870405377  loss : 0.3291008034521446\n",
      "iterations 5792 accuracy : 0.930896870405377  loss : 0.3290920965211245\n",
      "iterations 5793 accuracy : 0.930896870405377  loss : 0.329082887520657\n",
      "iterations 5794 accuracy : 0.930896870405377  loss : 0.32907393295341797\n",
      "iterations 5795 accuracy : 0.930896870405377  loss : 0.3290651074319771\n",
      "iterations 5796 accuracy : 0.930896870405377  loss : 0.32905581340854073\n",
      "iterations 5797 accuracy : 0.930896870405377  loss : 0.3290472677226088\n",
      "iterations 5798 accuracy : 0.930896870405377  loss : 0.32903801550184414\n",
      "iterations 5799 accuracy : 0.930896870405377  loss : 0.32902933472408025\n",
      "iterations 5800 accuracy : 0.930896870405377  loss : 0.32902045407572106\n",
      "iterations 5801 accuracy : 0.930896870405377  loss : 0.3290113780756408\n",
      "iterations 5802 accuracy : 0.930896870405377  loss : 0.32900222189951456\n",
      "iterations 5803 accuracy : 0.930896870405377  loss : 0.3289934748914242\n",
      "iterations 5804 accuracy : 0.930896870405377  loss : 0.3289846347517762\n",
      "iterations 5805 accuracy : 0.930896870405377  loss : 0.3289757445320779\n",
      "iterations 5806 accuracy : 0.930896870405377  loss : 0.3289670794754186\n",
      "iterations 5807 accuracy : 0.930896870405377  loss : 0.3289578142334883\n",
      "iterations 5808 accuracy : 0.930896870405377  loss : 0.32894872817868925\n",
      "iterations 5809 accuracy : 0.930896870405377  loss : 0.32893936990076633\n",
      "iterations 5810 accuracy : 0.930896870405377  loss : 0.32892992849939834\n",
      "iterations 5811 accuracy : 0.930896870405377  loss : 0.328921745486329\n",
      "iterations 5812 accuracy : 0.930896870405377  loss : 0.328912369617947\n",
      "iterations 5813 accuracy : 0.930896870405377  loss : 0.3289032256534851\n",
      "iterations 5814 accuracy : 0.930896870405377  loss : 0.328895024105469\n",
      "iterations 5815 accuracy : 0.930896870405377  loss : 0.32888604029215474\n",
      "iterations 5816 accuracy : 0.930896870405377  loss : 0.32887677156101974\n",
      "iterations 5817 accuracy : 0.930896870405377  loss : 0.32886714647896836\n",
      "iterations 5818 accuracy : 0.930896870405377  loss : 0.32885802535414027\n",
      "iterations 5819 accuracy : 0.930896870405377  loss : 0.3288495407234853\n",
      "iterations 5820 accuracy : 0.930896870405377  loss : 0.3288403785983715\n",
      "iterations 5821 accuracy : 0.930896870405377  loss : 0.32883099469870913\n",
      "iterations 5822 accuracy : 0.930896870405377  loss : 0.32882244975854247\n",
      "iterations 5823 accuracy : 0.930896870405377  loss : 0.328813805153631\n",
      "iterations 5824 accuracy : 0.930896870405377  loss : 0.32880458506642557\n",
      "iterations 5825 accuracy : 0.930896870405377  loss : 0.32879559637986017\n",
      "iterations 5826 accuracy : 0.930896870405377  loss : 0.3287869817433573\n",
      "iterations 5827 accuracy : 0.930896870405377  loss : 0.32877798973341715\n",
      "iterations 5828 accuracy : 0.930896870405377  loss : 0.32876861022448406\n",
      "iterations 5829 accuracy : 0.930896870405377  loss : 0.32875950445941815\n",
      "iterations 5830 accuracy : 0.930896870405377  loss : 0.3287505871514103\n",
      "iterations 5831 accuracy : 0.930896870405377  loss : 0.3287408002562511\n",
      "iterations 5832 accuracy : 0.930896870405377  loss : 0.3287315544208743\n",
      "iterations 5833 accuracy : 0.930896870405377  loss : 0.3287228322586277\n",
      "iterations 5834 accuracy : 0.930896870405377  loss : 0.32871391917064885\n",
      "iterations 5835 accuracy : 0.930896870405377  loss : 0.3287044822918064\n",
      "iterations 5836 accuracy : 0.930896870405377  loss : 0.3286960912784294\n",
      "iterations 5837 accuracy : 0.930896870405377  loss : 0.3286871443800063\n",
      "iterations 5838 accuracy : 0.930896870405377  loss : 0.3286779749521001\n",
      "iterations 5839 accuracy : 0.930896870405377  loss : 0.32866932893347583\n",
      "iterations 5840 accuracy : 0.930896870405377  loss : 0.32866012566761404\n",
      "iterations 5841 accuracy : 0.930896870405377  loss : 0.3286509006314747\n",
      "iterations 5842 accuracy : 0.930896870405377  loss : 0.32864150748319726\n",
      "iterations 5843 accuracy : 0.930896870405377  loss : 0.3286325641577207\n",
      "iterations 5844 accuracy : 0.930896870405377  loss : 0.3286229223000815\n",
      "iterations 5845 accuracy : 0.930896870405377  loss : 0.32861405567042806\n",
      "iterations 5846 accuracy : 0.930896870405377  loss : 0.3286051979073339\n",
      "iterations 5847 accuracy : 0.930896870405377  loss : 0.32859633544832145\n",
      "iterations 5848 accuracy : 0.930896870405377  loss : 0.3285872464444759\n",
      "iterations 5849 accuracy : 0.930896870405377  loss : 0.32857818484397133\n",
      "iterations 5850 accuracy : 0.930896870405377  loss : 0.32856934738396815\n",
      "iterations 5851 accuracy : 0.930896870405377  loss : 0.3285602630672264\n",
      "iterations 5852 accuracy : 0.930896870405377  loss : 0.32855104131405877\n",
      "iterations 5853 accuracy : 0.930896870405377  loss : 0.32854232483951856\n",
      "iterations 5854 accuracy : 0.930896870405377  loss : 0.3285327585170536\n",
      "iterations 5855 accuracy : 0.930896870405377  loss : 0.32852409401072613\n",
      "iterations 5856 accuracy : 0.930896870405377  loss : 0.32851618762043405\n",
      "iterations 5857 accuracy : 0.930896870405377  loss : 0.3285075232837218\n",
      "iterations 5858 accuracy : 0.930896870405377  loss : 0.32849801256387756\n",
      "iterations 5859 accuracy : 0.930896870405377  loss : 0.32848935983016814\n",
      "iterations 5860 accuracy : 0.930896870405377  loss : 0.32848054414026595\n",
      "iterations 5861 accuracy : 0.930896870405377  loss : 0.3284720834791639\n",
      "iterations 5862 accuracy : 0.930896870405377  loss : 0.32846283193707193\n",
      "iterations 5863 accuracy : 0.930896870405377  loss : 0.3284538616896022\n",
      "iterations 5864 accuracy : 0.930896870405377  loss : 0.3284444572131899\n",
      "iterations 5865 accuracy : 0.930896870405377  loss : 0.3284357748118427\n",
      "iterations 5866 accuracy : 0.930896870405377  loss : 0.32842686989317155\n",
      "iterations 5867 accuracy : 0.930896870405377  loss : 0.3284178864725782\n",
      "iterations 5868 accuracy : 0.930896870405377  loss : 0.3284090739770188\n",
      "iterations 5869 accuracy : 0.930896870405377  loss : 0.32839991358409537\n",
      "iterations 5870 accuracy : 0.930896870405377  loss : 0.32839110295756807\n",
      "iterations 5871 accuracy : 0.930896870405377  loss : 0.3283822190150843\n",
      "iterations 5872 accuracy : 0.930896870405377  loss : 0.3283736923619034\n",
      "iterations 5873 accuracy : 0.930896870405377  loss : 0.3283650564055747\n",
      "iterations 5874 accuracy : 0.930896870405377  loss : 0.3283562912072048\n",
      "iterations 5875 accuracy : 0.930896870405377  loss : 0.3283470915579989\n",
      "iterations 5876 accuracy : 0.930896870405377  loss : 0.3283375830037531\n",
      "iterations 5877 accuracy : 0.930896870405377  loss : 0.3283288966521115\n",
      "iterations 5878 accuracy : 0.930896870405377  loss : 0.3283197475556341\n",
      "iterations 5879 accuracy : 0.930896870405377  loss : 0.32831084959750534\n",
      "iterations 5880 accuracy : 0.930896870405377  loss : 0.3283025174894922\n",
      "iterations 5881 accuracy : 0.930896870405377  loss : 0.3282937433053982\n",
      "iterations 5882 accuracy : 0.930896870405377  loss : 0.3282844150499403\n",
      "iterations 5883 accuracy : 0.930896870405377  loss : 0.32827601833986564\n",
      "iterations 5884 accuracy : 0.930896870405377  loss : 0.3282669762869231\n",
      "iterations 5885 accuracy : 0.930896870405377  loss : 0.32825840073645346\n",
      "iterations 5886 accuracy : 0.930896870405377  loss : 0.32825034604499304\n",
      "iterations 5887 accuracy : 0.930896870405377  loss : 0.328240912270257\n",
      "iterations 5888 accuracy : 0.930896870405377  loss : 0.3282323917725465\n",
      "iterations 5889 accuracy : 0.930896870405377  loss : 0.32822343140924837\n",
      "iterations 5890 accuracy : 0.930896870405377  loss : 0.32821523121566165\n",
      "iterations 5891 accuracy : 0.930896870405377  loss : 0.3282057421170324\n",
      "iterations 5892 accuracy : 0.930896870405377  loss : 0.3281973190368759\n",
      "iterations 5893 accuracy : 0.930896870405377  loss : 0.32818841208108634\n",
      "iterations 5894 accuracy : 0.930896870405377  loss : 0.3281798140280916\n",
      "iterations 5895 accuracy : 0.930896870405377  loss : 0.32817084293865983\n",
      "iterations 5896 accuracy : 0.930896870405377  loss : 0.3281619261687533\n",
      "iterations 5897 accuracy : 0.930896870405377  loss : 0.32815243981702685\n",
      "iterations 5898 accuracy : 0.930896870405377  loss : 0.3281428449769228\n",
      "iterations 5899 accuracy : 0.930896870405377  loss : 0.32813426607120905\n",
      "iterations 5900 accuracy : 0.930896870405377  loss : 0.3281250849849248\n",
      "iterations 5901 accuracy : 0.930896870405377  loss : 0.3281162309239588\n",
      "iterations 5902 accuracy : 0.930896870405377  loss : 0.32810792606880984\n",
      "iterations 5903 accuracy : 0.930896870405377  loss : 0.32809900206605636\n",
      "iterations 5904 accuracy : 0.930896870405377  loss : 0.3280902872430241\n",
      "iterations 5905 accuracy : 0.930896870405377  loss : 0.32808133969508146\n",
      "iterations 5906 accuracy : 0.930896870405377  loss : 0.3280717373255941\n",
      "iterations 5907 accuracy : 0.930896870405377  loss : 0.3280628135815843\n",
      "iterations 5908 accuracy : 0.930896870405377  loss : 0.328053963584035\n",
      "iterations 5909 accuracy : 0.930896870405377  loss : 0.32804491204190556\n",
      "iterations 5910 accuracy : 0.930896870405377  loss : 0.32803656660927016\n",
      "iterations 5911 accuracy : 0.930896870405377  loss : 0.3280287332985156\n",
      "iterations 5912 accuracy : 0.930896870405377  loss : 0.3280204514152625\n",
      "iterations 5913 accuracy : 0.930896870405377  loss : 0.3280116822582318\n",
      "iterations 5914 accuracy : 0.930896870405377  loss : 0.32800251716108403\n",
      "iterations 5915 accuracy : 0.930896870405377  loss : 0.3279941457149388\n",
      "iterations 5916 accuracy : 0.930896870405377  loss : 0.3279845834312428\n",
      "iterations 5917 accuracy : 0.930896870405377  loss : 0.3279764091830361\n",
      "iterations 5918 accuracy : 0.930896870405377  loss : 0.3279679873072308\n",
      "iterations 5919 accuracy : 0.930896870405377  loss : 0.3279598013801185\n",
      "iterations 5920 accuracy : 0.930896870405377  loss : 0.327950812206892\n",
      "iterations 5921 accuracy : 0.930896870405377  loss : 0.32794230850666717\n",
      "iterations 5922 accuracy : 0.930896870405377  loss : 0.3279339186003824\n",
      "iterations 5923 accuracy : 0.930896870405377  loss : 0.32792474175883013\n",
      "iterations 5924 accuracy : 0.930896870405377  loss : 0.3279160842377917\n",
      "iterations 5925 accuracy : 0.930896870405377  loss : 0.3279075174606956\n",
      "iterations 5926 accuracy : 0.930896870405377  loss : 0.3278996453144833\n",
      "iterations 5927 accuracy : 0.930896870405377  loss : 0.32789125734611113\n",
      "iterations 5928 accuracy : 0.930896870405377  loss : 0.32788314788663053\n",
      "iterations 5929 accuracy : 0.930896870405377  loss : 0.32787378136719525\n",
      "iterations 5930 accuracy : 0.930896870405377  loss : 0.3278641065830687\n",
      "iterations 5931 accuracy : 0.930896870405377  loss : 0.3278548148916698\n",
      "iterations 5932 accuracy : 0.930896870405377  loss : 0.32784657760213215\n",
      "iterations 5933 accuracy : 0.930896870405377  loss : 0.327836963595521\n",
      "iterations 5934 accuracy : 0.930896870405377  loss : 0.3278283644186028\n",
      "iterations 5935 accuracy : 0.930896870405377  loss : 0.32781962641587004\n",
      "iterations 5936 accuracy : 0.930896870405377  loss : 0.327810882799153\n",
      "iterations 5937 accuracy : 0.930896870405377  loss : 0.3278025144648953\n",
      "iterations 5938 accuracy : 0.930896870405377  loss : 0.32779337369765355\n",
      "iterations 5939 accuracy : 0.930896870405377  loss : 0.3277846115931036\n",
      "iterations 5940 accuracy : 0.930896870405377  loss : 0.3277763104107112\n",
      "iterations 5941 accuracy : 0.930896870405377  loss : 0.3277675527976179\n",
      "iterations 5942 accuracy : 0.930896870405377  loss : 0.3277584877616152\n",
      "iterations 5943 accuracy : 0.930896870405377  loss : 0.32774988889660395\n",
      "iterations 5944 accuracy : 0.9306868304977945  loss : 0.3277408332457741\n",
      "iterations 5945 accuracy : 0.9306868304977945  loss : 0.32773207207543936\n",
      "iterations 5946 accuracy : 0.9306868304977945  loss : 0.327723376997999\n",
      "iterations 5947 accuracy : 0.9306868304977945  loss : 0.32771540402299615\n",
      "iterations 5948 accuracy : 0.930896870405377  loss : 0.32770704107229653\n",
      "iterations 5949 accuracy : 0.9306868304977945  loss : 0.3276985524277995\n",
      "iterations 5950 accuracy : 0.9306868304977945  loss : 0.3276901616474905\n",
      "iterations 5951 accuracy : 0.9306868304977945  loss : 0.3276811605394213\n",
      "iterations 5952 accuracy : 0.9306868304977945  loss : 0.32767219099763834\n",
      "iterations 5953 accuracy : 0.9306868304977945  loss : 0.32766367632868404\n",
      "iterations 5954 accuracy : 0.9306868304977945  loss : 0.32765448265475444\n",
      "iterations 5955 accuracy : 0.9306868304977945  loss : 0.3276451699379264\n",
      "iterations 5956 accuracy : 0.9306868304977945  loss : 0.32763646284196224\n",
      "iterations 5957 accuracy : 0.9306868304977945  loss : 0.3276283743013856\n",
      "iterations 5958 accuracy : 0.9306868304977945  loss : 0.3276203003041696\n",
      "iterations 5959 accuracy : 0.9306868304977945  loss : 0.32761170041891335\n",
      "iterations 5960 accuracy : 0.9306868304977945  loss : 0.3276028641626173\n",
      "iterations 5961 accuracy : 0.9306868304977945  loss : 0.32759400370842284\n",
      "iterations 5962 accuracy : 0.9306868304977945  loss : 0.3275848300500688\n",
      "iterations 5963 accuracy : 0.9306868304977945  loss : 0.3275761115457511\n",
      "iterations 5964 accuracy : 0.9306868304977945  loss : 0.327566343724815\n",
      "iterations 5965 accuracy : 0.9306868304977945  loss : 0.32755807622234223\n",
      "iterations 5966 accuracy : 0.9306868304977945  loss : 0.32754937488351094\n",
      "iterations 5967 accuracy : 0.9306868304977945  loss : 0.3275405654163771\n",
      "iterations 5968 accuracy : 0.9304767905902122  loss : 0.32753170256775654\n",
      "iterations 5969 accuracy : 0.9304767905902122  loss : 0.3275223296298018\n",
      "iterations 5970 accuracy : 0.9304767905902122  loss : 0.32751336177398543\n",
      "iterations 5971 accuracy : 0.9304767905902122  loss : 0.32750462454583557\n",
      "iterations 5972 accuracy : 0.9304767905902122  loss : 0.3274961012919978\n",
      "iterations 5973 accuracy : 0.9304767905902122  loss : 0.32748807739397745\n",
      "iterations 5974 accuracy : 0.9304767905902122  loss : 0.32747935254756494\n",
      "iterations 5975 accuracy : 0.9304767905902122  loss : 0.32747073766153867\n",
      "iterations 5976 accuracy : 0.9304767905902122  loss : 0.32746207686896434\n",
      "iterations 5977 accuracy : 0.9304767905902122  loss : 0.32745370038919747\n",
      "iterations 5978 accuracy : 0.9304767905902122  loss : 0.32744434519666193\n",
      "iterations 5979 accuracy : 0.9304767905902122  loss : 0.3274352182773185\n",
      "iterations 5980 accuracy : 0.9304767905902122  loss : 0.3274265513878018\n",
      "iterations 5981 accuracy : 0.9304767905902122  loss : 0.3274179518179994\n",
      "iterations 5982 accuracy : 0.9304767905902122  loss : 0.3274091535307392\n",
      "iterations 5983 accuracy : 0.9304767905902122  loss : 0.3274004618306373\n",
      "iterations 5984 accuracy : 0.9306868304977945  loss : 0.3273917427351668\n",
      "iterations 5985 accuracy : 0.9306868304977945  loss : 0.3273830109412707\n",
      "iterations 5986 accuracy : 0.9306868304977945  loss : 0.3273746043371861\n",
      "iterations 5987 accuracy : 0.9306868304977945  loss : 0.3273660254167706\n",
      "iterations 5988 accuracy : 0.9306868304977945  loss : 0.32735768950632316\n",
      "iterations 5989 accuracy : 0.9306868304977945  loss : 0.32734928025955257\n",
      "iterations 5990 accuracy : 0.9306868304977945  loss : 0.32734056279377455\n",
      "iterations 5991 accuracy : 0.9306868304977945  loss : 0.32733191766441\n",
      "iterations 5992 accuracy : 0.9304767905902122  loss : 0.32732356401734725\n",
      "iterations 5993 accuracy : 0.9306868304977945  loss : 0.32731471249982536\n",
      "iterations 5994 accuracy : 0.9306868304977945  loss : 0.3273060405233587\n",
      "iterations 5995 accuracy : 0.9306868304977945  loss : 0.3272975139597458\n",
      "iterations 5996 accuracy : 0.9306868304977945  loss : 0.32728884003733333\n",
      "iterations 5997 accuracy : 0.9306868304977945  loss : 0.32728085689111236\n",
      "iterations 5998 accuracy : 0.9306868304977945  loss : 0.32727206243356327\n",
      "iterations 5999 accuracy : 0.9306868304977945  loss : 0.3272634177866519\n",
      "iterations 6000 accuracy : 0.9306868304977945  loss : 0.3272551054340461\n",
      "iterations 6001 accuracy : 0.9306868304977945  loss : 0.3272463098126864\n",
      "iterations 6002 accuracy : 0.9306868304977945  loss : 0.3272381249165921\n",
      "iterations 6003 accuracy : 0.9304767905902122  loss : 0.3272302476543208\n",
      "iterations 6004 accuracy : 0.9304767905902122  loss : 0.3272221583112585\n",
      "iterations 6005 accuracy : 0.9304767905902122  loss : 0.3272137838911744\n",
      "iterations 6006 accuracy : 0.9304767905902122  loss : 0.3272064463397249\n",
      "iterations 6007 accuracy : 0.9304767905902122  loss : 0.3271979773619283\n",
      "iterations 6008 accuracy : 0.9304767905902122  loss : 0.3271889135585313\n",
      "iterations 6009 accuracy : 0.9304767905902122  loss : 0.32718040598595477\n",
      "iterations 6010 accuracy : 0.9304767905902122  loss : 0.3271723047050498\n",
      "iterations 6011 accuracy : 0.9304767905902122  loss : 0.32716424460153815\n",
      "iterations 6012 accuracy : 0.9304767905902122  loss : 0.32715551492986394\n",
      "iterations 6013 accuracy : 0.9304767905902122  loss : 0.3271473228200093\n",
      "iterations 6014 accuracy : 0.9304767905902122  loss : 0.32713860256099664\n",
      "iterations 6015 accuracy : 0.9304767905902122  loss : 0.3271305584786956\n",
      "iterations 6016 accuracy : 0.9304767905902122  loss : 0.32712275637373717\n",
      "iterations 6017 accuracy : 0.9304767905902122  loss : 0.32711503791140045\n",
      "iterations 6018 accuracy : 0.9306868304977945  loss : 0.3271069140722451\n",
      "iterations 6019 accuracy : 0.9306868304977945  loss : 0.3270990369181673\n",
      "iterations 6020 accuracy : 0.9306868304977945  loss : 0.3270903962351447\n",
      "iterations 6021 accuracy : 0.9306868304977945  loss : 0.32708159634864437\n",
      "iterations 6022 accuracy : 0.9306868304977945  loss : 0.3270727146582119\n",
      "iterations 6023 accuracy : 0.9306868304977945  loss : 0.3270637725750169\n",
      "iterations 6024 accuracy : 0.9306868304977945  loss : 0.32705531341493826\n",
      "iterations 6025 accuracy : 0.9306868304977945  loss : 0.32704705397543093\n",
      "iterations 6026 accuracy : 0.9304767905902122  loss : 0.3270379528054547\n",
      "iterations 6027 accuracy : 0.9304767905902122  loss : 0.3270297380016002\n",
      "iterations 6028 accuracy : 0.9306868304977945  loss : 0.3270217751253601\n",
      "iterations 6029 accuracy : 0.9306868304977945  loss : 0.32701374877299505\n",
      "iterations 6030 accuracy : 0.9306868304977945  loss : 0.32700489967944574\n",
      "iterations 6031 accuracy : 0.9306868304977945  loss : 0.3269960935701666\n",
      "iterations 6032 accuracy : 0.9304767905902122  loss : 0.32698715900255587\n",
      "iterations 6033 accuracy : 0.9304767905902122  loss : 0.32697859621167347\n",
      "iterations 6034 accuracy : 0.9304767905902122  loss : 0.32697017284185503\n",
      "iterations 6035 accuracy : 0.9304767905902122  loss : 0.3269615158480169\n",
      "iterations 6036 accuracy : 0.9304767905902122  loss : 0.32695319703728914\n",
      "iterations 6037 accuracy : 0.9304767905902122  loss : 0.3269449699646903\n",
      "iterations 6038 accuracy : 0.9304767905902122  loss : 0.3269365021486097\n",
      "iterations 6039 accuracy : 0.9304767905902122  loss : 0.3269279600321401\n",
      "iterations 6040 accuracy : 0.9304767905902122  loss : 0.3269198019275539\n",
      "iterations 6041 accuracy : 0.9304767905902122  loss : 0.3269116702687199\n",
      "iterations 6042 accuracy : 0.9304767905902122  loss : 0.32690315941749903\n",
      "iterations 6043 accuracy : 0.9304767905902122  loss : 0.3268952701862123\n",
      "iterations 6044 accuracy : 0.9304767905902122  loss : 0.32688727411249\n",
      "iterations 6045 accuracy : 0.9304767905902122  loss : 0.3268784469085232\n",
      "iterations 6046 accuracy : 0.9304767905902122  loss : 0.3268701663758327\n",
      "iterations 6047 accuracy : 0.9304767905902122  loss : 0.3268623766753044\n",
      "iterations 6048 accuracy : 0.9304767905902122  loss : 0.32685387536755506\n",
      "iterations 6049 accuracy : 0.9304767905902122  loss : 0.32684473607434567\n",
      "iterations 6050 accuracy : 0.9304767905902122  loss : 0.32683630041032175\n",
      "iterations 6051 accuracy : 0.9304767905902122  loss : 0.32682815545784305\n",
      "iterations 6052 accuracy : 0.9304767905902122  loss : 0.3268202203248915\n",
      "iterations 6053 accuracy : 0.9304767905902122  loss : 0.3268112449779416\n",
      "iterations 6054 accuracy : 0.9304767905902122  loss : 0.326802914179239\n",
      "iterations 6055 accuracy : 0.9304767905902122  loss : 0.32679378865277076\n",
      "iterations 6056 accuracy : 0.9304767905902122  loss : 0.3267852966822579\n",
      "iterations 6057 accuracy : 0.9304767905902122  loss : 0.3267765189659604\n",
      "iterations 6058 accuracy : 0.9304767905902122  loss : 0.3267681620802751\n",
      "iterations 6059 accuracy : 0.9304767905902122  loss : 0.32675954755090253\n",
      "iterations 6060 accuracy : 0.9304767905902122  loss : 0.32675117014286986\n",
      "iterations 6061 accuracy : 0.9304767905902122  loss : 0.32674253269849324\n",
      "iterations 6062 accuracy : 0.9304767905902122  loss : 0.3267349213369234\n",
      "iterations 6063 accuracy : 0.9304767905902122  loss : 0.32672647513248104\n",
      "iterations 6064 accuracy : 0.9304767905902122  loss : 0.3267177273150124\n",
      "iterations 6065 accuracy : 0.9304767905902122  loss : 0.3267087272327822\n",
      "iterations 6066 accuracy : 0.9304767905902122  loss : 0.32669962507916916\n",
      "iterations 6067 accuracy : 0.9304767905902122  loss : 0.32669152850702005\n",
      "iterations 6068 accuracy : 0.9306868304977945  loss : 0.32668152782220855\n",
      "iterations 6069 accuracy : 0.9306868304977945  loss : 0.32667341261436333\n",
      "iterations 6070 accuracy : 0.9306868304977945  loss : 0.3266652581849134\n",
      "iterations 6071 accuracy : 0.9306868304977945  loss : 0.32665733603529973\n",
      "iterations 6072 accuracy : 0.9306868304977945  loss : 0.32664895961610374\n",
      "iterations 6073 accuracy : 0.9306868304977945  loss : 0.3266404501112367\n",
      "iterations 6074 accuracy : 0.9304767905902122  loss : 0.32663348827134264\n",
      "iterations 6075 accuracy : 0.9304767905902122  loss : 0.3266249608759448\n",
      "iterations 6076 accuracy : 0.9304767905902122  loss : 0.3266169734408807\n",
      "iterations 6077 accuracy : 0.9304767905902122  loss : 0.32660817730302977\n",
      "iterations 6078 accuracy : 0.9304767905902122  loss : 0.3266000005061809\n",
      "iterations 6079 accuracy : 0.9304767905902122  loss : 0.326592017454849\n",
      "iterations 6080 accuracy : 0.9304767905902122  loss : 0.32658340994255325\n",
      "iterations 6081 accuracy : 0.9304767905902122  loss : 0.3265755863112016\n",
      "iterations 6082 accuracy : 0.9304767905902122  loss : 0.32656677896906655\n",
      "iterations 6083 accuracy : 0.9304767905902122  loss : 0.3265587826148152\n",
      "iterations 6084 accuracy : 0.9304767905902122  loss : 0.3265500541187927\n",
      "iterations 6085 accuracy : 0.9304767905902122  loss : 0.3265413395790925\n",
      "iterations 6086 accuracy : 0.9304767905902122  loss : 0.3265335491638878\n",
      "iterations 6087 accuracy : 0.9304767905902122  loss : 0.3265255568839721\n",
      "iterations 6088 accuracy : 0.9304767905902122  loss : 0.3265174045230548\n",
      "iterations 6089 accuracy : 0.9304767905902122  loss : 0.32650902532163956\n",
      "iterations 6090 accuracy : 0.9304767905902122  loss : 0.3265010996397385\n",
      "iterations 6091 accuracy : 0.9304767905902122  loss : 0.32649242779452536\n",
      "iterations 6092 accuracy : 0.9304767905902122  loss : 0.3264842867521415\n",
      "iterations 6093 accuracy : 0.9304767905902122  loss : 0.3264765925176257\n",
      "iterations 6094 accuracy : 0.9304767905902122  loss : 0.3264677433580244\n",
      "iterations 6095 accuracy : 0.9304767905902122  loss : 0.3264596792025072\n",
      "iterations 6096 accuracy : 0.9304767905902122  loss : 0.3264511899142497\n",
      "iterations 6097 accuracy : 0.9304767905902122  loss : 0.32644286373188014\n",
      "iterations 6098 accuracy : 0.9304767905902122  loss : 0.3264343979278892\n",
      "iterations 6099 accuracy : 0.9304767905902122  loss : 0.3264255795188451\n",
      "iterations 6100 accuracy : 0.9304767905902122  loss : 0.3264168835964491\n",
      "iterations 6101 accuracy : 0.9304767905902122  loss : 0.3264087260920628\n",
      "iterations 6102 accuracy : 0.9304767905902122  loss : 0.32640002988712946\n",
      "iterations 6103 accuracy : 0.9304767905902122  loss : 0.3263915261454309\n",
      "iterations 6104 accuracy : 0.9304767905902122  loss : 0.3263836122664138\n",
      "iterations 6105 accuracy : 0.9304767905902122  loss : 0.3263753447626001\n",
      "iterations 6106 accuracy : 0.9304767905902122  loss : 0.3263674997264376\n",
      "iterations 6107 accuracy : 0.9304767905902122  loss : 0.3263595478200066\n",
      "iterations 6108 accuracy : 0.9304767905902122  loss : 0.32635181674639985\n",
      "iterations 6109 accuracy : 0.9304767905902122  loss : 0.3263429713143864\n",
      "iterations 6110 accuracy : 0.9304767905902122  loss : 0.3263350553985972\n",
      "iterations 6111 accuracy : 0.9304767905902122  loss : 0.3263259247459928\n",
      "iterations 6112 accuracy : 0.9304767905902122  loss : 0.3263170581331708\n",
      "iterations 6113 accuracy : 0.9304767905902122  loss : 0.326308914535925\n",
      "iterations 6114 accuracy : 0.9304767905902122  loss : 0.3263008289865543\n",
      "iterations 6115 accuracy : 0.9306868304977945  loss : 0.3262923787039596\n",
      "iterations 6116 accuracy : 0.9306868304977945  loss : 0.3262837330042686\n",
      "iterations 6117 accuracy : 0.9306868304977945  loss : 0.32627611584200467\n",
      "iterations 6118 accuracy : 0.9304767905902122  loss : 0.3262682838401336\n",
      "iterations 6119 accuracy : 0.9306868304977945  loss : 0.32626014995966507\n",
      "iterations 6120 accuracy : 0.9306868304977945  loss : 0.3262514988042129\n",
      "iterations 6121 accuracy : 0.9306868304977945  loss : 0.3262427965014252\n",
      "iterations 6122 accuracy : 0.9304767905902122  loss : 0.326235881516837\n",
      "iterations 6123 accuracy : 0.9304767905902122  loss : 0.3262277485620568\n",
      "iterations 6124 accuracy : 0.9304767905902122  loss : 0.32622001388877486\n",
      "iterations 6125 accuracy : 0.9304767905902122  loss : 0.3262118343627629\n",
      "iterations 6126 accuracy : 0.9304767905902122  loss : 0.32620383867049607\n",
      "iterations 6127 accuracy : 0.9304767905902122  loss : 0.3261948902336293\n",
      "iterations 6128 accuracy : 0.9304767905902122  loss : 0.32618671717466996\n",
      "iterations 6129 accuracy : 0.9304767905902122  loss : 0.3261788883473865\n",
      "iterations 6130 accuracy : 0.9304767905902122  loss : 0.32617039242338336\n",
      "iterations 6131 accuracy : 0.9304767905902122  loss : 0.32616214759596995\n",
      "iterations 6132 accuracy : 0.9304767905902122  loss : 0.32615421725343047\n",
      "iterations 6133 accuracy : 0.9304767905902122  loss : 0.3261455132321668\n",
      "iterations 6134 accuracy : 0.9304767905902122  loss : 0.32613621951482114\n",
      "iterations 6135 accuracy : 0.9304767905902122  loss : 0.32612816957073243\n",
      "iterations 6136 accuracy : 0.9306868304977945  loss : 0.3261198222401807\n",
      "iterations 6137 accuracy : 0.9306868304977945  loss : 0.3261118449707794\n",
      "iterations 6138 accuracy : 0.9306868304977945  loss : 0.3261037867259704\n",
      "iterations 6139 accuracy : 0.9306868304977945  loss : 0.3260958021666759\n",
      "iterations 6140 accuracy : 0.9306868304977945  loss : 0.3260874170876184\n",
      "iterations 6141 accuracy : 0.9306868304977945  loss : 0.32607878074680013\n",
      "iterations 6142 accuracy : 0.9306868304977945  loss : 0.3260701417342057\n",
      "iterations 6143 accuracy : 0.9306868304977945  loss : 0.32606156314771384\n",
      "iterations 6144 accuracy : 0.9306868304977945  loss : 0.3260533089210535\n",
      "iterations 6145 accuracy : 0.9306868304977945  loss : 0.3260448370411788\n",
      "iterations 6146 accuracy : 0.9306868304977945  loss : 0.3260366338774187\n",
      "iterations 6147 accuracy : 0.9306868304977945  loss : 0.32602773832130966\n",
      "iterations 6148 accuracy : 0.9306868304977945  loss : 0.3260201199040683\n",
      "iterations 6149 accuracy : 0.9306868304977945  loss : 0.3260121511862001\n",
      "iterations 6150 accuracy : 0.9306868304977945  loss : 0.32600437257021914\n",
      "iterations 6151 accuracy : 0.9306868304977945  loss : 0.32599602331413113\n",
      "iterations 6152 accuracy : 0.9306868304977945  loss : 0.3259874791642811\n",
      "iterations 6153 accuracy : 0.9306868304977945  loss : 0.3259793619675986\n",
      "iterations 6154 accuracy : 0.9306868304977945  loss : 0.32597118736766667\n",
      "iterations 6155 accuracy : 0.9306868304977945  loss : 0.32596318404956814\n",
      "iterations 6156 accuracy : 0.9306868304977945  loss : 0.32595548950282294\n",
      "iterations 6157 accuracy : 0.9306868304977945  loss : 0.32594751352078655\n",
      "iterations 6158 accuracy : 0.9306868304977945  loss : 0.3259397505765119\n",
      "iterations 6159 accuracy : 0.9306868304977945  loss : 0.32593121686202625\n",
      "iterations 6160 accuracy : 0.9306868304977945  loss : 0.32592347218025103\n",
      "iterations 6161 accuracy : 0.9306868304977945  loss : 0.32591538842136486\n",
      "iterations 6162 accuracy : 0.9306868304977945  loss : 0.3259081547111275\n",
      "iterations 6163 accuracy : 0.9306868304977945  loss : 0.32589946756078525\n",
      "iterations 6164 accuracy : 0.9306868304977945  loss : 0.325891352195523\n",
      "iterations 6165 accuracy : 0.9306868304977945  loss : 0.3258835436445526\n",
      "iterations 6166 accuracy : 0.9306868304977945  loss : 0.3258754054685349\n",
      "iterations 6167 accuracy : 0.9306868304977945  loss : 0.3258677308112803\n",
      "iterations 6168 accuracy : 0.9306868304977945  loss : 0.3258603528553479\n",
      "iterations 6169 accuracy : 0.9306868304977945  loss : 0.3258527967719802\n",
      "iterations 6170 accuracy : 0.9306868304977945  loss : 0.32584482335044135\n",
      "iterations 6171 accuracy : 0.9306868304977945  loss : 0.325835990612343\n",
      "iterations 6172 accuracy : 0.9306868304977945  loss : 0.3258281893605654\n",
      "iterations 6173 accuracy : 0.9306868304977945  loss : 0.32582039963306453\n",
      "iterations 6174 accuracy : 0.9306868304977945  loss : 0.32581207502070086\n",
      "iterations 6175 accuracy : 0.9306868304977945  loss : 0.3258039703175546\n",
      "iterations 6176 accuracy : 0.9306868304977945  loss : 0.3257952744829353\n",
      "iterations 6177 accuracy : 0.9306868304977945  loss : 0.3257872204952466\n",
      "iterations 6178 accuracy : 0.9306868304977945  loss : 0.32577908454139487\n",
      "iterations 6179 accuracy : 0.9306868304977945  loss : 0.32577113224241894\n",
      "iterations 6180 accuracy : 0.9306868304977945  loss : 0.32576292484200126\n",
      "iterations 6181 accuracy : 0.9306868304977945  loss : 0.3257544810148869\n",
      "iterations 6182 accuracy : 0.9306868304977945  loss : 0.325745797461395\n",
      "iterations 6183 accuracy : 0.9306868304977945  loss : 0.32573708865289186\n",
      "iterations 6184 accuracy : 0.9306868304977945  loss : 0.32572870626315265\n",
      "iterations 6185 accuracy : 0.9306868304977945  loss : 0.3257210248198135\n",
      "iterations 6186 accuracy : 0.9306868304977945  loss : 0.3257129106025846\n",
      "iterations 6187 accuracy : 0.9306868304977945  loss : 0.32570491891641457\n",
      "iterations 6188 accuracy : 0.9306868304977945  loss : 0.3256971024471298\n",
      "iterations 6189 accuracy : 0.9306868304977945  loss : 0.3256889609796058\n",
      "iterations 6190 accuracy : 0.9306868304977945  loss : 0.3256805509894724\n",
      "iterations 6191 accuracy : 0.9306868304977945  loss : 0.3256722087935199\n",
      "iterations 6192 accuracy : 0.9306868304977945  loss : 0.3256642776929659\n",
      "iterations 6193 accuracy : 0.9306868304977945  loss : 0.3256566654209291\n",
      "iterations 6194 accuracy : 0.9306868304977945  loss : 0.32564859386699435\n",
      "iterations 6195 accuracy : 0.9306868304977945  loss : 0.3256404775375759\n",
      "iterations 6196 accuracy : 0.9306868304977945  loss : 0.32563276863617124\n",
      "iterations 6197 accuracy : 0.9304767905902122  loss : 0.3256242014299753\n",
      "iterations 6198 accuracy : 0.9306868304977945  loss : 0.32561625382720527\n",
      "iterations 6199 accuracy : 0.9306868304977945  loss : 0.32560796325854624\n",
      "iterations 6200 accuracy : 0.9304767905902122  loss : 0.3255998373854104\n",
      "iterations 6201 accuracy : 0.9304767905902122  loss : 0.32559164010712127\n",
      "iterations 6202 accuracy : 0.9304767905902122  loss : 0.3255834543603902\n",
      "iterations 6203 accuracy : 0.9306868304977945  loss : 0.3255758521446761\n",
      "iterations 6204 accuracy : 0.9304767905902122  loss : 0.3255674271459488\n",
      "iterations 6205 accuracy : 0.9304767905902122  loss : 0.32555882497184013\n",
      "iterations 6206 accuracy : 0.9304767905902122  loss : 0.3255503832569187\n",
      "iterations 6207 accuracy : 0.9304767905902122  loss : 0.3255417381840568\n",
      "iterations 6208 accuracy : 0.9304767905902122  loss : 0.3255339744709082\n",
      "iterations 6209 accuracy : 0.9304767905902122  loss : 0.32552568376758184\n",
      "iterations 6210 accuracy : 0.9304767905902122  loss : 0.325517340289234\n",
      "iterations 6211 accuracy : 0.9304767905902122  loss : 0.3255090496869611\n",
      "iterations 6212 accuracy : 0.9304767905902122  loss : 0.32550083984167216\n",
      "iterations 6213 accuracy : 0.9304767905902122  loss : 0.325492553448288\n",
      "iterations 6214 accuracy : 0.9304767905902122  loss : 0.3254846594746864\n",
      "iterations 6215 accuracy : 0.9304767905902122  loss : 0.3254770264678777\n",
      "iterations 6216 accuracy : 0.9304767905902122  loss : 0.325469247018686\n",
      "iterations 6217 accuracy : 0.9302667506826297  loss : 0.325460719674632\n",
      "iterations 6218 accuracy : 0.9302667506826297  loss : 0.3254524690241827\n",
      "iterations 6219 accuracy : 0.9302667506826297  loss : 0.32544401049579935\n",
      "iterations 6220 accuracy : 0.9302667506826297  loss : 0.32543552387433305\n",
      "iterations 6221 accuracy : 0.9302667506826297  loss : 0.32542751088713284\n",
      "iterations 6222 accuracy : 0.9302667506826297  loss : 0.325419153473116\n",
      "iterations 6223 accuracy : 0.9302667506826297  loss : 0.32541155290962787\n",
      "iterations 6224 accuracy : 0.9302667506826297  loss : 0.3254035011464144\n",
      "iterations 6225 accuracy : 0.9302667506826297  loss : 0.3253954363283229\n",
      "iterations 6226 accuracy : 0.9302667506826297  loss : 0.32538669748787\n",
      "iterations 6227 accuracy : 0.9302667506826297  loss : 0.32537994862926045\n",
      "iterations 6228 accuracy : 0.9302667506826297  loss : 0.3253716481405112\n",
      "iterations 6229 accuracy : 0.9302667506826297  loss : 0.32536299329553253\n",
      "iterations 6230 accuracy : 0.9302667506826297  loss : 0.32535503926668513\n",
      "iterations 6231 accuracy : 0.9302667506826297  loss : 0.3253467722673571\n",
      "iterations 6232 accuracy : 0.9302667506826297  loss : 0.3253388804069405\n",
      "iterations 6233 accuracy : 0.9302667506826297  loss : 0.32533047013448113\n",
      "iterations 6234 accuracy : 0.9302667506826297  loss : 0.3253228059322864\n",
      "iterations 6235 accuracy : 0.9302667506826297  loss : 0.3253140794889699\n",
      "iterations 6236 accuracy : 0.9302667506826297  loss : 0.32530636010372904\n",
      "iterations 6237 accuracy : 0.9302667506826297  loss : 0.32529791676362285\n",
      "iterations 6238 accuracy : 0.9302667506826297  loss : 0.32528983482224616\n",
      "iterations 6239 accuracy : 0.9302667506826297  loss : 0.3252819397173063\n",
      "iterations 6240 accuracy : 0.9302667506826297  loss : 0.3252739464248825\n",
      "iterations 6241 accuracy : 0.9302667506826297  loss : 0.3252656620687813\n",
      "iterations 6242 accuracy : 0.9302667506826297  loss : 0.32525731204542535\n",
      "iterations 6243 accuracy : 0.9302667506826297  loss : 0.3252492371185578\n",
      "iterations 6244 accuracy : 0.9302667506826297  loss : 0.32524155913036584\n",
      "iterations 6245 accuracy : 0.9302667506826297  loss : 0.32523324178708185\n",
      "iterations 6246 accuracy : 0.9302667506826297  loss : 0.32522549058537953\n",
      "iterations 6247 accuracy : 0.9302667506826297  loss : 0.3252175116971606\n",
      "iterations 6248 accuracy : 0.9302667506826297  loss : 0.3252102111887244\n",
      "iterations 6249 accuracy : 0.9302667506826297  loss : 0.3252017452889764\n",
      "iterations 6250 accuracy : 0.9302667506826297  loss : 0.3251937058169172\n",
      "iterations 6251 accuracy : 0.9302667506826297  loss : 0.32518532045784654\n",
      "iterations 6252 accuracy : 0.9302667506826297  loss : 0.3251776488261542\n",
      "iterations 6253 accuracy : 0.9302667506826297  loss : 0.325170462124245\n",
      "iterations 6254 accuracy : 0.9302667506826297  loss : 0.325162722635199\n",
      "iterations 6255 accuracy : 0.9302667506826297  loss : 0.32515432265692507\n",
      "iterations 6256 accuracy : 0.9302667506826297  loss : 0.3251463778777271\n",
      "iterations 6257 accuracy : 0.9302667506826297  loss : 0.32513868696625775\n",
      "iterations 6258 accuracy : 0.9302667506826297  loss : 0.32513096990733725\n",
      "iterations 6259 accuracy : 0.9302667506826297  loss : 0.32512310939612354\n",
      "iterations 6260 accuracy : 0.9302667506826297  loss : 0.3251156758619327\n",
      "iterations 6261 accuracy : 0.9302667506826297  loss : 0.32510747718753547\n",
      "iterations 6262 accuracy : 0.9302667506826297  loss : 0.325100212923251\n",
      "iterations 6263 accuracy : 0.9302667506826297  loss : 0.3250934116964326\n",
      "iterations 6264 accuracy : 0.9302667506826297  loss : 0.3250847715065395\n",
      "iterations 6265 accuracy : 0.9302667506826297  loss : 0.3250770092067396\n",
      "iterations 6266 accuracy : 0.9302667506826297  loss : 0.3250694390573286\n",
      "iterations 6267 accuracy : 0.9302667506826297  loss : 0.3250615254808857\n",
      "iterations 6268 accuracy : 0.9302667506826297  loss : 0.32505302931069624\n",
      "iterations 6269 accuracy : 0.9302667506826297  loss : 0.3250450469836952\n",
      "iterations 6270 accuracy : 0.9302667506826297  loss : 0.32503786190223316\n",
      "iterations 6271 accuracy : 0.9302667506826297  loss : 0.3250299411013229\n",
      "iterations 6272 accuracy : 0.9302667506826297  loss : 0.3250217019856807\n",
      "iterations 6273 accuracy : 0.9302667506826297  loss : 0.3250139157698799\n",
      "iterations 6274 accuracy : 0.9302667506826297  loss : 0.32500590637855975\n",
      "iterations 6275 accuracy : 0.9302667506826297  loss : 0.3249984646909415\n",
      "iterations 6276 accuracy : 0.9302667506826297  loss : 0.3249909261695621\n",
      "iterations 6277 accuracy : 0.9302667506826297  loss : 0.3249838471299357\n",
      "iterations 6278 accuracy : 0.9302667506826297  loss : 0.3249765295284903\n",
      "iterations 6279 accuracy : 0.9302667506826297  loss : 0.32496848698994846\n",
      "iterations 6280 accuracy : 0.9302667506826297  loss : 0.3249600086717778\n",
      "iterations 6281 accuracy : 0.9302667506826297  loss : 0.32495226983575454\n",
      "iterations 6282 accuracy : 0.9302667506826297  loss : 0.32494429232370675\n",
      "iterations 6283 accuracy : 0.9302667506826297  loss : 0.3249367164212552\n",
      "iterations 6284 accuracy : 0.9302667506826297  loss : 0.32492917453172654\n",
      "iterations 6285 accuracy : 0.9302667506826297  loss : 0.32492149511546764\n",
      "iterations 6286 accuracy : 0.9302667506826297  loss : 0.3249136952285492\n",
      "iterations 6287 accuracy : 0.9302667506826297  loss : 0.32490511746941125\n",
      "iterations 6288 accuracy : 0.9302667506826297  loss : 0.32489745238119\n",
      "iterations 6289 accuracy : 0.9302667506826297  loss : 0.32489017155509586\n",
      "iterations 6290 accuracy : 0.9302667506826297  loss : 0.3248826836031337\n",
      "iterations 6291 accuracy : 0.9302667506826297  loss : 0.3248747428206632\n",
      "iterations 6292 accuracy : 0.9302667506826297  loss : 0.32486663580336134\n",
      "iterations 6293 accuracy : 0.9302667506826297  loss : 0.32485998207857825\n",
      "iterations 6294 accuracy : 0.9302667506826297  loss : 0.3248525741492701\n",
      "iterations 6295 accuracy : 0.9302667506826297  loss : 0.3248441524907521\n",
      "iterations 6296 accuracy : 0.9302667506826297  loss : 0.32483598991332663\n",
      "iterations 6297 accuracy : 0.9302667506826297  loss : 0.3248277854565372\n",
      "iterations 6298 accuracy : 0.9302667506826297  loss : 0.3248195297198619\n",
      "iterations 6299 accuracy : 0.9302667506826297  loss : 0.3248115795608957\n",
      "iterations 6300 accuracy : 0.9302667506826297  loss : 0.324803795133641\n",
      "iterations 6301 accuracy : 0.9302667506826297  loss : 0.3247973237187564\n",
      "iterations 6302 accuracy : 0.9302667506826297  loss : 0.32478945581977037\n",
      "iterations 6303 accuracy : 0.9302667506826297  loss : 0.32478192299890274\n",
      "iterations 6304 accuracy : 0.9302667506826297  loss : 0.3247744198634094\n",
      "iterations 6305 accuracy : 0.9302667506826297  loss : 0.32476708381900515\n",
      "iterations 6306 accuracy : 0.9302667506826297  loss : 0.3247592072470714\n",
      "iterations 6307 accuracy : 0.9302667506826297  loss : 0.3247508518962484\n",
      "iterations 6308 accuracy : 0.9302667506826297  loss : 0.32474312262511973\n",
      "iterations 6309 accuracy : 0.9302667506826297  loss : 0.32473494630815725\n",
      "iterations 6310 accuracy : 0.9302667506826297  loss : 0.3247272663131475\n",
      "iterations 6311 accuracy : 0.9302667506826297  loss : 0.32471898117055426\n",
      "iterations 6312 accuracy : 0.9302667506826297  loss : 0.3247115250545832\n",
      "iterations 6313 accuracy : 0.9302667506826297  loss : 0.32470402166926626\n",
      "iterations 6314 accuracy : 0.9302667506826297  loss : 0.3246960836819981\n",
      "iterations 6315 accuracy : 0.9302667506826297  loss : 0.3246885855498822\n",
      "iterations 6316 accuracy : 0.9302667506826297  loss : 0.32467992894372694\n",
      "iterations 6317 accuracy : 0.9302667506826297  loss : 0.32467241689363374\n",
      "iterations 6318 accuracy : 0.9302667506826297  loss : 0.3246649237062501\n",
      "iterations 6319 accuracy : 0.9302667506826297  loss : 0.324657250682014\n",
      "iterations 6320 accuracy : 0.9302667506826297  loss : 0.3246491675382761\n",
      "iterations 6321 accuracy : 0.9302667506826297  loss : 0.3246415730647615\n",
      "iterations 6322 accuracy : 0.9302667506826297  loss : 0.32463370437224537\n",
      "iterations 6323 accuracy : 0.9302667506826297  loss : 0.32462553559798857\n",
      "iterations 6324 accuracy : 0.9302667506826297  loss : 0.3246174750464423\n",
      "iterations 6325 accuracy : 0.9302667506826297  loss : 0.3246094959312045\n",
      "iterations 6326 accuracy : 0.9302667506826297  loss : 0.3246017201590702\n",
      "iterations 6327 accuracy : 0.9302667506826297  loss : 0.32459459836374094\n",
      "iterations 6328 accuracy : 0.9302667506826297  loss : 0.32458697904851885\n",
      "iterations 6329 accuracy : 0.9302667506826297  loss : 0.3245795114744592\n",
      "iterations 6330 accuracy : 0.9304767905902122  loss : 0.3245725849875788\n",
      "iterations 6331 accuracy : 0.9302667506826297  loss : 0.32456489941955924\n",
      "iterations 6332 accuracy : 0.9302667506826297  loss : 0.32455652122382816\n",
      "iterations 6333 accuracy : 0.9302667506826297  loss : 0.32454847229017847\n",
      "iterations 6334 accuracy : 0.9302667506826297  loss : 0.32453971561486533\n",
      "iterations 6335 accuracy : 0.9302667506826297  loss : 0.32453194684804176\n",
      "iterations 6336 accuracy : 0.9302667506826297  loss : 0.32452397550580175\n",
      "iterations 6337 accuracy : 0.9302667506826297  loss : 0.324516281337566\n",
      "iterations 6338 accuracy : 0.9302667506826297  loss : 0.32450833162117476\n",
      "iterations 6339 accuracy : 0.9302667506826297  loss : 0.3245009297160724\n",
      "iterations 6340 accuracy : 0.9302667506826297  loss : 0.32449280390762797\n",
      "iterations 6341 accuracy : 0.9302667506826297  loss : 0.3244844918295319\n",
      "iterations 6342 accuracy : 0.9302667506826297  loss : 0.3244770684783912\n",
      "iterations 6343 accuracy : 0.9302667506826297  loss : 0.32446960202784286\n",
      "iterations 6344 accuracy : 0.9302667506826297  loss : 0.32446156978813423\n",
      "iterations 6345 accuracy : 0.9302667506826297  loss : 0.32445319111902937\n",
      "iterations 6346 accuracy : 0.9302667506826297  loss : 0.32444516933970513\n",
      "iterations 6347 accuracy : 0.9302667506826297  loss : 0.3244375523233197\n",
      "iterations 6348 accuracy : 0.9302667506826297  loss : 0.32442978677082324\n",
      "iterations 6349 accuracy : 0.9302667506826297  loss : 0.32442160567476286\n",
      "iterations 6350 accuracy : 0.9302667506826297  loss : 0.3244138400304242\n",
      "iterations 6351 accuracy : 0.9302667506826297  loss : 0.32440583150218016\n",
      "iterations 6352 accuracy : 0.9302667506826297  loss : 0.3243978586463594\n",
      "iterations 6353 accuracy : 0.9302667506826297  loss : 0.32439004148982065\n",
      "iterations 6354 accuracy : 0.9302667506826297  loss : 0.3243826158579704\n",
      "iterations 6355 accuracy : 0.9302667506826297  loss : 0.324375758825309\n",
      "iterations 6356 accuracy : 0.9302667506826297  loss : 0.32436791289206396\n",
      "iterations 6357 accuracy : 0.9302667506826297  loss : 0.3243604991721831\n",
      "iterations 6358 accuracy : 0.9302667506826297  loss : 0.3243529255282777\n",
      "iterations 6359 accuracy : 0.9302667506826297  loss : 0.32434497242680826\n",
      "iterations 6360 accuracy : 0.9302667506826297  loss : 0.3243377643045997\n",
      "iterations 6361 accuracy : 0.9302667506826297  loss : 0.32432984282203037\n",
      "iterations 6362 accuracy : 0.9302667506826297  loss : 0.32432200193819416\n",
      "iterations 6363 accuracy : 0.9302667506826297  loss : 0.3243141273797683\n",
      "iterations 6364 accuracy : 0.9302667506826297  loss : 0.32430613285611637\n",
      "iterations 6365 accuracy : 0.9302667506826297  loss : 0.3242977084163\n",
      "iterations 6366 accuracy : 0.9302667506826297  loss : 0.3242904136908572\n",
      "iterations 6367 accuracy : 0.9302667506826297  loss : 0.32428298801709976\n",
      "iterations 6368 accuracy : 0.9302667506826297  loss : 0.324274376386445\n",
      "iterations 6369 accuracy : 0.9302667506826297  loss : 0.324267255958947\n",
      "iterations 6370 accuracy : 0.9302667506826297  loss : 0.32425951119379687\n",
      "iterations 6371 accuracy : 0.9302667506826297  loss : 0.3242522943733805\n",
      "iterations 6372 accuracy : 0.9302667506826297  loss : 0.3242447419799299\n",
      "iterations 6373 accuracy : 0.9302667506826297  loss : 0.32423735007789395\n",
      "iterations 6374 accuracy : 0.9302667506826297  loss : 0.32422966300158673\n",
      "iterations 6375 accuracy : 0.9302667506826297  loss : 0.3242217070116468\n",
      "iterations 6376 accuracy : 0.9302667506826297  loss : 0.32421407603782454\n",
      "iterations 6377 accuracy : 0.9302667506826297  loss : 0.3242067796108794\n",
      "iterations 6378 accuracy : 0.9302667506826297  loss : 0.32419880188054295\n",
      "iterations 6379 accuracy : 0.9302667506826297  loss : 0.32419074483874505\n",
      "iterations 6380 accuracy : 0.9302667506826297  loss : 0.32418213657579287\n",
      "iterations 6381 accuracy : 0.9302667506826297  loss : 0.3241743291962366\n",
      "iterations 6382 accuracy : 0.9302667506826297  loss : 0.32416631822926967\n",
      "iterations 6383 accuracy : 0.9302667506826297  loss : 0.32415848323301594\n",
      "iterations 6384 accuracy : 0.9302667506826297  loss : 0.32415082877711204\n",
      "iterations 6385 accuracy : 0.9302667506826297  loss : 0.3241428649918711\n",
      "iterations 6386 accuracy : 0.9302667506826297  loss : 0.324135413703842\n",
      "iterations 6387 accuracy : 0.9302667506826297  loss : 0.3241270628559584\n",
      "iterations 6388 accuracy : 0.9302667506826297  loss : 0.3241190761813472\n",
      "iterations 6389 accuracy : 0.9302667506826297  loss : 0.3241106061585814\n",
      "iterations 6390 accuracy : 0.9302667506826297  loss : 0.32410274203472467\n",
      "iterations 6391 accuracy : 0.9302667506826297  loss : 0.3240948149324435\n",
      "iterations 6392 accuracy : 0.9302667506826297  loss : 0.3240859664923613\n",
      "iterations 6393 accuracy : 0.9302667506826297  loss : 0.3240779481768973\n",
      "iterations 6394 accuracy : 0.9302667506826297  loss : 0.32407102505872565\n",
      "iterations 6395 accuracy : 0.9302667506826297  loss : 0.32406389619754516\n",
      "iterations 6396 accuracy : 0.9302667506826297  loss : 0.3240569728072513\n",
      "iterations 6397 accuracy : 0.9302667506826297  loss : 0.32404963326120423\n",
      "iterations 6398 accuracy : 0.9302667506826297  loss : 0.3240413991500771\n",
      "iterations 6399 accuracy : 0.9302667506826297  loss : 0.32403332485071173\n",
      "iterations 6400 accuracy : 0.9302667506826297  loss : 0.3240260373183352\n",
      "iterations 6401 accuracy : 0.9302667506826297  loss : 0.3240183479059167\n",
      "iterations 6402 accuracy : 0.9302667506826297  loss : 0.32401026592789683\n",
      "iterations 6403 accuracy : 0.9302667506826297  loss : 0.324002616003823\n",
      "iterations 6404 accuracy : 0.9302667506826297  loss : 0.3239946187839051\n",
      "iterations 6405 accuracy : 0.9302667506826297  loss : 0.32398640687984137\n",
      "iterations 6406 accuracy : 0.9302667506826297  loss : 0.32397834144094784\n",
      "iterations 6407 accuracy : 0.9302667506826297  loss : 0.3239698873484375\n",
      "iterations 6408 accuracy : 0.9302667506826297  loss : 0.32396253958570465\n",
      "iterations 6409 accuracy : 0.9302667506826297  loss : 0.32395498140671436\n",
      "iterations 6410 accuracy : 0.9302667506826297  loss : 0.3239475952555661\n",
      "iterations 6411 accuracy : 0.9302667506826297  loss : 0.3239402036105048\n",
      "iterations 6412 accuracy : 0.9302667506826297  loss : 0.3239325955368616\n",
      "iterations 6413 accuracy : 0.9302667506826297  loss : 0.3239241620224379\n",
      "iterations 6414 accuracy : 0.9302667506826297  loss : 0.3239171599056763\n",
      "iterations 6415 accuracy : 0.9302667506826297  loss : 0.3239091091424051\n",
      "iterations 6416 accuracy : 0.9300567107750473  loss : 0.32390111527474263\n",
      "iterations 6417 accuracy : 0.9302667506826297  loss : 0.3238939663161241\n",
      "iterations 6418 accuracy : 0.9300567107750473  loss : 0.32388631079837965\n",
      "iterations 6419 accuracy : 0.9302667506826297  loss : 0.323879029320756\n",
      "iterations 6420 accuracy : 0.9302667506826297  loss : 0.32387135327809324\n",
      "iterations 6421 accuracy : 0.9300567107750473  loss : 0.3238633163473669\n",
      "iterations 6422 accuracy : 0.9300567107750473  loss : 0.32385570500701066\n",
      "iterations 6423 accuracy : 0.9300567107750473  loss : 0.32384819645920415\n",
      "iterations 6424 accuracy : 0.9300567107750473  loss : 0.32384087456698823\n",
      "iterations 6425 accuracy : 0.9300567107750473  loss : 0.3238336192075405\n",
      "iterations 6426 accuracy : 0.9302667506826297  loss : 0.32382631781810234\n",
      "iterations 6427 accuracy : 0.9300567107750473  loss : 0.3238188228925178\n",
      "iterations 6428 accuracy : 0.9300567107750473  loss : 0.3238112498195268\n",
      "iterations 6429 accuracy : 0.9300567107750473  loss : 0.32380411044840873\n",
      "iterations 6430 accuracy : 0.9300567107750473  loss : 0.3237966929940628\n",
      "iterations 6431 accuracy : 0.9302667506826297  loss : 0.32378988222528043\n",
      "iterations 6432 accuracy : 0.9302667506826297  loss : 0.3237826259590614\n",
      "iterations 6433 accuracy : 0.9302667506826297  loss : 0.3237752468886043\n",
      "iterations 6434 accuracy : 0.9300567107750473  loss : 0.3237672706464748\n",
      "iterations 6435 accuracy : 0.9302667506826297  loss : 0.32376007565450865\n",
      "iterations 6436 accuracy : 0.9302667506826297  loss : 0.3237528245595784\n",
      "iterations 6437 accuracy : 0.9302667506826297  loss : 0.3237457855971908\n",
      "iterations 6438 accuracy : 0.9302667506826297  loss : 0.3237384451575068\n",
      "iterations 6439 accuracy : 0.9302667506826297  loss : 0.32373110190372967\n",
      "iterations 6440 accuracy : 0.9302667506826297  loss : 0.32372363309398866\n",
      "iterations 6441 accuracy : 0.9302667506826297  loss : 0.32371603663062637\n",
      "iterations 6442 accuracy : 0.9302667506826297  loss : 0.32370938083787965\n",
      "iterations 6443 accuracy : 0.9302667506826297  loss : 0.3237023995878894\n",
      "iterations 6444 accuracy : 0.9302667506826297  loss : 0.3236947116120959\n",
      "iterations 6445 accuracy : 0.9302667506826297  loss : 0.3236869620826477\n",
      "iterations 6446 accuracy : 0.9302667506826297  loss : 0.32368004946261036\n",
      "iterations 6447 accuracy : 0.9302667506826297  loss : 0.3236720581702739\n",
      "iterations 6448 accuracy : 0.9302667506826297  loss : 0.32366459681468807\n",
      "iterations 6449 accuracy : 0.9302667506826297  loss : 0.3236575109368339\n",
      "iterations 6450 accuracy : 0.9302667506826297  loss : 0.32364977721647575\n",
      "iterations 6451 accuracy : 0.9302667506826297  loss : 0.3236425856157716\n",
      "iterations 6452 accuracy : 0.9302667506826297  loss : 0.32363496025990823\n",
      "iterations 6453 accuracy : 0.9302667506826297  loss : 0.32362749108995986\n",
      "iterations 6454 accuracy : 0.9302667506826297  loss : 0.3236190270979198\n",
      "iterations 6455 accuracy : 0.9302667506826297  loss : 0.3236113594832981\n",
      "iterations 6456 accuracy : 0.9302667506826297  loss : 0.3236039061959092\n",
      "iterations 6457 accuracy : 0.9302667506826297  loss : 0.3235969629690833\n",
      "iterations 6458 accuracy : 0.9302667506826297  loss : 0.3235897893291561\n",
      "iterations 6459 accuracy : 0.9302667506826297  loss : 0.3235822314280409\n",
      "iterations 6460 accuracy : 0.9302667506826297  loss : 0.3235738475906658\n",
      "iterations 6461 accuracy : 0.9302667506826297  loss : 0.3235664174931087\n",
      "iterations 6462 accuracy : 0.9302667506826297  loss : 0.3235587974968875\n",
      "iterations 6463 accuracy : 0.9302667506826297  loss : 0.32355160271527356\n",
      "iterations 6464 accuracy : 0.9302667506826297  loss : 0.32354383379779966\n",
      "iterations 6465 accuracy : 0.9302667506826297  loss : 0.32353623800186826\n",
      "iterations 6466 accuracy : 0.9302667506826297  loss : 0.32352858813106194\n",
      "iterations 6467 accuracy : 0.9302667506826297  loss : 0.32352077820140634\n",
      "iterations 6468 accuracy : 0.9302667506826297  loss : 0.3235138521185164\n",
      "iterations 6469 accuracy : 0.9302667506826297  loss : 0.32350675036755944\n",
      "iterations 6470 accuracy : 0.9302667506826297  loss : 0.32349966733959956\n",
      "iterations 6471 accuracy : 0.9302667506826297  loss : 0.3234921481406612\n",
      "iterations 6472 accuracy : 0.9302667506826297  loss : 0.3234851338800536\n",
      "iterations 6473 accuracy : 0.9302667506826297  loss : 0.3234779238529718\n",
      "iterations 6474 accuracy : 0.9302667506826297  loss : 0.32347040479252287\n",
      "iterations 6475 accuracy : 0.9302667506826297  loss : 0.3234626955642596\n",
      "iterations 6476 accuracy : 0.9302667506826297  loss : 0.3234554642171341\n",
      "iterations 6477 accuracy : 0.9302667506826297  loss : 0.32344786318187885\n",
      "iterations 6478 accuracy : 0.9302667506826297  loss : 0.32343978088993147\n",
      "iterations 6479 accuracy : 0.9302667506826297  loss : 0.3234327098060367\n",
      "iterations 6480 accuracy : 0.9302667506826297  loss : 0.3234251095061009\n",
      "iterations 6481 accuracy : 0.9302667506826297  loss : 0.3234176094147715\n",
      "iterations 6482 accuracy : 0.9302667506826297  loss : 0.3234102973872404\n",
      "iterations 6483 accuracy : 0.9302667506826297  loss : 0.32340272183429136\n",
      "iterations 6484 accuracy : 0.9302667506826297  loss : 0.32339548528339607\n",
      "iterations 6485 accuracy : 0.9302667506826297  loss : 0.32338733943271264\n",
      "iterations 6486 accuracy : 0.9302667506826297  loss : 0.32337995926713636\n",
      "iterations 6487 accuracy : 0.9302667506826297  loss : 0.3233719872178213\n",
      "iterations 6488 accuracy : 0.9302667506826297  loss : 0.323364232274932\n",
      "iterations 6489 accuracy : 0.9302667506826297  loss : 0.32335692200328486\n",
      "iterations 6490 accuracy : 0.9302667506826297  loss : 0.32334987155517897\n",
      "iterations 6491 accuracy : 0.9302667506826297  loss : 0.3233423330924743\n",
      "iterations 6492 accuracy : 0.9302667506826297  loss : 0.32333465246094184\n",
      "iterations 6493 accuracy : 0.9302667506826297  loss : 0.3233271344495242\n",
      "iterations 6494 accuracy : 0.9302667506826297  loss : 0.32331956980116333\n",
      "iterations 6495 accuracy : 0.9302667506826297  loss : 0.3233119486047178\n",
      "iterations 6496 accuracy : 0.9302667506826297  loss : 0.3233044340224764\n",
      "iterations 6497 accuracy : 0.9302667506826297  loss : 0.32329690190853766\n",
      "iterations 6498 accuracy : 0.9300567107750473  loss : 0.32328892249297186\n",
      "iterations 6499 accuracy : 0.9300567107750473  loss : 0.3232814692340244\n",
      "iterations 6500 accuracy : 0.9300567107750473  loss : 0.323273900737848\n",
      "iterations 6501 accuracy : 0.9300567107750473  loss : 0.323266021235493\n",
      "iterations 6502 accuracy : 0.9300567107750473  loss : 0.32325883412561407\n",
      "iterations 6503 accuracy : 0.9300567107750473  loss : 0.3232514015157725\n",
      "iterations 6504 accuracy : 0.9300567107750473  loss : 0.3232441599769197\n",
      "iterations 6505 accuracy : 0.9302667506826297  loss : 0.3232368889739925\n",
      "iterations 6506 accuracy : 0.9302667506826297  loss : 0.32322927940406176\n",
      "iterations 6507 accuracy : 0.9300567107750473  loss : 0.3232216437399403\n",
      "iterations 6508 accuracy : 0.9300567107750473  loss : 0.3232135612710134\n",
      "iterations 6509 accuracy : 0.9300567107750473  loss : 0.32320594748912823\n",
      "iterations 6510 accuracy : 0.9300567107750473  loss : 0.3231978183785888\n",
      "iterations 6511 accuracy : 0.9300567107750473  loss : 0.32319099236855753\n",
      "iterations 6512 accuracy : 0.9300567107750473  loss : 0.3231838184757833\n",
      "iterations 6513 accuracy : 0.9300567107750473  loss : 0.3231769245137286\n",
      "iterations 6514 accuracy : 0.9300567107750473  loss : 0.32316958877202306\n",
      "iterations 6515 accuracy : 0.9298466708674649  loss : 0.32316187883649383\n",
      "iterations 6516 accuracy : 0.9300567107750473  loss : 0.32315445826676914\n",
      "iterations 6517 accuracy : 0.9300567107750473  loss : 0.3231472298580735\n",
      "iterations 6518 accuracy : 0.9300567107750473  loss : 0.3231402356214864\n",
      "iterations 6519 accuracy : 0.9300567107750473  loss : 0.3231329058466444\n",
      "iterations 6520 accuracy : 0.9298466708674649  loss : 0.3231253951216855\n",
      "iterations 6521 accuracy : 0.9300567107750473  loss : 0.32311833364352466\n",
      "iterations 6522 accuracy : 0.9300567107750473  loss : 0.32311126194996465\n",
      "iterations 6523 accuracy : 0.9300567107750473  loss : 0.32310418225550863\n",
      "iterations 6524 accuracy : 0.9302667506826297  loss : 0.32309744001466245\n",
      "iterations 6525 accuracy : 0.9302667506826297  loss : 0.3230900653715247\n",
      "iterations 6526 accuracy : 0.9300567107750473  loss : 0.3230823526467014\n",
      "iterations 6527 accuracy : 0.9300567107750473  loss : 0.32307489401235034\n",
      "iterations 6528 accuracy : 0.9300567107750473  loss : 0.32306746444244266\n",
      "iterations 6529 accuracy : 0.9300567107750473  loss : 0.3230602307445325\n",
      "iterations 6530 accuracy : 0.9300567107750473  loss : 0.32305287300153207\n",
      "iterations 6531 accuracy : 0.9300567107750473  loss : 0.32304540313859803\n",
      "iterations 6532 accuracy : 0.9300567107750473  loss : 0.32303821987775394\n",
      "iterations 6533 accuracy : 0.9300567107750473  loss : 0.3230310401148787\n",
      "iterations 6534 accuracy : 0.9300567107750473  loss : 0.3230234737351689\n",
      "iterations 6535 accuracy : 0.9300567107750473  loss : 0.32301653070299874\n",
      "iterations 6536 accuracy : 0.9300567107750473  loss : 0.3230091889397389\n",
      "iterations 6537 accuracy : 0.9300567107750473  loss : 0.32300208178025824\n",
      "iterations 6538 accuracy : 0.9302667506826297  loss : 0.3229952894543318\n",
      "iterations 6539 accuracy : 0.9302667506826297  loss : 0.32298788996739497\n",
      "iterations 6540 accuracy : 0.9302667506826297  loss : 0.32298083286806917\n",
      "iterations 6541 accuracy : 0.9302667506826297  loss : 0.32297396585173666\n",
      "iterations 6542 accuracy : 0.9302667506826297  loss : 0.3229665714556914\n",
      "iterations 6543 accuracy : 0.9302667506826297  loss : 0.32295824717330174\n",
      "iterations 6544 accuracy : 0.9302667506826297  loss : 0.3229516769824371\n",
      "iterations 6545 accuracy : 0.9302667506826297  loss : 0.322944472396518\n",
      "iterations 6546 accuracy : 0.9302667506826297  loss : 0.32293760544902045\n",
      "iterations 6547 accuracy : 0.9302667506826297  loss : 0.322929513649467\n",
      "iterations 6548 accuracy : 0.9302667506826297  loss : 0.32292231512195146\n",
      "iterations 6549 accuracy : 0.9300567107750473  loss : 0.32291414055802925\n",
      "iterations 6550 accuracy : 0.9300567107750473  loss : 0.3229069349226257\n",
      "iterations 6551 accuracy : 0.9302667506826297  loss : 0.3229000306067498\n",
      "iterations 6552 accuracy : 0.9298466708674649  loss : 0.32289178073231023\n",
      "iterations 6553 accuracy : 0.9300567107750473  loss : 0.32288464332235506\n",
      "iterations 6554 accuracy : 0.9302667506826297  loss : 0.32287795557101723\n",
      "iterations 6555 accuracy : 0.9302667506826297  loss : 0.3228715168479212\n",
      "iterations 6556 accuracy : 0.9302667506826297  loss : 0.32286332031392195\n",
      "iterations 6557 accuracy : 0.9302667506826297  loss : 0.3228561169318268\n",
      "iterations 6558 accuracy : 0.9302667506826297  loss : 0.322848892517182\n",
      "iterations 6559 accuracy : 0.9302667506826297  loss : 0.32284251378134604\n",
      "iterations 6560 accuracy : 0.9302667506826297  loss : 0.3228345743722727\n",
      "iterations 6561 accuracy : 0.9302667506826297  loss : 0.32282708195796966\n",
      "iterations 6562 accuracy : 0.9302667506826297  loss : 0.3228203794882338\n",
      "iterations 6563 accuracy : 0.9302667506826297  loss : 0.3228126317596833\n",
      "iterations 6564 accuracy : 0.9302667506826297  loss : 0.32280548398256215\n",
      "iterations 6565 accuracy : 0.9302667506826297  loss : 0.3227977812407501\n",
      "iterations 6566 accuracy : 0.9302667506826297  loss : 0.32279069843785557\n",
      "iterations 6567 accuracy : 0.9302667506826297  loss : 0.3227834068499539\n",
      "iterations 6568 accuracy : 0.9298466708674649  loss : 0.3227750962802324\n",
      "iterations 6569 accuracy : 0.9302667506826297  loss : 0.32276825069005777\n",
      "iterations 6570 accuracy : 0.9298466708674649  loss : 0.322760404783101\n",
      "iterations 6571 accuracy : 0.9298466708674649  loss : 0.32275263677000593\n",
      "iterations 6572 accuracy : 0.9298466708674649  loss : 0.3227453241857098\n",
      "iterations 6573 accuracy : 0.9298466708674649  loss : 0.32273824767677683\n",
      "iterations 6574 accuracy : 0.9298466708674649  loss : 0.32273026140965727\n",
      "iterations 6575 accuracy : 0.9298466708674649  loss : 0.322722770447523\n",
      "iterations 6576 accuracy : 0.9300567107750473  loss : 0.32271491317404205\n",
      "iterations 6577 accuracy : 0.9300567107750473  loss : 0.32270788027051733\n",
      "iterations 6578 accuracy : 0.9300567107750473  loss : 0.32270031266844845\n",
      "iterations 6579 accuracy : 0.9298466708674649  loss : 0.32269365754544976\n",
      "iterations 6580 accuracy : 0.9300567107750473  loss : 0.322686217006546\n",
      "iterations 6581 accuracy : 0.9300567107750473  loss : 0.3226791204976102\n",
      "iterations 6582 accuracy : 0.9300567107750473  loss : 0.32267118459621624\n",
      "iterations 6583 accuracy : 0.9300567107750473  loss : 0.3226632892555456\n",
      "iterations 6584 accuracy : 0.9300567107750473  loss : 0.32265568646674364\n",
      "iterations 6585 accuracy : 0.9300567107750473  loss : 0.3226484961081296\n",
      "iterations 6586 accuracy : 0.9300567107750473  loss : 0.3226410567492232\n",
      "iterations 6587 accuracy : 0.9300567107750473  loss : 0.3226336372595816\n",
      "iterations 6588 accuracy : 0.9300567107750473  loss : 0.32262609160102\n",
      "iterations 6589 accuracy : 0.9300567107750473  loss : 0.3226188474717082\n",
      "iterations 6590 accuracy : 0.9300567107750473  loss : 0.3226110914185063\n",
      "iterations 6591 accuracy : 0.9300567107750473  loss : 0.3226040750203623\n",
      "iterations 6592 accuracy : 0.9300567107750473  loss : 0.3225969855413596\n",
      "iterations 6593 accuracy : 0.9300567107750473  loss : 0.32258992684958593\n",
      "iterations 6594 accuracy : 0.9300567107750473  loss : 0.3225824541824158\n",
      "iterations 6595 accuracy : 0.9300567107750473  loss : 0.32257528159667914\n",
      "iterations 6596 accuracy : 0.9300567107750473  loss : 0.3225679511361364\n",
      "iterations 6597 accuracy : 0.9300567107750473  loss : 0.3225614605299988\n",
      "iterations 6598 accuracy : 0.9300567107750473  loss : 0.32255436500970003\n",
      "iterations 6599 accuracy : 0.9300567107750473  loss : 0.32254622888510814\n",
      "iterations 6600 accuracy : 0.9300567107750473  loss : 0.3225382531454709\n",
      "iterations 6601 accuracy : 0.9300567107750473  loss : 0.3225310012033706\n",
      "iterations 6602 accuracy : 0.9300567107750473  loss : 0.3225239959194119\n",
      "iterations 6603 accuracy : 0.9300567107750473  loss : 0.3225174549043708\n",
      "iterations 6604 accuracy : 0.9300567107750473  loss : 0.3225100643099632\n",
      "iterations 6605 accuracy : 0.9300567107750473  loss : 0.3225027975676798\n",
      "iterations 6606 accuracy : 0.9298466708674649  loss : 0.3224952833834764\n",
      "iterations 6607 accuracy : 0.9298466708674649  loss : 0.32248778996527006\n",
      "iterations 6608 accuracy : 0.9298466708674649  loss : 0.3224808766438346\n",
      "iterations 6609 accuracy : 0.9300567107750473  loss : 0.3224738073159061\n",
      "iterations 6610 accuracy : 0.9298466708674649  loss : 0.32246616046159304\n",
      "iterations 6611 accuracy : 0.9298466708674649  loss : 0.32245900953954065\n",
      "iterations 6612 accuracy : 0.9298466708674649  loss : 0.32245170719490807\n",
      "iterations 6613 accuracy : 0.9298466708674649  loss : 0.32244434765263696\n",
      "iterations 6614 accuracy : 0.9298466708674649  loss : 0.32243720339845294\n",
      "iterations 6615 accuracy : 0.9298466708674649  loss : 0.3224296620224458\n",
      "iterations 6616 accuracy : 0.9298466708674649  loss : 0.3224222238009969\n",
      "iterations 6617 accuracy : 0.9298466708674649  loss : 0.3224148827077691\n",
      "iterations 6618 accuracy : 0.9298466708674649  loss : 0.3224073395185389\n",
      "iterations 6619 accuracy : 0.9298466708674649  loss : 0.322399931635161\n",
      "iterations 6620 accuracy : 0.9298466708674649  loss : 0.3223930693122985\n",
      "iterations 6621 accuracy : 0.9298466708674649  loss : 0.32238595662180536\n",
      "iterations 6622 accuracy : 0.9298466708674649  loss : 0.322378845326837\n",
      "iterations 6623 accuracy : 0.9298466708674649  loss : 0.32237251527895716\n",
      "iterations 6624 accuracy : 0.9298466708674649  loss : 0.3223645493005829\n",
      "iterations 6625 accuracy : 0.9298466708674649  loss : 0.3223577764384641\n",
      "iterations 6626 accuracy : 0.9298466708674649  loss : 0.3223513494315331\n",
      "iterations 6627 accuracy : 0.9298466708674649  loss : 0.3223441519108114\n",
      "iterations 6628 accuracy : 0.9298466708674649  loss : 0.3223366029871121\n",
      "iterations 6629 accuracy : 0.9298466708674649  loss : 0.32232997101817995\n",
      "iterations 6630 accuracy : 0.9298466708674649  loss : 0.3223228344980639\n",
      "iterations 6631 accuracy : 0.9300567107750473  loss : 0.3223161139325231\n",
      "iterations 6632 accuracy : 0.9298466708674649  loss : 0.32230817160457004\n",
      "iterations 6633 accuracy : 0.9298466708674649  loss : 0.32230068908485987\n",
      "iterations 6634 accuracy : 0.9298466708674649  loss : 0.32229369223497606\n",
      "iterations 6635 accuracy : 0.9298466708674649  loss : 0.32228653696935167\n",
      "iterations 6636 accuracy : 0.9298466708674649  loss : 0.3222789164412618\n",
      "iterations 6637 accuracy : 0.9298466708674649  loss : 0.32227124442070754\n",
      "iterations 6638 accuracy : 0.9298466708674649  loss : 0.32226432280698136\n",
      "iterations 6639 accuracy : 0.9298466708674649  loss : 0.3222576417520287\n",
      "iterations 6640 accuracy : 0.9298466708674649  loss : 0.322250646372284\n",
      "iterations 6641 accuracy : 0.9298466708674649  loss : 0.3222434879020087\n",
      "iterations 6642 accuracy : 0.9298466708674649  loss : 0.3222361221608001\n",
      "iterations 6643 accuracy : 0.9298466708674649  loss : 0.3222290470512261\n",
      "iterations 6644 accuracy : 0.9298466708674649  loss : 0.3222221566765814\n",
      "iterations 6645 accuracy : 0.9298466708674649  loss : 0.3222154287144564\n",
      "iterations 6646 accuracy : 0.9298466708674649  loss : 0.3222078517810682\n",
      "iterations 6647 accuracy : 0.9298466708674649  loss : 0.32220141551816883\n",
      "iterations 6648 accuracy : 0.9298466708674649  loss : 0.3221937403856729\n",
      "iterations 6649 accuracy : 0.9298466708674649  loss : 0.3221869050756227\n",
      "iterations 6650 accuracy : 0.9300567107750473  loss : 0.3221799305273355\n",
      "iterations 6651 accuracy : 0.9300567107750473  loss : 0.32217332173090074\n",
      "iterations 6652 accuracy : 0.9300567107750473  loss : 0.3221658676130276\n",
      "iterations 6653 accuracy : 0.9298466708674649  loss : 0.3221587726456997\n",
      "iterations 6654 accuracy : 0.9298466708674649  loss : 0.32215146786356846\n",
      "iterations 6655 accuracy : 0.9298466708674649  loss : 0.3221437067532233\n",
      "iterations 6656 accuracy : 0.9298466708674649  loss : 0.3221359736420432\n",
      "iterations 6657 accuracy : 0.9298466708674649  loss : 0.3221291178212512\n",
      "iterations 6658 accuracy : 0.9298466708674649  loss : 0.32212167895938365\n",
      "iterations 6659 accuracy : 0.9298466708674649  loss : 0.32211446104316843\n",
      "iterations 6660 accuracy : 0.9298466708674649  loss : 0.3221075805521093\n",
      "iterations 6661 accuracy : 0.9298466708674649  loss : 0.3221000498295067\n",
      "iterations 6662 accuracy : 0.9298466708674649  loss : 0.322093168543129\n",
      "iterations 6663 accuracy : 0.9298466708674649  loss : 0.3220860967470136\n",
      "iterations 6664 accuracy : 0.9298466708674649  loss : 0.32207820224867134\n",
      "iterations 6665 accuracy : 0.9298466708674649  loss : 0.3220706855426863\n",
      "iterations 6666 accuracy : 0.9298466708674649  loss : 0.3220632801552814\n",
      "iterations 6667 accuracy : 0.9298466708674649  loss : 0.32205640840405975\n",
      "iterations 6668 accuracy : 0.9298466708674649  loss : 0.3220496651337207\n",
      "iterations 6669 accuracy : 0.9298466708674649  loss : 0.32204225401631875\n",
      "iterations 6670 accuracy : 0.9298466708674649  loss : 0.32203494270337985\n",
      "iterations 6671 accuracy : 0.9298466708674649  loss : 0.3220277747610778\n",
      "iterations 6672 accuracy : 0.9298466708674649  loss : 0.3220207227316773\n",
      "iterations 6673 accuracy : 0.9298466708674649  loss : 0.32201338942667324\n",
      "iterations 6674 accuracy : 0.9298466708674649  loss : 0.32200695759628667\n",
      "iterations 6675 accuracy : 0.9298466708674649  loss : 0.3220003846803612\n",
      "iterations 6676 accuracy : 0.9298466708674649  loss : 0.3219927002794959\n",
      "iterations 6677 accuracy : 0.9298466708674649  loss : 0.3219854215575781\n",
      "iterations 6678 accuracy : 0.9298466708674649  loss : 0.32197857571641875\n",
      "iterations 6679 accuracy : 0.9298466708674649  loss : 0.3219715077040478\n",
      "iterations 6680 accuracy : 0.9298466708674649  loss : 0.3219643070581652\n",
      "iterations 6681 accuracy : 0.9298466708674649  loss : 0.3219576464878945\n",
      "iterations 6682 accuracy : 0.9298466708674649  loss : 0.32195011924850986\n",
      "iterations 6683 accuracy : 0.9298466708674649  loss : 0.3219427841703324\n",
      "iterations 6684 accuracy : 0.9298466708674649  loss : 0.3219360196506602\n",
      "iterations 6685 accuracy : 0.9298466708674649  loss : 0.32192867100983447\n",
      "iterations 6686 accuracy : 0.9298466708674649  loss : 0.32192139446150736\n",
      "iterations 6687 accuracy : 0.9298466708674649  loss : 0.3219139701435139\n",
      "iterations 6688 accuracy : 0.9298466708674649  loss : 0.32190689038128295\n",
      "iterations 6689 accuracy : 0.9298466708674649  loss : 0.32189953071513255\n",
      "iterations 6690 accuracy : 0.9298466708674649  loss : 0.3218928260572227\n",
      "iterations 6691 accuracy : 0.9298466708674649  loss : 0.32188611065383105\n",
      "iterations 6692 accuracy : 0.9298466708674649  loss : 0.3218788771326467\n",
      "iterations 6693 accuracy : 0.9298466708674649  loss : 0.3218722155334057\n",
      "iterations 6694 accuracy : 0.9298466708674649  loss : 0.3218650686481439\n",
      "iterations 6695 accuracy : 0.9298466708674649  loss : 0.32185775213748957\n",
      "iterations 6696 accuracy : 0.9298466708674649  loss : 0.3218507976797144\n",
      "iterations 6697 accuracy : 0.9298466708674649  loss : 0.32184356804506664\n",
      "iterations 6698 accuracy : 0.9298466708674649  loss : 0.3218357407031442\n",
      "iterations 6699 accuracy : 0.9298466708674649  loss : 0.32182836572914114\n",
      "iterations 6700 accuracy : 0.9298466708674649  loss : 0.32182176621723435\n",
      "iterations 6701 accuracy : 0.9298466708674649  loss : 0.3218146130437413\n",
      "iterations 6702 accuracy : 0.9298466708674649  loss : 0.3218073056421403\n",
      "iterations 6703 accuracy : 0.9298466708674649  loss : 0.3218006594841606\n",
      "iterations 6704 accuracy : 0.9298466708674649  loss : 0.32179396215582234\n",
      "iterations 6705 accuracy : 0.9298466708674649  loss : 0.32178644016009383\n",
      "iterations 6706 accuracy : 0.9298466708674649  loss : 0.32177990054107564\n",
      "iterations 6707 accuracy : 0.9298466708674649  loss : 0.3217727032157168\n",
      "iterations 6708 accuracy : 0.9298466708674649  loss : 0.32176479515770867\n",
      "iterations 6709 accuracy : 0.9298466708674649  loss : 0.32175820595772325\n",
      "iterations 6710 accuracy : 0.9298466708674649  loss : 0.32175143283471297\n",
      "iterations 6711 accuracy : 0.9298466708674649  loss : 0.32174445578441846\n",
      "iterations 6712 accuracy : 0.9298466708674649  loss : 0.32173725683443416\n",
      "iterations 6713 accuracy : 0.9298466708674649  loss : 0.3217300273356361\n",
      "iterations 6714 accuracy : 0.9298466708674649  loss : 0.3217231113935723\n",
      "iterations 6715 accuracy : 0.9298466708674649  loss : 0.3217163729225486\n",
      "iterations 6716 accuracy : 0.9298466708674649  loss : 0.3217097655874951\n",
      "iterations 6717 accuracy : 0.9298466708674649  loss : 0.3217029051331078\n",
      "iterations 6718 accuracy : 0.9298466708674649  loss : 0.3216960719672985\n",
      "iterations 6719 accuracy : 0.9298466708674649  loss : 0.3216892178098055\n",
      "iterations 6720 accuracy : 0.9298466708674649  loss : 0.3216816480292016\n",
      "iterations 6721 accuracy : 0.9298466708674649  loss : 0.3216749751125105\n",
      "iterations 6722 accuracy : 0.9298466708674649  loss : 0.3216674087617181\n",
      "iterations 6723 accuracy : 0.9298466708674649  loss : 0.3216603497490441\n",
      "iterations 6724 accuracy : 0.9298466708674649  loss : 0.32165316517545756\n",
      "iterations 6725 accuracy : 0.9298466708674649  loss : 0.3216463143247761\n",
      "iterations 6726 accuracy : 0.9298466708674649  loss : 0.3216391856483582\n",
      "iterations 6727 accuracy : 0.9298466708674649  loss : 0.32163242667851993\n",
      "iterations 6728 accuracy : 0.9298466708674649  loss : 0.32162547432925265\n",
      "iterations 6729 accuracy : 0.9298466708674649  loss : 0.321617845093123\n",
      "iterations 6730 accuracy : 0.9298466708674649  loss : 0.32161082969966853\n",
      "iterations 6731 accuracy : 0.9298466708674649  loss : 0.321603966020872\n",
      "iterations 6732 accuracy : 0.9298466708674649  loss : 0.32159677892074656\n",
      "iterations 6733 accuracy : 0.9298466708674649  loss : 0.32159017058222383\n",
      "iterations 6734 accuracy : 0.9298466708674649  loss : 0.32158300235287596\n",
      "iterations 6735 accuracy : 0.9298466708674649  loss : 0.321575526823139\n",
      "iterations 6736 accuracy : 0.9298466708674649  loss : 0.3215679832712962\n",
      "iterations 6737 accuracy : 0.9298466708674649  loss : 0.32156075485165525\n",
      "iterations 6738 accuracy : 0.9298466708674649  loss : 0.32155414632268253\n",
      "iterations 6739 accuracy : 0.9298466708674649  loss : 0.32154690710231354\n",
      "iterations 6740 accuracy : 0.9298466708674649  loss : 0.32153967979429166\n",
      "iterations 6741 accuracy : 0.9298466708674649  loss : 0.3215330099567647\n",
      "iterations 6742 accuracy : 0.9298466708674649  loss : 0.3215260940079093\n",
      "iterations 6743 accuracy : 0.9298466708674649  loss : 0.32151893742468046\n",
      "iterations 6744 accuracy : 0.9298466708674649  loss : 0.32151242321831847\n",
      "iterations 6745 accuracy : 0.9298466708674649  loss : 0.32150533992993896\n",
      "iterations 6746 accuracy : 0.9298466708674649  loss : 0.32149853307340764\n",
      "iterations 6747 accuracy : 0.9298466708674649  loss : 0.32149214095068746\n",
      "iterations 6748 accuracy : 0.9298466708674649  loss : 0.3214857256455437\n",
      "iterations 6749 accuracy : 0.9298466708674649  loss : 0.32147924196015254\n",
      "iterations 6750 accuracy : 0.9298466708674649  loss : 0.3214725979616906\n",
      "iterations 6751 accuracy : 0.9298466708674649  loss : 0.3214652803781105\n",
      "iterations 6752 accuracy : 0.9298466708674649  loss : 0.3214585786042722\n",
      "iterations 6753 accuracy : 0.9298466708674649  loss : 0.3214511313623605\n",
      "iterations 6754 accuracy : 0.9298466708674649  loss : 0.3214446065203447\n",
      "iterations 6755 accuracy : 0.9298466708674649  loss : 0.3214369503080717\n",
      "iterations 6756 accuracy : 0.9298466708674649  loss : 0.32143043321791914\n",
      "iterations 6757 accuracy : 0.9298466708674649  loss : 0.32142351702596694\n",
      "iterations 6758 accuracy : 0.9298466708674649  loss : 0.321416786562166\n",
      "iterations 6759 accuracy : 0.9298466708674649  loss : 0.3214095687832683\n",
      "iterations 6760 accuracy : 0.9298466708674649  loss : 0.32140331472382205\n",
      "iterations 6761 accuracy : 0.9298466708674649  loss : 0.32139692350508636\n",
      "iterations 6762 accuracy : 0.9298466708674649  loss : 0.3213903730237662\n",
      "iterations 6763 accuracy : 0.9298466708674649  loss : 0.32138347787218896\n",
      "iterations 6764 accuracy : 0.9298466708674649  loss : 0.3213763099481915\n",
      "iterations 6765 accuracy : 0.9298466708674649  loss : 0.3213696138362811\n",
      "iterations 6766 accuracy : 0.9298466708674649  loss : 0.3213631537160624\n",
      "iterations 6767 accuracy : 0.9298466708674649  loss : 0.32135596295497815\n",
      "iterations 6768 accuracy : 0.9298466708674649  loss : 0.3213492815944679\n",
      "iterations 6769 accuracy : 0.9298466708674649  loss : 0.3213426427624448\n",
      "iterations 6770 accuracy : 0.9298466708674649  loss : 0.3213358122625404\n",
      "iterations 6771 accuracy : 0.9298466708674649  loss : 0.3213290555107668\n",
      "iterations 6772 accuracy : 0.9298466708674649  loss : 0.3213219788864462\n",
      "iterations 6773 accuracy : 0.9298466708674649  loss : 0.3213147936674859\n",
      "iterations 6774 accuracy : 0.9298466708674649  loss : 0.32130821429129536\n",
      "iterations 6775 accuracy : 0.9298466708674649  loss : 0.32130086664037244\n",
      "iterations 6776 accuracy : 0.9298466708674649  loss : 0.32129351687083174\n",
      "iterations 6777 accuracy : 0.9298466708674649  loss : 0.32128582280695317\n",
      "iterations 6778 accuracy : 0.9298466708674649  loss : 0.32127874232355713\n",
      "iterations 6779 accuracy : 0.9298466708674649  loss : 0.3212721315766682\n",
      "iterations 6780 accuracy : 0.9298466708674649  loss : 0.32126579601268107\n",
      "iterations 6781 accuracy : 0.9298466708674649  loss : 0.3212588536231198\n",
      "iterations 6782 accuracy : 0.9298466708674649  loss : 0.3212513526579064\n",
      "iterations 6783 accuracy : 0.9298466708674649  loss : 0.3212436007788371\n",
      "iterations 6784 accuracy : 0.9298466708674649  loss : 0.3212366124287168\n",
      "iterations 6785 accuracy : 0.9298466708674649  loss : 0.32122921931226517\n",
      "iterations 6786 accuracy : 0.9298466708674649  loss : 0.32122221324444944\n",
      "iterations 6787 accuracy : 0.9298466708674649  loss : 0.3212151625127349\n",
      "iterations 6788 accuracy : 0.9298466708674649  loss : 0.3212086740408805\n",
      "iterations 6789 accuracy : 0.9298466708674649  loss : 0.3212015435847953\n",
      "iterations 6790 accuracy : 0.9298466708674649  loss : 0.3211946556410911\n",
      "iterations 6791 accuracy : 0.9298466708674649  loss : 0.3211879558390845\n",
      "iterations 6792 accuracy : 0.9298466708674649  loss : 0.3211811511981843\n",
      "iterations 6793 accuracy : 0.9298466708674649  loss : 0.32117417141585014\n",
      "iterations 6794 accuracy : 0.9298466708674649  loss : 0.32116716105029125\n",
      "iterations 6795 accuracy : 0.9298466708674649  loss : 0.3211608194042918\n",
      "iterations 6796 accuracy : 0.9298466708674649  loss : 0.3211537234628587\n",
      "iterations 6797 accuracy : 0.9298466708674649  loss : 0.32114681484465074\n",
      "iterations 6798 accuracy : 0.9298466708674649  loss : 0.32113956286610645\n",
      "iterations 6799 accuracy : 0.9298466708674649  loss : 0.3211328000316592\n",
      "iterations 6800 accuracy : 0.9298466708674649  loss : 0.3211256213607616\n",
      "iterations 6801 accuracy : 0.9298466708674649  loss : 0.3211184847089164\n",
      "iterations 6802 accuracy : 0.9298466708674649  loss : 0.3211115964660731\n",
      "iterations 6803 accuracy : 0.9298466708674649  loss : 0.32110470972808663\n",
      "iterations 6804 accuracy : 0.9298466708674649  loss : 0.32109806025007503\n",
      "iterations 6805 accuracy : 0.9298466708674649  loss : 0.32109106086478345\n",
      "iterations 6806 accuracy : 0.9298466708674649  loss : 0.3210840848011484\n",
      "iterations 6807 accuracy : 0.9298466708674649  loss : 0.32107721999826344\n",
      "iterations 6808 accuracy : 0.9298466708674649  loss : 0.32107071129330533\n",
      "iterations 6809 accuracy : 0.9298466708674649  loss : 0.3210640925467467\n",
      "iterations 6810 accuracy : 0.9298466708674649  loss : 0.32105739936042804\n",
      "iterations 6811 accuracy : 0.9298466708674649  loss : 0.3210505036782267\n",
      "iterations 6812 accuracy : 0.9298466708674649  loss : 0.32104366567229503\n",
      "iterations 6813 accuracy : 0.9298466708674649  loss : 0.3210369903640534\n",
      "iterations 6814 accuracy : 0.9298466708674649  loss : 0.3210303600369234\n",
      "iterations 6815 accuracy : 0.9298466708674649  loss : 0.3210236023994616\n",
      "iterations 6816 accuracy : 0.9298466708674649  loss : 0.32101697661745193\n",
      "iterations 6817 accuracy : 0.9298466708674649  loss : 0.32101009942346126\n",
      "iterations 6818 accuracy : 0.9298466708674649  loss : 0.3210035617488563\n",
      "iterations 6819 accuracy : 0.9298466708674649  loss : 0.32099612189561133\n",
      "iterations 6820 accuracy : 0.9298466708674649  loss : 0.32098929599221226\n",
      "iterations 6821 accuracy : 0.9298466708674649  loss : 0.32098185827599135\n",
      "iterations 6822 accuracy : 0.9298466708674649  loss : 0.3209753692613063\n",
      "iterations 6823 accuracy : 0.9298466708674649  loss : 0.3209684920646305\n",
      "iterations 6824 accuracy : 0.9298466708674649  loss : 0.320961836414147\n",
      "iterations 6825 accuracy : 0.9298466708674649  loss : 0.3209553184417191\n",
      "iterations 6826 accuracy : 0.9298466708674649  loss : 0.3209486355022031\n",
      "iterations 6827 accuracy : 0.9298466708674649  loss : 0.32094219755268216\n",
      "iterations 6828 accuracy : 0.9298466708674649  loss : 0.32093554530950075\n",
      "iterations 6829 accuracy : 0.9298466708674649  loss : 0.32092855356630473\n",
      "iterations 6830 accuracy : 0.9298466708674649  loss : 0.3209209747785408\n",
      "iterations 6831 accuracy : 0.9298466708674649  loss : 0.32091422551863935\n",
      "iterations 6832 accuracy : 0.9298466708674649  loss : 0.32090724354402267\n",
      "iterations 6833 accuracy : 0.9298466708674649  loss : 0.3209001091815244\n",
      "iterations 6834 accuracy : 0.9296366309598824  loss : 0.32089278343777144\n",
      "iterations 6835 accuracy : 0.9298466708674649  loss : 0.3208862075345251\n",
      "iterations 6836 accuracy : 0.9296366309598824  loss : 0.32087926296798747\n",
      "iterations 6837 accuracy : 0.9296366309598824  loss : 0.32087206712950833\n",
      "iterations 6838 accuracy : 0.9296366309598824  loss : 0.3208656713750341\n",
      "iterations 6839 accuracy : 0.9298466708674649  loss : 0.3208591543948094\n",
      "iterations 6840 accuracy : 0.9298466708674649  loss : 0.32085215766922004\n",
      "iterations 6841 accuracy : 0.9296366309598824  loss : 0.3208451920085914\n",
      "iterations 6842 accuracy : 0.9296366309598824  loss : 0.32083791469578155\n",
      "iterations 6843 accuracy : 0.9296366309598824  loss : 0.3208313892002596\n",
      "iterations 6844 accuracy : 0.9296366309598824  loss : 0.32082419291972325\n",
      "iterations 6845 accuracy : 0.9296366309598824  loss : 0.3208176021394044\n",
      "iterations 6846 accuracy : 0.9296366309598824  loss : 0.32081119411827286\n",
      "iterations 6847 accuracy : 0.9298466708674649  loss : 0.3208051433844564\n",
      "iterations 6848 accuracy : 0.9298466708674649  loss : 0.32079848036671205\n",
      "iterations 6849 accuracy : 0.9298466708674649  loss : 0.3207917377569624\n",
      "iterations 6850 accuracy : 0.9296366309598824  loss : 0.3207845412170667\n",
      "iterations 6851 accuracy : 0.9296366309598824  loss : 0.32077739814820994\n",
      "iterations 6852 accuracy : 0.9296366309598824  loss : 0.32076998616907015\n",
      "iterations 6853 accuracy : 0.9296366309598824  loss : 0.32076386716100036\n",
      "iterations 6854 accuracy : 0.9298466708674649  loss : 0.3207572499524049\n",
      "iterations 6855 accuracy : 0.9298466708674649  loss : 0.3207505030067823\n",
      "iterations 6856 accuracy : 0.9298466708674649  loss : 0.3207439084548802\n",
      "iterations 6857 accuracy : 0.9298466708674649  loss : 0.32073709101387393\n",
      "iterations 6858 accuracy : 0.9296366309598824  loss : 0.320730156455393\n",
      "iterations 6859 accuracy : 0.9296366309598824  loss : 0.3207233505952117\n",
      "iterations 6860 accuracy : 0.9296366309598824  loss : 0.320715990414477\n",
      "iterations 6861 accuracy : 0.9296366309598824  loss : 0.32070946529108135\n",
      "iterations 6862 accuracy : 0.9296366309598824  loss : 0.32070287791182644\n",
      "iterations 6863 accuracy : 0.9296366309598824  loss : 0.3206959613998815\n",
      "iterations 6864 accuracy : 0.9296366309598824  loss : 0.32068919441027466\n",
      "iterations 6865 accuracy : 0.9296366309598824  loss : 0.320682759413286\n",
      "iterations 6866 accuracy : 0.9296366309598824  loss : 0.32067635310513953\n",
      "iterations 6867 accuracy : 0.9296366309598824  loss : 0.3206698312768298\n",
      "iterations 6868 accuracy : 0.9296366309598824  loss : 0.3206627400907701\n",
      "iterations 6869 accuracy : 0.9296366309598824  loss : 0.3206558077640708\n",
      "iterations 6870 accuracy : 0.9296366309598824  loss : 0.32064906503243956\n",
      "iterations 6871 accuracy : 0.9296366309598824  loss : 0.32064261804013133\n",
      "iterations 6872 accuracy : 0.9296366309598824  loss : 0.3206358251244906\n",
      "iterations 6873 accuracy : 0.9296366309598824  loss : 0.3206289940474471\n",
      "iterations 6874 accuracy : 0.9296366309598824  loss : 0.3206226821047454\n",
      "iterations 6875 accuracy : 0.9296366309598824  loss : 0.3206156375712924\n",
      "iterations 6876 accuracy : 0.9296366309598824  loss : 0.32060841943832047\n",
      "iterations 6877 accuracy : 0.9294265910523  loss : 0.32060088009593635\n",
      "iterations 6878 accuracy : 0.9294265910523  loss : 0.32059469988174\n",
      "iterations 6879 accuracy : 0.9294265910523  loss : 0.3205875269433023\n",
      "iterations 6880 accuracy : 0.9294265910523  loss : 0.32058020800884135\n",
      "iterations 6881 accuracy : 0.9294265910523  loss : 0.3205732651127839\n",
      "iterations 6882 accuracy : 0.9294265910523  loss : 0.3205670262552449\n",
      "iterations 6883 accuracy : 0.9294265910523  loss : 0.32056026336712407\n",
      "iterations 6884 accuracy : 0.9294265910523  loss : 0.3205534874009694\n",
      "iterations 6885 accuracy : 0.9294265910523  loss : 0.3205466288628117\n",
      "iterations 6886 accuracy : 0.9294265910523  loss : 0.32053990645707153\n",
      "iterations 6887 accuracy : 0.9294265910523  loss : 0.3205331103802218\n",
      "iterations 6888 accuracy : 0.9294265910523  loss : 0.3205261269551306\n",
      "iterations 6889 accuracy : 0.9294265910523  loss : 0.320518820724879\n",
      "iterations 6890 accuracy : 0.9294265910523  loss : 0.3205124412384696\n",
      "iterations 6891 accuracy : 0.9294265910523  loss : 0.3205057624024218\n",
      "iterations 6892 accuracy : 0.9294265910523  loss : 0.3204993776534532\n",
      "iterations 6893 accuracy : 0.9294265910523  loss : 0.32049374777782436\n",
      "iterations 6894 accuracy : 0.9294265910523  loss : 0.3204864786959788\n",
      "iterations 6895 accuracy : 0.9294265910523  loss : 0.32048006285872366\n",
      "iterations 6896 accuracy : 0.9294265910523  loss : 0.3204732671725542\n",
      "iterations 6897 accuracy : 0.9294265910523  loss : 0.3204666729359399\n",
      "iterations 6898 accuracy : 0.9294265910523  loss : 0.32046008504591267\n",
      "iterations 6899 accuracy : 0.9294265910523  loss : 0.3204534184038366\n",
      "iterations 6900 accuracy : 0.9294265910523  loss : 0.3204471691761334\n",
      "iterations 6901 accuracy : 0.9294265910523  loss : 0.3204408097956487\n",
      "iterations 6902 accuracy : 0.9294265910523  loss : 0.32043387114008914\n",
      "iterations 6903 accuracy : 0.9294265910523  loss : 0.32042698009729875\n",
      "iterations 6904 accuracy : 0.9294265910523  loss : 0.3204208532093703\n",
      "iterations 6905 accuracy : 0.9294265910523  loss : 0.32041396435034314\n",
      "iterations 6906 accuracy : 0.9294265910523  loss : 0.3204075684595636\n",
      "iterations 6907 accuracy : 0.9294265910523  loss : 0.32040101700477097\n",
      "iterations 6908 accuracy : 0.9294265910523  loss : 0.32039412900278685\n",
      "iterations 6909 accuracy : 0.9296366309598824  loss : 0.3203880389767385\n",
      "iterations 6910 accuracy : 0.9296366309598824  loss : 0.3203815862444976\n",
      "iterations 6911 accuracy : 0.9296366309598824  loss : 0.3203749943855724\n",
      "iterations 6912 accuracy : 0.9296366309598824  loss : 0.3203683063179428\n",
      "iterations 6913 accuracy : 0.9296366309598824  loss : 0.32036176377482734\n",
      "iterations 6914 accuracy : 0.9296366309598824  loss : 0.32035467824268793\n",
      "iterations 6915 accuracy : 0.9294265910523  loss : 0.32034769025972326\n",
      "iterations 6916 accuracy : 0.9294265910523  loss : 0.3203409041417773\n",
      "iterations 6917 accuracy : 0.9296366309598824  loss : 0.3203345783181164\n",
      "iterations 6918 accuracy : 0.9296366309598824  loss : 0.32032837767243705\n",
      "iterations 6919 accuracy : 0.9296366309598824  loss : 0.3203215186709129\n",
      "iterations 6920 accuracy : 0.9296366309598824  loss : 0.32031522131529006\n",
      "iterations 6921 accuracy : 0.9296366309598824  loss : 0.3203087396218405\n",
      "iterations 6922 accuracy : 0.9296366309598824  loss : 0.3203021431606204\n",
      "iterations 6923 accuracy : 0.9294265910523  loss : 0.3202946386040351\n",
      "iterations 6924 accuracy : 0.9294265910523  loss : 0.32028746002821945\n",
      "iterations 6925 accuracy : 0.9294265910523  loss : 0.32028029521515444\n",
      "iterations 6926 accuracy : 0.9294265910523  loss : 0.3202736972558164\n",
      "iterations 6927 accuracy : 0.9294265910523  loss : 0.32026729304917795\n",
      "iterations 6928 accuracy : 0.9294265910523  loss : 0.3202604541213655\n",
      "iterations 6929 accuracy : 0.9294265910523  loss : 0.3202534984785342\n",
      "iterations 6930 accuracy : 0.9294265910523  loss : 0.32024657821102265\n",
      "iterations 6931 accuracy : 0.9294265910523  loss : 0.3202403099439924\n",
      "iterations 6932 accuracy : 0.9294265910523  loss : 0.3202335599034699\n",
      "iterations 6933 accuracy : 0.9294265910523  loss : 0.3202265848357173\n",
      "iterations 6934 accuracy : 0.9294265910523  loss : 0.32021984916092155\n",
      "iterations 6935 accuracy : 0.9294265910523  loss : 0.3202135198671565\n",
      "iterations 6936 accuracy : 0.9294265910523  loss : 0.3202070786114802\n",
      "iterations 6937 accuracy : 0.9294265910523  loss : 0.3202005859214559\n",
      "iterations 6938 accuracy : 0.9294265910523  loss : 0.3201936420319865\n",
      "iterations 6939 accuracy : 0.9294265910523  loss : 0.32018700794514054\n",
      "iterations 6940 accuracy : 0.9294265910523  loss : 0.32018010514239587\n",
      "iterations 6941 accuracy : 0.9294265910523  loss : 0.32017431368068305\n",
      "iterations 6942 accuracy : 0.9294265910523  loss : 0.32016757441709937\n",
      "iterations 6943 accuracy : 0.9294265910523  loss : 0.32016126047099747\n",
      "iterations 6944 accuracy : 0.9294265910523  loss : 0.3201543047359831\n",
      "iterations 6945 accuracy : 0.9294265910523  loss : 0.3201479741758582\n",
      "iterations 6946 accuracy : 0.9294265910523  loss : 0.3201411094370241\n",
      "iterations 6947 accuracy : 0.9294265910523  loss : 0.3201345921382613\n",
      "iterations 6948 accuracy : 0.9294265910523  loss : 0.3201281507343312\n",
      "iterations 6949 accuracy : 0.9294265910523  loss : 0.32012170765070636\n",
      "iterations 6950 accuracy : 0.9294265910523  loss : 0.3201154918301289\n",
      "iterations 6951 accuracy : 0.9296366309598824  loss : 0.3201092280459677\n",
      "iterations 6952 accuracy : 0.9296366309598824  loss : 0.3201028342018026\n",
      "iterations 6953 accuracy : 0.9296366309598824  loss : 0.3200963275664572\n",
      "iterations 6954 accuracy : 0.9296366309598824  loss : 0.3200896651044756\n",
      "iterations 6955 accuracy : 0.9296366309598824  loss : 0.3200827769458452\n",
      "iterations 6956 accuracy : 0.9294265910523  loss : 0.3200760002713394\n",
      "iterations 6957 accuracy : 0.9294265910523  loss : 0.3200698070814556\n",
      "iterations 6958 accuracy : 0.9294265910523  loss : 0.320062982877517\n",
      "iterations 6959 accuracy : 0.9294265910523  loss : 0.3200561287726862\n",
      "iterations 6960 accuracy : 0.9294265910523  loss : 0.32004953312334655\n",
      "iterations 6961 accuracy : 0.9294265910523  loss : 0.32004209085424323\n",
      "iterations 6962 accuracy : 0.9296366309598824  loss : 0.32003501357017455\n",
      "iterations 6963 accuracy : 0.9296366309598824  loss : 0.32002842866361\n",
      "iterations 6964 accuracy : 0.9296366309598824  loss : 0.32002229176194646\n",
      "iterations 6965 accuracy : 0.9296366309598824  loss : 0.3200152640337174\n",
      "iterations 6966 accuracy : 0.9296366309598824  loss : 0.32000870358484124\n",
      "iterations 6967 accuracy : 0.9296366309598824  loss : 0.32000253827577285\n",
      "iterations 6968 accuracy : 0.9296366309598824  loss : 0.31999647512161583\n",
      "iterations 6969 accuracy : 0.9296366309598824  loss : 0.31998978867018574\n",
      "iterations 6970 accuracy : 0.9296366309598824  loss : 0.3199831996366686\n",
      "iterations 6971 accuracy : 0.9296366309598824  loss : 0.319976517298055\n",
      "iterations 6972 accuracy : 0.9296366309598824  loss : 0.31996946009298066\n",
      "iterations 6973 accuracy : 0.9296366309598824  loss : 0.31996277408310164\n",
      "iterations 6974 accuracy : 0.9296366309598824  loss : 0.31995613317329447\n",
      "iterations 6975 accuracy : 0.9296366309598824  loss : 0.31994993419987405\n",
      "iterations 6976 accuracy : 0.9296366309598824  loss : 0.31994355071929204\n",
      "iterations 6977 accuracy : 0.9296366309598824  loss : 0.3199369173969624\n",
      "iterations 6978 accuracy : 0.9296366309598824  loss : 0.3199303067531714\n",
      "iterations 6979 accuracy : 0.9296366309598824  loss : 0.3199240530277396\n",
      "iterations 6980 accuracy : 0.9296366309598824  loss : 0.31991695792405794\n",
      "iterations 6981 accuracy : 0.9296366309598824  loss : 0.31991044600738944\n",
      "iterations 6982 accuracy : 0.9296366309598824  loss : 0.31990393069753065\n",
      "iterations 6983 accuracy : 0.9296366309598824  loss : 0.31989717005303514\n",
      "iterations 6984 accuracy : 0.9296366309598824  loss : 0.3198907942590503\n",
      "iterations 6985 accuracy : 0.9296366309598824  loss : 0.319884147862432\n",
      "iterations 6986 accuracy : 0.9296366309598824  loss : 0.31987758864308863\n",
      "iterations 6987 accuracy : 0.9296366309598824  loss : 0.3198705726340825\n",
      "iterations 6988 accuracy : 0.9296366309598824  loss : 0.3198639306879255\n",
      "iterations 6989 accuracy : 0.9296366309598824  loss : 0.31985775594814414\n",
      "iterations 6990 accuracy : 0.9296366309598824  loss : 0.31985139439516147\n",
      "iterations 6991 accuracy : 0.9296366309598824  loss : 0.3198448151261552\n",
      "iterations 6992 accuracy : 0.9296366309598824  loss : 0.31983858681182303\n",
      "iterations 6993 accuracy : 0.9296366309598824  loss : 0.3198323775207136\n",
      "iterations 6994 accuracy : 0.9296366309598824  loss : 0.31982595293644694\n",
      "iterations 6995 accuracy : 0.9296366309598824  loss : 0.3198193858902645\n",
      "iterations 6996 accuracy : 0.9296366309598824  loss : 0.3198123768643683\n",
      "iterations 6997 accuracy : 0.9296366309598824  loss : 0.3198052763173737\n",
      "iterations 6998 accuracy : 0.9296366309598824  loss : 0.3197997089859963\n",
      "iterations 6999 accuracy : 0.9296366309598824  loss : 0.3197930335999778\n",
      "iterations 7000 accuracy : 0.9296366309598824  loss : 0.31978639499037154\n",
      "iterations 7001 accuracy : 0.9296366309598824  loss : 0.3197801916906559\n",
      "iterations 7002 accuracy : 0.9296366309598824  loss : 0.31977346025891956\n",
      "iterations 7003 accuracy : 0.9296366309598824  loss : 0.3197671998482555\n",
      "iterations 7004 accuracy : 0.9296366309598824  loss : 0.3197605805136192\n",
      "iterations 7005 accuracy : 0.9296366309598824  loss : 0.31975412500396305\n",
      "iterations 7006 accuracy : 0.9296366309598824  loss : 0.3197472783497384\n",
      "iterations 7007 accuracy : 0.9296366309598824  loss : 0.3197403035064804\n",
      "iterations 7008 accuracy : 0.9296366309598824  loss : 0.31973387586666874\n",
      "iterations 7009 accuracy : 0.9296366309598824  loss : 0.3197276023376646\n",
      "iterations 7010 accuracy : 0.9296366309598824  loss : 0.31972109171185853\n",
      "iterations 7011 accuracy : 0.9296366309598824  loss : 0.31971471534999757\n",
      "iterations 7012 accuracy : 0.9296366309598824  loss : 0.3197085764766676\n",
      "iterations 7013 accuracy : 0.9296366309598824  loss : 0.3197018815870487\n",
      "iterations 7014 accuracy : 0.9296366309598824  loss : 0.31969591729145985\n",
      "iterations 7015 accuracy : 0.9296366309598824  loss : 0.31968929085765113\n",
      "iterations 7016 accuracy : 0.9296366309598824  loss : 0.31968291947973543\n",
      "iterations 7017 accuracy : 0.9296366309598824  loss : 0.3196760444219371\n",
      "iterations 7018 accuracy : 0.9296366309598824  loss : 0.3196698353241239\n",
      "iterations 7019 accuracy : 0.9296366309598824  loss : 0.319663485724037\n",
      "iterations 7020 accuracy : 0.9296366309598824  loss : 0.3196567504732405\n",
      "iterations 7021 accuracy : 0.9296366309598824  loss : 0.3196498117428962\n",
      "iterations 7022 accuracy : 0.9296366309598824  loss : 0.3196441426963397\n",
      "iterations 7023 accuracy : 0.9296366309598824  loss : 0.31963729416218595\n",
      "iterations 7024 accuracy : 0.9296366309598824  loss : 0.31963107762198095\n",
      "iterations 7025 accuracy : 0.9296366309598824  loss : 0.31962454411352004\n",
      "iterations 7026 accuracy : 0.9296366309598824  loss : 0.3196175816191443\n",
      "iterations 7027 accuracy : 0.9296366309598824  loss : 0.31961052427210584\n",
      "iterations 7028 accuracy : 0.9296366309598824  loss : 0.3196036679717965\n",
      "iterations 7029 accuracy : 0.9296366309598824  loss : 0.31959677923320695\n",
      "iterations 7030 accuracy : 0.9296366309598824  loss : 0.31959081290335223\n",
      "iterations 7031 accuracy : 0.9296366309598824  loss : 0.3195849230099368\n",
      "iterations 7032 accuracy : 0.9296366309598824  loss : 0.31957845958071945\n",
      "iterations 7033 accuracy : 0.9296366309598824  loss : 0.31957170892687126\n",
      "iterations 7034 accuracy : 0.9296366309598824  loss : 0.31956529069858075\n",
      "iterations 7035 accuracy : 0.9296366309598824  loss : 0.3195589016587238\n",
      "iterations 7036 accuracy : 0.9296366309598824  loss : 0.3195518708900142\n",
      "iterations 7037 accuracy : 0.9296366309598824  loss : 0.3195451312557884\n",
      "iterations 7038 accuracy : 0.9296366309598824  loss : 0.31953909476026154\n",
      "iterations 7039 accuracy : 0.9296366309598824  loss : 0.31953274756510597\n",
      "iterations 7040 accuracy : 0.9296366309598824  loss : 0.31952634188264606\n",
      "iterations 7041 accuracy : 0.9296366309598824  loss : 0.31951973172362613\n",
      "iterations 7042 accuracy : 0.9296366309598824  loss : 0.3195131671673757\n",
      "iterations 7043 accuracy : 0.9294265910523  loss : 0.3195064395741989\n",
      "iterations 7044 accuracy : 0.9296366309598824  loss : 0.3195004023060677\n",
      "iterations 7045 accuracy : 0.9296366309598824  loss : 0.31949397255531015\n",
      "iterations 7046 accuracy : 0.9294265910523  loss : 0.31948741939355346\n",
      "iterations 7047 accuracy : 0.9294265910523  loss : 0.3194809716037778\n",
      "iterations 7048 accuracy : 0.9294265910523  loss : 0.31947432253648556\n",
      "iterations 7049 accuracy : 0.9294265910523  loss : 0.3194676421903539\n",
      "iterations 7050 accuracy : 0.9294265910523  loss : 0.31946128071765095\n",
      "iterations 7051 accuracy : 0.9296366309598824  loss : 0.3194553520507244\n",
      "iterations 7052 accuracy : 0.9296366309598824  loss : 0.3194490140503718\n",
      "iterations 7053 accuracy : 0.9294265910523  loss : 0.31944254731624183\n",
      "iterations 7054 accuracy : 0.9294265910523  loss : 0.3194361155356809\n",
      "iterations 7055 accuracy : 0.9294265910523  loss : 0.3194291021202267\n",
      "iterations 7056 accuracy : 0.9294265910523  loss : 0.3194229877494106\n",
      "iterations 7057 accuracy : 0.9294265910523  loss : 0.319416544210779\n",
      "iterations 7058 accuracy : 0.9294265910523  loss : 0.3194102994798319\n",
      "iterations 7059 accuracy : 0.9294265910523  loss : 0.31940350204186163\n",
      "iterations 7060 accuracy : 0.9294265910523  loss : 0.3193970930677966\n",
      "iterations 7061 accuracy : 0.9294265910523  loss : 0.31939072368853155\n",
      "iterations 7062 accuracy : 0.9296366309598824  loss : 0.3193849594160896\n",
      "iterations 7063 accuracy : 0.9294265910523  loss : 0.3193780276770086\n",
      "iterations 7064 accuracy : 0.9294265910523  loss : 0.31937163610081243\n",
      "iterations 7065 accuracy : 0.9294265910523  loss : 0.31936562473241953\n",
      "iterations 7066 accuracy : 0.9294265910523  loss : 0.3193591411129844\n",
      "iterations 7067 accuracy : 0.9294265910523  loss : 0.3193527794338226\n",
      "iterations 7068 accuracy : 0.9294265910523  loss : 0.31934621776395883\n",
      "iterations 7069 accuracy : 0.9294265910523  loss : 0.3193400374623785\n",
      "iterations 7070 accuracy : 0.9294265910523  loss : 0.31933414362378437\n",
      "iterations 7071 accuracy : 0.9294265910523  loss : 0.3193276634555857\n",
      "iterations 7072 accuracy : 0.9294265910523  loss : 0.31932109528435565\n",
      "iterations 7073 accuracy : 0.9290065112371351  loss : 0.3193141106956921\n",
      "iterations 7074 accuracy : 0.9290065112371351  loss : 0.3193076170280059\n",
      "iterations 7075 accuracy : 0.9290065112371351  loss : 0.3193011351739727\n",
      "iterations 7076 accuracy : 0.9290065112371351  loss : 0.31929519951820123\n",
      "iterations 7077 accuracy : 0.9290065112371351  loss : 0.31928874155640563\n",
      "iterations 7078 accuracy : 0.9290065112371351  loss : 0.3192824179418177\n",
      "iterations 7079 accuracy : 0.9290065112371351  loss : 0.3192755291171205\n",
      "iterations 7080 accuracy : 0.9290065112371351  loss : 0.3192688918937504\n",
      "iterations 7081 accuracy : 0.9290065112371351  loss : 0.3192623491277887\n",
      "iterations 7082 accuracy : 0.9290065112371351  loss : 0.31925582429431215\n",
      "iterations 7083 accuracy : 0.9290065112371351  loss : 0.31924911861879784\n",
      "iterations 7084 accuracy : 0.9290065112371351  loss : 0.31924267116451444\n",
      "iterations 7085 accuracy : 0.9290065112371351  loss : 0.3192359858368192\n",
      "iterations 7086 accuracy : 0.9290065112371351  loss : 0.31922931226181406\n",
      "iterations 7087 accuracy : 0.9290065112371351  loss : 0.3192223399592616\n",
      "iterations 7088 accuracy : 0.9290065112371351  loss : 0.31921581398471816\n",
      "iterations 7089 accuracy : 0.9290065112371351  loss : 0.3192092500544264\n",
      "iterations 7090 accuracy : 0.9290065112371351  loss : 0.3192030773720898\n",
      "iterations 7091 accuracy : 0.9290065112371351  loss : 0.31919674135756304\n",
      "iterations 7092 accuracy : 0.9290065112371351  loss : 0.31919066473356106\n",
      "iterations 7093 accuracy : 0.9292165511447175  loss : 0.31918413348782626\n",
      "iterations 7094 accuracy : 0.9292165511447175  loss : 0.31917796117670955\n",
      "iterations 7095 accuracy : 0.9292165511447175  loss : 0.3191713985499271\n",
      "iterations 7096 accuracy : 0.9292165511447175  loss : 0.31916519813201816\n",
      "iterations 7097 accuracy : 0.9292165511447175  loss : 0.31915898837113704\n",
      "iterations 7098 accuracy : 0.9292165511447175  loss : 0.3191532262726678\n",
      "iterations 7099 accuracy : 0.9290065112371351  loss : 0.31914718424428284\n",
      "iterations 7100 accuracy : 0.9292165511447175  loss : 0.3191399415655888\n",
      "iterations 7101 accuracy : 0.9292165511447175  loss : 0.31913327026656235\n",
      "iterations 7102 accuracy : 0.9290065112371351  loss : 0.3191272155246108\n",
      "iterations 7103 accuracy : 0.9292165511447175  loss : 0.31912069781694247\n",
      "iterations 7104 accuracy : 0.9290065112371351  loss : 0.31911450152138277\n",
      "iterations 7105 accuracy : 0.9290065112371351  loss : 0.3191082743004575\n",
      "iterations 7106 accuracy : 0.9292165511447175  loss : 0.3191016973984514\n",
      "iterations 7107 accuracy : 0.9292165511447175  loss : 0.3190952618663907\n",
      "iterations 7108 accuracy : 0.9290065112371351  loss : 0.31908931148767566\n",
      "iterations 7109 accuracy : 0.9292165511447175  loss : 0.3190823406623799\n",
      "iterations 7110 accuracy : 0.9292165511447175  loss : 0.31907633082842796\n",
      "iterations 7111 accuracy : 0.9292165511447175  loss : 0.3190698994553542\n",
      "iterations 7112 accuracy : 0.9292165511447175  loss : 0.3190631270188839\n",
      "iterations 7113 accuracy : 0.9292165511447175  loss : 0.3190568010516113\n",
      "iterations 7114 accuracy : 0.9292165511447175  loss : 0.31905070435973104\n",
      "iterations 7115 accuracy : 0.9290065112371351  loss : 0.31904439173712973\n",
      "iterations 7116 accuracy : 0.9290065112371351  loss : 0.31903813435847717\n",
      "iterations 7117 accuracy : 0.9290065112371351  loss : 0.31903268442176286\n",
      "iterations 7118 accuracy : 0.9290065112371351  loss : 0.31902595619345236\n",
      "iterations 7119 accuracy : 0.9290065112371351  loss : 0.31901997368853025\n",
      "iterations 7120 accuracy : 0.9290065112371351  loss : 0.3190131349859179\n",
      "iterations 7121 accuracy : 0.9290065112371351  loss : 0.3190071974848394\n",
      "iterations 7122 accuracy : 0.9290065112371351  loss : 0.31900073231179044\n",
      "iterations 7123 accuracy : 0.9290065112371351  loss : 0.3189941141380775\n",
      "iterations 7124 accuracy : 0.9290065112371351  loss : 0.31898791571912055\n",
      "iterations 7125 accuracy : 0.9290065112371351  loss : 0.318981715818618\n",
      "iterations 7126 accuracy : 0.9290065112371351  loss : 0.3189753637685476\n",
      "iterations 7127 accuracy : 0.9292165511447175  loss : 0.3189688564732979\n",
      "iterations 7128 accuracy : 0.9290065112371351  loss : 0.31896288993093774\n",
      "iterations 7129 accuracy : 0.9290065112371351  loss : 0.3189564844102165\n",
      "iterations 7130 accuracy : 0.9292165511447175  loss : 0.3189497578584678\n",
      "iterations 7131 accuracy : 0.9292165511447175  loss : 0.31894337482702406\n",
      "iterations 7132 accuracy : 0.9292165511447175  loss : 0.3189373722877228\n",
      "iterations 7133 accuracy : 0.9292165511447175  loss : 0.3189309888987646\n",
      "iterations 7134 accuracy : 0.9292165511447175  loss : 0.3189244525141747\n",
      "iterations 7135 accuracy : 0.9292165511447175  loss : 0.31891834895384474\n",
      "iterations 7136 accuracy : 0.9292165511447175  loss : 0.3189122594388107\n",
      "iterations 7137 accuracy : 0.9292165511447175  loss : 0.3189056664349799\n",
      "iterations 7138 accuracy : 0.9292165511447175  loss : 0.3188997577929552\n",
      "iterations 7139 accuracy : 0.9290065112371351  loss : 0.31889394957287986\n",
      "iterations 7140 accuracy : 0.9292165511447175  loss : 0.31888758102312886\n",
      "iterations 7141 accuracy : 0.9292165511447175  loss : 0.3188813523144219\n",
      "iterations 7142 accuracy : 0.9292165511447175  loss : 0.3188746285652121\n",
      "iterations 7143 accuracy : 0.9292165511447175  loss : 0.3188686860089915\n",
      "iterations 7144 accuracy : 0.9292165511447175  loss : 0.3188626423712331\n",
      "iterations 7145 accuracy : 0.9292165511447175  loss : 0.31885597567722135\n",
      "iterations 7146 accuracy : 0.9292165511447175  loss : 0.31884985685309275\n",
      "iterations 7147 accuracy : 0.9292165511447175  loss : 0.31884344431423406\n",
      "iterations 7148 accuracy : 0.9292165511447175  loss : 0.31883713676833925\n",
      "iterations 7149 accuracy : 0.9292165511447175  loss : 0.318830936610938\n",
      "iterations 7150 accuracy : 0.9292165511447175  loss : 0.3188244305627123\n",
      "iterations 7151 accuracy : 0.9292165511447175  loss : 0.31881797428130493\n",
      "iterations 7152 accuracy : 0.9292165511447175  loss : 0.3188117698800992\n",
      "iterations 7153 accuracy : 0.9292165511447175  loss : 0.3188055384748192\n",
      "iterations 7154 accuracy : 0.9292165511447175  loss : 0.3187996523855526\n",
      "iterations 7155 accuracy : 0.9292165511447175  loss : 0.31879361547691343\n",
      "iterations 7156 accuracy : 0.9292165511447175  loss : 0.31878725584007783\n",
      "iterations 7157 accuracy : 0.9292165511447175  loss : 0.31878096524004845\n",
      "iterations 7158 accuracy : 0.9292165511447175  loss : 0.3187744360017489\n",
      "iterations 7159 accuracy : 0.9292165511447175  loss : 0.31876871552891556\n",
      "iterations 7160 accuracy : 0.9292165511447175  loss : 0.31876283853996407\n",
      "iterations 7161 accuracy : 0.9292165511447175  loss : 0.3187564258166393\n",
      "iterations 7162 accuracy : 0.9292165511447175  loss : 0.318750261528252\n",
      "iterations 7163 accuracy : 0.9292165511447175  loss : 0.3187439334730544\n",
      "iterations 7164 accuracy : 0.9292165511447175  loss : 0.31873713563007494\n",
      "iterations 7165 accuracy : 0.9292165511447175  loss : 0.3187306469652228\n",
      "iterations 7166 accuracy : 0.9292165511447175  loss : 0.31872420935028195\n",
      "iterations 7167 accuracy : 0.9292165511447175  loss : 0.3187178408267799\n",
      "iterations 7168 accuracy : 0.9292165511447175  loss : 0.31871169194986637\n",
      "iterations 7169 accuracy : 0.9292165511447175  loss : 0.3187060163828738\n",
      "iterations 7170 accuracy : 0.9292165511447175  loss : 0.3186995137449675\n",
      "iterations 7171 accuracy : 0.9292165511447175  loss : 0.3186928077557945\n",
      "iterations 7172 accuracy : 0.9292165511447175  loss : 0.3186871736564003\n",
      "iterations 7173 accuracy : 0.9292165511447175  loss : 0.31868059171509344\n",
      "iterations 7174 accuracy : 0.9292165511447175  loss : 0.31867465444471926\n",
      "iterations 7175 accuracy : 0.9292165511447175  loss : 0.31866853116586535\n",
      "iterations 7176 accuracy : 0.9292165511447175  loss : 0.3186624723271446\n",
      "iterations 7177 accuracy : 0.9292165511447175  loss : 0.31865618793191186\n",
      "iterations 7178 accuracy : 0.9292165511447175  loss : 0.31864999897001556\n",
      "iterations 7179 accuracy : 0.9292165511447175  loss : 0.31864402742312625\n",
      "iterations 7180 accuracy : 0.9292165511447175  loss : 0.31863817872356204\n",
      "iterations 7181 accuracy : 0.9292165511447175  loss : 0.3186320950461985\n",
      "iterations 7182 accuracy : 0.9292165511447175  loss : 0.31862596650708\n",
      "iterations 7183 accuracy : 0.9292165511447175  loss : 0.3186195566332869\n",
      "iterations 7184 accuracy : 0.9292165511447175  loss : 0.3186132853360899\n",
      "iterations 7185 accuracy : 0.9292165511447175  loss : 0.31860667089476474\n",
      "iterations 7186 accuracy : 0.9292165511447175  loss : 0.3186006986409758\n",
      "iterations 7187 accuracy : 0.9292165511447175  loss : 0.31859366441645115\n",
      "iterations 7188 accuracy : 0.9292165511447175  loss : 0.318587130317986\n",
      "iterations 7189 accuracy : 0.9292165511447175  loss : 0.3185811566785053\n",
      "iterations 7190 accuracy : 0.9292165511447175  loss : 0.31857525461512565\n",
      "iterations 7191 accuracy : 0.9292165511447175  loss : 0.31856856171464343\n",
      "iterations 7192 accuracy : 0.9292165511447175  loss : 0.31856214823288603\n",
      "iterations 7193 accuracy : 0.9292165511447175  loss : 0.3185560331362938\n",
      "iterations 7194 accuracy : 0.9292165511447175  loss : 0.3185497762251639\n",
      "iterations 7195 accuracy : 0.9292165511447175  loss : 0.31854378788275023\n",
      "iterations 7196 accuracy : 0.9292165511447175  loss : 0.31853728926172464\n",
      "iterations 7197 accuracy : 0.9292165511447175  loss : 0.31853118993844837\n",
      "iterations 7198 accuracy : 0.9292165511447175  loss : 0.31852527734262387\n",
      "iterations 7199 accuracy : 0.9292165511447175  loss : 0.31851886042361066\n",
      "iterations 7200 accuracy : 0.9292165511447175  loss : 0.31851270429308426\n",
      "iterations 7201 accuracy : 0.9292165511447175  loss : 0.3185064468284361\n",
      "iterations 7202 accuracy : 0.9292165511447175  loss : 0.31850046781064495\n",
      "iterations 7203 accuracy : 0.9292165511447175  loss : 0.31849429720813593\n",
      "iterations 7204 accuracy : 0.9292165511447175  loss : 0.31848813514805474\n",
      "iterations 7205 accuracy : 0.9292165511447175  loss : 0.31848185900905734\n",
      "iterations 7206 accuracy : 0.9292165511447175  loss : 0.3184748642025366\n",
      "iterations 7207 accuracy : 0.9292165511447175  loss : 0.3184688472391383\n",
      "iterations 7208 accuracy : 0.9292165511447175  loss : 0.31846246038608816\n",
      "iterations 7209 accuracy : 0.9292165511447175  loss : 0.318455899689763\n",
      "iterations 7210 accuracy : 0.9292165511447175  loss : 0.3184492400524756\n",
      "iterations 7211 accuracy : 0.9292165511447175  loss : 0.3184429170444052\n",
      "iterations 7212 accuracy : 0.9292165511447175  loss : 0.3184368761148316\n",
      "iterations 7213 accuracy : 0.9292165511447175  loss : 0.31843117158179685\n",
      "iterations 7214 accuracy : 0.9292165511447175  loss : 0.31842452453590214\n",
      "iterations 7215 accuracy : 0.9292165511447175  loss : 0.31841842921287417\n",
      "iterations 7216 accuracy : 0.9292165511447175  loss : 0.3184124136206418\n",
      "iterations 7217 accuracy : 0.9292165511447175  loss : 0.31840612249592876\n",
      "iterations 7218 accuracy : 0.9292165511447175  loss : 0.31840000460311946\n",
      "iterations 7219 accuracy : 0.9292165511447175  loss : 0.3183943589785163\n",
      "iterations 7220 accuracy : 0.9292165511447175  loss : 0.3183887584250701\n",
      "iterations 7221 accuracy : 0.9292165511447175  loss : 0.31838245172048796\n",
      "iterations 7222 accuracy : 0.9292165511447175  loss : 0.3183766391443979\n",
      "iterations 7223 accuracy : 0.9292165511447175  loss : 0.3183704296150914\n",
      "iterations 7224 accuracy : 0.9292165511447175  loss : 0.31836429511987746\n",
      "iterations 7225 accuracy : 0.9292165511447175  loss : 0.31835811561898153\n",
      "iterations 7226 accuracy : 0.9292165511447175  loss : 0.3183515380727993\n",
      "iterations 7227 accuracy : 0.9292165511447175  loss : 0.3183450066908965\n",
      "iterations 7228 accuracy : 0.9292165511447175  loss : 0.3183389594455967\n",
      "iterations 7229 accuracy : 0.9292165511447175  loss : 0.31833274952105883\n",
      "iterations 7230 accuracy : 0.9292165511447175  loss : 0.31832662023073277\n",
      "iterations 7231 accuracy : 0.9292165511447175  loss : 0.3183209709019908\n",
      "iterations 7232 accuracy : 0.9292165511447175  loss : 0.3183148086348264\n",
      "iterations 7233 accuracy : 0.9292165511447175  loss : 0.31830827919179866\n",
      "iterations 7234 accuracy : 0.9292165511447175  loss : 0.3183021966493527\n",
      "iterations 7235 accuracy : 0.9292165511447175  loss : 0.3182958601410845\n",
      "iterations 7236 accuracy : 0.9294265910523  loss : 0.31828980033413107\n",
      "iterations 7237 accuracy : 0.9292165511447175  loss : 0.3182838677132887\n",
      "iterations 7238 accuracy : 0.9294265910523  loss : 0.3182779096077442\n",
      "iterations 7239 accuracy : 0.9294265910523  loss : 0.3182718495351229\n",
      "iterations 7240 accuracy : 0.9292165511447175  loss : 0.31826611407634203\n",
      "iterations 7241 accuracy : 0.9292165511447175  loss : 0.31826026392805246\n",
      "iterations 7242 accuracy : 0.9292165511447175  loss : 0.3182547949137655\n",
      "iterations 7243 accuracy : 0.9292165511447175  loss : 0.3182490191990085\n",
      "iterations 7244 accuracy : 0.9292165511447175  loss : 0.3182431836697771\n",
      "iterations 7245 accuracy : 0.9292165511447175  loss : 0.3182374924358241\n",
      "iterations 7246 accuracy : 0.9292165511447175  loss : 0.31823138648653243\n",
      "iterations 7247 accuracy : 0.9292165511447175  loss : 0.3182257801090734\n",
      "iterations 7248 accuracy : 0.9292165511447175  loss : 0.31821943603477704\n",
      "iterations 7249 accuracy : 0.9292165511447175  loss : 0.31821310660309676\n",
      "iterations 7250 accuracy : 0.9292165511447175  loss : 0.31820735555793883\n",
      "iterations 7251 accuracy : 0.9292165511447175  loss : 0.3182005499910351\n",
      "iterations 7252 accuracy : 0.9292165511447175  loss : 0.31819436836910286\n",
      "iterations 7253 accuracy : 0.9292165511447175  loss : 0.31818835471592083\n",
      "iterations 7254 accuracy : 0.9292165511447175  loss : 0.31818186830250633\n",
      "iterations 7255 accuracy : 0.9294265910523  loss : 0.3181752364908255\n",
      "iterations 7256 accuracy : 0.9294265910523  loss : 0.3181693443468266\n",
      "iterations 7257 accuracy : 0.9294265910523  loss : 0.31816357191969075\n",
      "iterations 7258 accuracy : 0.9294265910523  loss : 0.3181575478139181\n",
      "iterations 7259 accuracy : 0.9292165511447175  loss : 0.3181517294090177\n",
      "iterations 7260 accuracy : 0.9292165511447175  loss : 0.3181453957130188\n",
      "iterations 7261 accuracy : 0.9292165511447175  loss : 0.3181395290979675\n",
      "iterations 7262 accuracy : 0.9292165511447175  loss : 0.31813343817228423\n",
      "iterations 7263 accuracy : 0.9292165511447175  loss : 0.3181276619351766\n",
      "iterations 7264 accuracy : 0.9292165511447175  loss : 0.31812154263949743\n",
      "iterations 7265 accuracy : 0.9292165511447175  loss : 0.3181152782516143\n",
      "iterations 7266 accuracy : 0.9292165511447175  loss : 0.3181091703823032\n",
      "iterations 7267 accuracy : 0.9292165511447175  loss : 0.31810294178198434\n",
      "iterations 7268 accuracy : 0.9294265910523  loss : 0.31809688226795846\n",
      "iterations 7269 accuracy : 0.9294265910523  loss : 0.31809062236401037\n",
      "iterations 7270 accuracy : 0.9294265910523  loss : 0.31808482580814673\n",
      "iterations 7271 accuracy : 0.9294265910523  loss : 0.3180787563704891\n",
      "iterations 7272 accuracy : 0.9294265910523  loss : 0.3180726118785766\n",
      "iterations 7273 accuracy : 0.9294265910523  loss : 0.318066534975427\n",
      "iterations 7274 accuracy : 0.9294265910523  loss : 0.31806047083441896\n",
      "iterations 7275 accuracy : 0.9294265910523  loss : 0.3180544432828061\n",
      "iterations 7276 accuracy : 0.9294265910523  loss : 0.31804885823330786\n",
      "iterations 7277 accuracy : 0.9294265910523  loss : 0.3180424324290963\n",
      "iterations 7278 accuracy : 0.9294265910523  loss : 0.3180363740604348\n",
      "iterations 7279 accuracy : 0.9294265910523  loss : 0.3180297860901514\n",
      "iterations 7280 accuracy : 0.9294265910523  loss : 0.31802303322500936\n",
      "iterations 7281 accuracy : 0.9294265910523  loss : 0.3180168971292793\n",
      "iterations 7282 accuracy : 0.9294265910523  loss : 0.318010147110535\n",
      "iterations 7283 accuracy : 0.9294265910523  loss : 0.31800414941696226\n",
      "iterations 7284 accuracy : 0.9294265910523  loss : 0.31799777166797644\n",
      "iterations 7285 accuracy : 0.9294265910523  loss : 0.3179917704103359\n",
      "iterations 7286 accuracy : 0.9294265910523  loss : 0.3179858482576939\n",
      "iterations 7287 accuracy : 0.9294265910523  loss : 0.3179797656161368\n",
      "iterations 7288 accuracy : 0.9294265910523  loss : 0.31797357989260183\n",
      "iterations 7289 accuracy : 0.9294265910523  loss : 0.3179675990658892\n",
      "iterations 7290 accuracy : 0.9294265910523  loss : 0.3179622242886978\n",
      "iterations 7291 accuracy : 0.9294265910523  loss : 0.31795595305996366\n",
      "iterations 7292 accuracy : 0.9294265910523  loss : 0.31794978110360395\n",
      "iterations 7293 accuracy : 0.9294265910523  loss : 0.3179438712618226\n",
      "iterations 7294 accuracy : 0.9294265910523  loss : 0.3179377276698456\n",
      "iterations 7295 accuracy : 0.9294265910523  loss : 0.31793192996421715\n",
      "iterations 7296 accuracy : 0.9294265910523  loss : 0.3179253063329197\n",
      "iterations 7297 accuracy : 0.9294265910523  loss : 0.31791920353175246\n",
      "iterations 7298 accuracy : 0.9294265910523  loss : 0.3179131997511911\n",
      "iterations 7299 accuracy : 0.9294265910523  loss : 0.3179071358627961\n",
      "iterations 7300 accuracy : 0.9294265910523  loss : 0.3179010042508249\n",
      "iterations 7301 accuracy : 0.9294265910523  loss : 0.3178949396264243\n",
      "iterations 7302 accuracy : 0.9294265910523  loss : 0.31788839591465284\n",
      "iterations 7303 accuracy : 0.9294265910523  loss : 0.3178826405062912\n",
      "iterations 7304 accuracy : 0.9294265910523  loss : 0.3178766475265208\n",
      "iterations 7305 accuracy : 0.9294265910523  loss : 0.31787062761137636\n",
      "iterations 7306 accuracy : 0.9296366309598824  loss : 0.317864322094114\n",
      "iterations 7307 accuracy : 0.9294265910523  loss : 0.31785832508880796\n",
      "iterations 7308 accuracy : 0.9294265910523  loss : 0.3178526662730562\n",
      "iterations 7309 accuracy : 0.9294265910523  loss : 0.31784639634820866\n",
      "iterations 7310 accuracy : 0.9294265910523  loss : 0.31784004545088285\n",
      "iterations 7311 accuracy : 0.9294265910523  loss : 0.3178345126108928\n",
      "iterations 7312 accuracy : 0.9294265910523  loss : 0.3178281778066494\n",
      "iterations 7313 accuracy : 0.9294265910523  loss : 0.31782201269022786\n",
      "iterations 7314 accuracy : 0.9294265910523  loss : 0.31781621832426\n",
      "iterations 7315 accuracy : 0.9294265910523  loss : 0.31781050848247533\n",
      "iterations 7316 accuracy : 0.9296366309598824  loss : 0.31780450292481366\n",
      "iterations 7317 accuracy : 0.9296366309598824  loss : 0.31779851467426906\n",
      "iterations 7318 accuracy : 0.9294265910523  loss : 0.317792855006883\n",
      "iterations 7319 accuracy : 0.9294265910523  loss : 0.3177866725434685\n",
      "iterations 7320 accuracy : 0.9296366309598824  loss : 0.31778033399490746\n",
      "iterations 7321 accuracy : 0.9294265910523  loss : 0.3177740101278068\n",
      "iterations 7322 accuracy : 0.9294265910523  loss : 0.31776798571949005\n",
      "iterations 7323 accuracy : 0.9296366309598824  loss : 0.3177623469296743\n",
      "iterations 7324 accuracy : 0.9294265910523  loss : 0.31775659961916203\n",
      "iterations 7325 accuracy : 0.9296366309598824  loss : 0.3177503778819241\n",
      "iterations 7326 accuracy : 0.9294265910523  loss : 0.31774400530127855\n",
      "iterations 7327 accuracy : 0.9294265910523  loss : 0.31773802390340533\n",
      "iterations 7328 accuracy : 0.9294265910523  loss : 0.31773199984097217\n",
      "iterations 7329 accuracy : 0.9294265910523  loss : 0.31772580747855245\n",
      "iterations 7330 accuracy : 0.9294265910523  loss : 0.3177198957141454\n",
      "iterations 7331 accuracy : 0.9294265910523  loss : 0.3177137873488515\n",
      "iterations 7332 accuracy : 0.9294265910523  loss : 0.3177073765751885\n",
      "iterations 7333 accuracy : 0.9294265910523  loss : 0.31770101944180246\n",
      "iterations 7334 accuracy : 0.9294265910523  loss : 0.31769508613074005\n",
      "iterations 7335 accuracy : 0.9294265910523  loss : 0.31768919550461167\n",
      "iterations 7336 accuracy : 0.9294265910523  loss : 0.3176831310173185\n",
      "iterations 7337 accuracy : 0.9294265910523  loss : 0.3176770824957667\n",
      "iterations 7338 accuracy : 0.9294265910523  loss : 0.317671089461267\n",
      "iterations 7339 accuracy : 0.9294265910523  loss : 0.31766484301813097\n",
      "iterations 7340 accuracy : 0.9294265910523  loss : 0.31765890761622817\n",
      "iterations 7341 accuracy : 0.9294265910523  loss : 0.31765324539526185\n",
      "iterations 7342 accuracy : 0.9294265910523  loss : 0.3176476385569723\n",
      "iterations 7343 accuracy : 0.9294265910523  loss : 0.3176418615009846\n",
      "iterations 7344 accuracy : 0.9294265910523  loss : 0.3176358176091847\n",
      "iterations 7345 accuracy : 0.9294265910523  loss : 0.31762945821635974\n",
      "iterations 7346 accuracy : 0.9294265910523  loss : 0.31762349356690944\n",
      "iterations 7347 accuracy : 0.9294265910523  loss : 0.3176175763395028\n",
      "iterations 7348 accuracy : 0.9294265910523  loss : 0.3176116137789995\n",
      "iterations 7349 accuracy : 0.9296366309598824  loss : 0.3176059219529944\n",
      "iterations 7350 accuracy : 0.9294265910523  loss : 0.3175999474182657\n",
      "iterations 7351 accuracy : 0.9294265910523  loss : 0.3175946769019214\n",
      "iterations 7352 accuracy : 0.9294265910523  loss : 0.31758879953066355\n",
      "iterations 7353 accuracy : 0.9294265910523  loss : 0.31758285996094954\n",
      "iterations 7354 accuracy : 0.9294265910523  loss : 0.31757642540772574\n",
      "iterations 7355 accuracy : 0.9294265910523  loss : 0.317570243802547\n",
      "iterations 7356 accuracy : 0.9294265910523  loss : 0.3175645746555121\n",
      "iterations 7357 accuracy : 0.9294265910523  loss : 0.3175587537261215\n",
      "iterations 7358 accuracy : 0.9294265910523  loss : 0.3175533335086252\n",
      "iterations 7359 accuracy : 0.9294265910523  loss : 0.3175474894487394\n",
      "iterations 7360 accuracy : 0.9294265910523  loss : 0.3175411739760825\n",
      "iterations 7361 accuracy : 0.9294265910523  loss : 0.3175352651236312\n",
      "iterations 7362 accuracy : 0.9294265910523  loss : 0.31752913250087905\n",
      "iterations 7363 accuracy : 0.9294265910523  loss : 0.3175233617548476\n",
      "iterations 7364 accuracy : 0.9294265910523  loss : 0.3175175381939538\n",
      "iterations 7365 accuracy : 0.9294265910523  loss : 0.3175118160087096\n",
      "iterations 7366 accuracy : 0.9294265910523  loss : 0.31750594066575477\n",
      "iterations 7367 accuracy : 0.9294265910523  loss : 0.31749993697533846\n",
      "iterations 7368 accuracy : 0.9294265910523  loss : 0.3174938155618207\n",
      "iterations 7369 accuracy : 0.9294265910523  loss : 0.3174878775827592\n",
      "iterations 7370 accuracy : 0.9294265910523  loss : 0.31748227747972163\n",
      "iterations 7371 accuracy : 0.9294265910523  loss : 0.31747660141882317\n",
      "iterations 7372 accuracy : 0.9294265910523  loss : 0.3174712256399266\n",
      "iterations 7373 accuracy : 0.9294265910523  loss : 0.31746543779391123\n",
      "iterations 7374 accuracy : 0.9292165511447175  loss : 0.3174590036450929\n",
      "iterations 7375 accuracy : 0.9294265910523  loss : 0.31745267832642693\n",
      "iterations 7376 accuracy : 0.9294265910523  loss : 0.3174463805317892\n",
      "iterations 7377 accuracy : 0.9294265910523  loss : 0.317440675022296\n",
      "iterations 7378 accuracy : 0.9294265910523  loss : 0.3174339864555334\n",
      "iterations 7379 accuracy : 0.9294265910523  loss : 0.31742784954159753\n",
      "iterations 7380 accuracy : 0.9294265910523  loss : 0.3174216814963586\n",
      "iterations 7381 accuracy : 0.9294265910523  loss : 0.31741568141734927\n",
      "iterations 7382 accuracy : 0.9294265910523  loss : 0.3174093317811473\n",
      "iterations 7383 accuracy : 0.9294265910523  loss : 0.31740368748622755\n",
      "iterations 7384 accuracy : 0.9294265910523  loss : 0.31739840937774244\n",
      "iterations 7385 accuracy : 0.9294265910523  loss : 0.31739202603455313\n",
      "iterations 7386 accuracy : 0.9294265910523  loss : 0.317385814660196\n",
      "iterations 7387 accuracy : 0.9294265910523  loss : 0.31737989315535164\n",
      "iterations 7388 accuracy : 0.9294265910523  loss : 0.3173740685648509\n",
      "iterations 7389 accuracy : 0.9294265910523  loss : 0.31736792373331385\n",
      "iterations 7390 accuracy : 0.9294265910523  loss : 0.31736199302458207\n",
      "iterations 7391 accuracy : 0.9294265910523  loss : 0.3173562594892808\n",
      "iterations 7392 accuracy : 0.9294265910523  loss : 0.31735003768133024\n",
      "iterations 7393 accuracy : 0.9294265910523  loss : 0.31734426780710195\n",
      "iterations 7394 accuracy : 0.9294265910523  loss : 0.31733800845335697\n",
      "iterations 7395 accuracy : 0.9294265910523  loss : 0.3173323397862699\n",
      "iterations 7396 accuracy : 0.9294265910523  loss : 0.31732610176398834\n",
      "iterations 7397 accuracy : 0.9294265910523  loss : 0.3173206218227607\n",
      "iterations 7398 accuracy : 0.9294265910523  loss : 0.3173150629066925\n",
      "iterations 7399 accuracy : 0.9294265910523  loss : 0.3173092392622321\n",
      "iterations 7400 accuracy : 0.9294265910523  loss : 0.31730368724938623\n",
      "iterations 7401 accuracy : 0.9294265910523  loss : 0.31729778219362326\n",
      "iterations 7402 accuracy : 0.9294265910523  loss : 0.31729150742087237\n",
      "iterations 7403 accuracy : 0.9294265910523  loss : 0.3172857367874704\n",
      "iterations 7404 accuracy : 0.9294265910523  loss : 0.3172800350378733\n",
      "iterations 7405 accuracy : 0.9294265910523  loss : 0.3172737596748094\n",
      "iterations 7406 accuracy : 0.9294265910523  loss : 0.3172676835406437\n",
      "iterations 7407 accuracy : 0.9294265910523  loss : 0.31726183981972744\n",
      "iterations 7408 accuracy : 0.9294265910523  loss : 0.3172560457215951\n",
      "iterations 7409 accuracy : 0.9294265910523  loss : 0.3172499637880482\n",
      "iterations 7410 accuracy : 0.9294265910523  loss : 0.31724443243817135\n",
      "iterations 7411 accuracy : 0.9294265910523  loss : 0.31723868707698377\n",
      "iterations 7412 accuracy : 0.9294265910523  loss : 0.31723283222235166\n",
      "iterations 7413 accuracy : 0.9294265910523  loss : 0.31722679221140954\n",
      "iterations 7414 accuracy : 0.9294265910523  loss : 0.31722096312514536\n",
      "iterations 7415 accuracy : 0.9294265910523  loss : 0.31721489891766824\n",
      "iterations 7416 accuracy : 0.9294265910523  loss : 0.3172092266877934\n",
      "iterations 7417 accuracy : 0.9294265910523  loss : 0.31720377428944724\n",
      "iterations 7418 accuracy : 0.9294265910523  loss : 0.31719775063838374\n",
      "iterations 7419 accuracy : 0.9294265910523  loss : 0.31719207999127474\n",
      "iterations 7420 accuracy : 0.9294265910523  loss : 0.31718599633515554\n",
      "iterations 7421 accuracy : 0.9294265910523  loss : 0.3171800414636679\n",
      "iterations 7422 accuracy : 0.9294265910523  loss : 0.3171748790519029\n",
      "iterations 7423 accuracy : 0.9294265910523  loss : 0.3171691301431395\n",
      "iterations 7424 accuracy : 0.9294265910523  loss : 0.3171634437631681\n",
      "iterations 7425 accuracy : 0.9294265910523  loss : 0.3171581367003431\n",
      "iterations 7426 accuracy : 0.9294265910523  loss : 0.31715210977377034\n",
      "iterations 7427 accuracy : 0.9294265910523  loss : 0.31714649493562025\n",
      "iterations 7428 accuracy : 0.9294265910523  loss : 0.31714084093207207\n",
      "iterations 7429 accuracy : 0.9294265910523  loss : 0.31713507749511743\n",
      "iterations 7430 accuracy : 0.9294265910523  loss : 0.3171295351448348\n",
      "iterations 7431 accuracy : 0.9294265910523  loss : 0.31712360711209064\n",
      "iterations 7432 accuracy : 0.9294265910523  loss : 0.3171180341215163\n",
      "iterations 7433 accuracy : 0.9294265910523  loss : 0.3171124564034357\n",
      "iterations 7434 accuracy : 0.9294265910523  loss : 0.31710668995961666\n",
      "iterations 7435 accuracy : 0.9294265910523  loss : 0.3171008570735423\n",
      "iterations 7436 accuracy : 0.9294265910523  loss : 0.31709510240342953\n",
      "iterations 7437 accuracy : 0.9294265910523  loss : 0.31708936630539525\n",
      "iterations 7438 accuracy : 0.9294265910523  loss : 0.3170840358259751\n",
      "iterations 7439 accuracy : 0.9294265910523  loss : 0.3170780074623438\n",
      "iterations 7440 accuracy : 0.9292165511447175  loss : 0.31707234091232894\n",
      "iterations 7441 accuracy : 0.9294265910523  loss : 0.3170662538369478\n",
      "iterations 7442 accuracy : 0.9294265910523  loss : 0.3170601452468903\n",
      "iterations 7443 accuracy : 0.9294265910523  loss : 0.3170540183371127\n",
      "iterations 7444 accuracy : 0.9294265910523  loss : 0.31704808319132644\n",
      "iterations 7445 accuracy : 0.9294265910523  loss : 0.3170419930908064\n",
      "iterations 7446 accuracy : 0.9294265910523  loss : 0.3170364556219562\n",
      "iterations 7447 accuracy : 0.9294265910523  loss : 0.3170312086305852\n",
      "iterations 7448 accuracy : 0.9294265910523  loss : 0.3170253204537612\n",
      "iterations 7449 accuracy : 0.9294265910523  loss : 0.31701944085708056\n",
      "iterations 7450 accuracy : 0.9294265910523  loss : 0.31701357518531675\n",
      "iterations 7451 accuracy : 0.9294265910523  loss : 0.3170075187641021\n",
      "iterations 7452 accuracy : 0.9294265910523  loss : 0.317001424028259\n",
      "iterations 7453 accuracy : 0.9294265910523  loss : 0.31699587903152243\n",
      "iterations 7454 accuracy : 0.9294265910523  loss : 0.3169901866685752\n",
      "iterations 7455 accuracy : 0.9294265910523  loss : 0.31698437463883333\n",
      "iterations 7456 accuracy : 0.9294265910523  loss : 0.3169792022246167\n",
      "iterations 7457 accuracy : 0.9294265910523  loss : 0.3169735447174797\n",
      "iterations 7458 accuracy : 0.9294265910523  loss : 0.3169674419419272\n",
      "iterations 7459 accuracy : 0.9294265910523  loss : 0.3169615427222349\n",
      "iterations 7460 accuracy : 0.9294265910523  loss : 0.31695548108459465\n",
      "iterations 7461 accuracy : 0.9294265910523  loss : 0.31694941714014946\n",
      "iterations 7462 accuracy : 0.9294265910523  loss : 0.3169432941666983\n",
      "iterations 7463 accuracy : 0.9294265910523  loss : 0.31693744404062435\n",
      "iterations 7464 accuracy : 0.9294265910523  loss : 0.3169313476590505\n",
      "iterations 7465 accuracy : 0.9294265910523  loss : 0.31692532258124656\n",
      "iterations 7466 accuracy : 0.9294265910523  loss : 0.3169197276792421\n",
      "iterations 7467 accuracy : 0.9294265910523  loss : 0.3169139776252319\n",
      "iterations 7468 accuracy : 0.9294265910523  loss : 0.3169082157080159\n",
      "iterations 7469 accuracy : 0.9294265910523  loss : 0.3169024118812678\n",
      "iterations 7470 accuracy : 0.9294265910523  loss : 0.3168966145154228\n",
      "iterations 7471 accuracy : 0.9294265910523  loss : 0.31689060401008995\n",
      "iterations 7472 accuracy : 0.9294265910523  loss : 0.316884462689883\n",
      "iterations 7473 accuracy : 0.9294265910523  loss : 0.31687848445209843\n",
      "iterations 7474 accuracy : 0.9294265910523  loss : 0.31687267332957025\n",
      "iterations 7475 accuracy : 0.9294265910523  loss : 0.3168663733984892\n",
      "iterations 7476 accuracy : 0.9294265910523  loss : 0.31686053975570155\n",
      "iterations 7477 accuracy : 0.9294265910523  loss : 0.31685548476605924\n",
      "iterations 7478 accuracy : 0.9294265910523  loss : 0.3168497934032474\n",
      "iterations 7479 accuracy : 0.9294265910523  loss : 0.31684400014000397\n",
      "iterations 7480 accuracy : 0.9294265910523  loss : 0.3168380100368964\n",
      "iterations 7481 accuracy : 0.9294265910523  loss : 0.31683193888622513\n",
      "iterations 7482 accuracy : 0.9294265910523  loss : 0.3168263384871482\n",
      "iterations 7483 accuracy : 0.9294265910523  loss : 0.3168201087835813\n",
      "iterations 7484 accuracy : 0.9294265910523  loss : 0.31681411568322443\n",
      "iterations 7485 accuracy : 0.9294265910523  loss : 0.3168091135276311\n",
      "iterations 7486 accuracy : 0.9294265910523  loss : 0.3168030473210921\n",
      "iterations 7487 accuracy : 0.9294265910523  loss : 0.31679741537793454\n",
      "iterations 7488 accuracy : 0.9294265910523  loss : 0.31679177279371457\n",
      "iterations 7489 accuracy : 0.9294265910523  loss : 0.3167861656144886\n",
      "iterations 7490 accuracy : 0.9294265910523  loss : 0.316780139647009\n",
      "iterations 7491 accuracy : 0.9294265910523  loss : 0.31677441277998325\n",
      "iterations 7492 accuracy : 0.9294265910523  loss : 0.3167685904169223\n",
      "iterations 7493 accuracy : 0.9294265910523  loss : 0.3167630663851077\n",
      "iterations 7494 accuracy : 0.9294265910523  loss : 0.3167567296691931\n",
      "iterations 7495 accuracy : 0.9294265910523  loss : 0.3167508708317597\n",
      "iterations 7496 accuracy : 0.9294265910523  loss : 0.31674516683940457\n",
      "iterations 7497 accuracy : 0.9294265910523  loss : 0.3167399945629699\n",
      "iterations 7498 accuracy : 0.9294265910523  loss : 0.3167343230665538\n",
      "iterations 7499 accuracy : 0.9294265910523  loss : 0.3167285211890877\n",
      "iterations 7500 accuracy : 0.9294265910523  loss : 0.3167225614935682\n",
      "iterations 7501 accuracy : 0.9294265910523  loss : 0.31671758448689563\n",
      "iterations 7502 accuracy : 0.9294265910523  loss : 0.31671183217682153\n",
      "iterations 7503 accuracy : 0.9294265910523  loss : 0.316705904055612\n",
      "iterations 7504 accuracy : 0.9294265910523  loss : 0.3167002397954499\n",
      "iterations 7505 accuracy : 0.9294265910523  loss : 0.31669441656387154\n",
      "iterations 7506 accuracy : 0.9294265910523  loss : 0.31668858026243485\n",
      "iterations 7507 accuracy : 0.9294265910523  loss : 0.31668303878907195\n",
      "iterations 7508 accuracy : 0.9294265910523  loss : 0.31667742878647925\n",
      "iterations 7509 accuracy : 0.9294265910523  loss : 0.31667146952390784\n",
      "iterations 7510 accuracy : 0.9294265910523  loss : 0.31666554857813356\n",
      "iterations 7511 accuracy : 0.9294265910523  loss : 0.3166603371436628\n",
      "iterations 7512 accuracy : 0.9294265910523  loss : 0.31665426186858886\n",
      "iterations 7513 accuracy : 0.9294265910523  loss : 0.31664837431588255\n",
      "iterations 7514 accuracy : 0.9294265910523  loss : 0.3166424855649942\n",
      "iterations 7515 accuracy : 0.9294265910523  loss : 0.3166366568922936\n",
      "iterations 7516 accuracy : 0.9294265910523  loss : 0.31663043890038073\n",
      "iterations 7517 accuracy : 0.9294265910523  loss : 0.31662456421737206\n",
      "iterations 7518 accuracy : 0.9294265910523  loss : 0.3166189785323138\n",
      "iterations 7519 accuracy : 0.9294265910523  loss : 0.31661355750965736\n",
      "iterations 7520 accuracy : 0.9294265910523  loss : 0.3166080967914841\n",
      "iterations 7521 accuracy : 0.9294265910523  loss : 0.3166021256579006\n",
      "iterations 7522 accuracy : 0.9294265910523  loss : 0.3165958931251375\n",
      "iterations 7523 accuracy : 0.9294265910523  loss : 0.31659025516007855\n",
      "iterations 7524 accuracy : 0.9294265910523  loss : 0.31658433369690947\n",
      "iterations 7525 accuracy : 0.9294265910523  loss : 0.3165789799271937\n",
      "iterations 7526 accuracy : 0.9294265910523  loss : 0.3165733886921342\n",
      "iterations 7527 accuracy : 0.9294265910523  loss : 0.31656833206355534\n",
      "iterations 7528 accuracy : 0.9294265910523  loss : 0.3165631066841502\n",
      "iterations 7529 accuracy : 0.9294265910523  loss : 0.31655820010767804\n",
      "iterations 7530 accuracy : 0.9294265910523  loss : 0.31655244928193654\n",
      "iterations 7531 accuracy : 0.9294265910523  loss : 0.31654678749407267\n",
      "iterations 7532 accuracy : 0.9294265910523  loss : 0.31654078154710863\n",
      "iterations 7533 accuracy : 0.9294265910523  loss : 0.31653535369689834\n",
      "iterations 7534 accuracy : 0.9294265910523  loss : 0.3165295222946966\n",
      "iterations 7535 accuracy : 0.9294265910523  loss : 0.3165237475389223\n",
      "iterations 7536 accuracy : 0.9294265910523  loss : 0.31651855585866595\n",
      "iterations 7537 accuracy : 0.9294265910523  loss : 0.31651234519829036\n",
      "iterations 7538 accuracy : 0.9294265910523  loss : 0.3165064178005688\n",
      "iterations 7539 accuracy : 0.9294265910523  loss : 0.3165002148218464\n",
      "iterations 7540 accuracy : 0.9294265910523  loss : 0.3164946392430481\n",
      "iterations 7541 accuracy : 0.9294265910523  loss : 0.31648918046915636\n",
      "iterations 7542 accuracy : 0.9294265910523  loss : 0.31648357973567015\n",
      "iterations 7543 accuracy : 0.9294265910523  loss : 0.3164778292308088\n",
      "iterations 7544 accuracy : 0.9294265910523  loss : 0.31647206829444213\n",
      "iterations 7545 accuracy : 0.9294265910523  loss : 0.31646627357722557\n",
      "iterations 7546 accuracy : 0.9294265910523  loss : 0.31646124850418417\n",
      "iterations 7547 accuracy : 0.9294265910523  loss : 0.3164564023716502\n",
      "iterations 7548 accuracy : 0.9294265910523  loss : 0.3164500882920836\n",
      "iterations 7549 accuracy : 0.9294265910523  loss : 0.3164435529641412\n",
      "iterations 7550 accuracy : 0.9294265910523  loss : 0.31643810565282154\n",
      "iterations 7551 accuracy : 0.9294265910523  loss : 0.3164327392797651\n",
      "iterations 7552 accuracy : 0.9294265910523  loss : 0.3164271969718695\n",
      "iterations 7553 accuracy : 0.9294265910523  loss : 0.31642170537547887\n",
      "iterations 7554 accuracy : 0.9294265910523  loss : 0.31641622174385814\n",
      "iterations 7555 accuracy : 0.9294265910523  loss : 0.3164103954379659\n",
      "iterations 7556 accuracy : 0.9294265910523  loss : 0.31640496372251065\n",
      "iterations 7557 accuracy : 0.9294265910523  loss : 0.316399472071355\n",
      "iterations 7558 accuracy : 0.9294265910523  loss : 0.3163941486697709\n",
      "iterations 7559 accuracy : 0.9294265910523  loss : 0.3163887110352501\n",
      "iterations 7560 accuracy : 0.9294265910523  loss : 0.31638243938474336\n",
      "iterations 7561 accuracy : 0.9294265910523  loss : 0.31637662304069847\n",
      "iterations 7562 accuracy : 0.9294265910523  loss : 0.3163710038231718\n",
      "iterations 7563 accuracy : 0.9294265910523  loss : 0.3163654811949158\n",
      "iterations 7564 accuracy : 0.9294265910523  loss : 0.31635987929476583\n",
      "iterations 7565 accuracy : 0.9294265910523  loss : 0.3163542713809647\n",
      "iterations 7566 accuracy : 0.9294265910523  loss : 0.31634882675921766\n",
      "iterations 7567 accuracy : 0.9294265910523  loss : 0.3163425516181357\n",
      "iterations 7568 accuracy : 0.9294265910523  loss : 0.316336407579985\n",
      "iterations 7569 accuracy : 0.9294265910523  loss : 0.3163309147177172\n",
      "iterations 7570 accuracy : 0.9294265910523  loss : 0.3163249640313746\n",
      "iterations 7571 accuracy : 0.9294265910523  loss : 0.31631953499048865\n",
      "iterations 7572 accuracy : 0.9294265910523  loss : 0.3163142529624616\n",
      "iterations 7573 accuracy : 0.9294265910523  loss : 0.3163091509581712\n",
      "iterations 7574 accuracy : 0.9294265910523  loss : 0.3163035147114888\n",
      "iterations 7575 accuracy : 0.9294265910523  loss : 0.31629761186136246\n",
      "iterations 7576 accuracy : 0.9294265910523  loss : 0.3162921550243108\n",
      "iterations 7577 accuracy : 0.9294265910523  loss : 0.3162863530392311\n",
      "iterations 7578 accuracy : 0.9294265910523  loss : 0.31628109761339035\n",
      "iterations 7579 accuracy : 0.9294265910523  loss : 0.3162755254625882\n",
      "iterations 7580 accuracy : 0.9294265910523  loss : 0.31626975913797667\n",
      "iterations 7581 accuracy : 0.9294265910523  loss : 0.3162639665500533\n",
      "iterations 7582 accuracy : 0.9294265910523  loss : 0.31625789854444436\n",
      "iterations 7583 accuracy : 0.9294265910523  loss : 0.31625270707711156\n",
      "iterations 7584 accuracy : 0.9294265910523  loss : 0.3162473790115682\n",
      "iterations 7585 accuracy : 0.9294265910523  loss : 0.3162416559601812\n",
      "iterations 7586 accuracy : 0.9294265910523  loss : 0.3162361242624056\n",
      "iterations 7587 accuracy : 0.9294265910523  loss : 0.3162305049082144\n",
      "iterations 7588 accuracy : 0.9294265910523  loss : 0.3162247752597298\n",
      "iterations 7589 accuracy : 0.9294265910523  loss : 0.31621907642798186\n",
      "iterations 7590 accuracy : 0.9294265910523  loss : 0.31621319145973686\n",
      "iterations 7591 accuracy : 0.9294265910523  loss : 0.3162077621506327\n",
      "iterations 7592 accuracy : 0.9294265910523  loss : 0.3162019910178214\n",
      "iterations 7593 accuracy : 0.9294265910523  loss : 0.3161968228922776\n",
      "iterations 7594 accuracy : 0.9294265910523  loss : 0.3161913062764927\n",
      "iterations 7595 accuracy : 0.9294265910523  loss : 0.31618547114920387\n",
      "iterations 7596 accuracy : 0.9294265910523  loss : 0.3161796358400802\n",
      "iterations 7597 accuracy : 0.9294265910523  loss : 0.3161741858043827\n",
      "iterations 7598 accuracy : 0.9294265910523  loss : 0.31616845863765086\n",
      "iterations 7599 accuracy : 0.9294265910523  loss : 0.31616310794977875\n",
      "iterations 7600 accuracy : 0.9294265910523  loss : 0.3161579761448677\n",
      "iterations 7601 accuracy : 0.9294265910523  loss : 0.31615247555819154\n",
      "iterations 7602 accuracy : 0.9294265910523  loss : 0.3161471396196656\n",
      "iterations 7603 accuracy : 0.9294265910523  loss : 0.31614146457866954\n",
      "iterations 7604 accuracy : 0.9294265910523  loss : 0.31613615965479613\n",
      "iterations 7605 accuracy : 0.9294265910523  loss : 0.3161305046520944\n",
      "iterations 7606 accuracy : 0.9294265910523  loss : 0.3161246757549702\n",
      "iterations 7607 accuracy : 0.9294265910523  loss : 0.3161186819223175\n",
      "iterations 7608 accuracy : 0.9294265910523  loss : 0.31611307783888903\n",
      "iterations 7609 accuracy : 0.9294265910523  loss : 0.31610754262059504\n",
      "iterations 7610 accuracy : 0.9294265910523  loss : 0.31610181327479653\n",
      "iterations 7611 accuracy : 0.9294265910523  loss : 0.3160968600960776\n",
      "iterations 7612 accuracy : 0.9294265910523  loss : 0.31609113722581966\n",
      "iterations 7613 accuracy : 0.9294265910523  loss : 0.316085360735512\n",
      "iterations 7614 accuracy : 0.9294265910523  loss : 0.31607942556992513\n",
      "iterations 7615 accuracy : 0.9294265910523  loss : 0.31607422483365805\n",
      "iterations 7616 accuracy : 0.9294265910523  loss : 0.3160686571729985\n",
      "iterations 7617 accuracy : 0.9294265910523  loss : 0.3160631110494192\n",
      "iterations 7618 accuracy : 0.9294265910523  loss : 0.31605769603829936\n",
      "iterations 7619 accuracy : 0.9294265910523  loss : 0.3160528066225892\n",
      "iterations 7620 accuracy : 0.9294265910523  loss : 0.31604783156349914\n",
      "iterations 7621 accuracy : 0.9294265910523  loss : 0.3160424677326907\n",
      "iterations 7622 accuracy : 0.9294265910523  loss : 0.3160369441534651\n",
      "iterations 7623 accuracy : 0.9294265910523  loss : 0.3160318870932237\n",
      "iterations 7624 accuracy : 0.9294265910523  loss : 0.31602639095021196\n",
      "iterations 7625 accuracy : 0.9294265910523  loss : 0.31602084243041517\n",
      "iterations 7626 accuracy : 0.9294265910523  loss : 0.31601464696053105\n",
      "iterations 7627 accuracy : 0.9294265910523  loss : 0.31600906543987645\n",
      "iterations 7628 accuracy : 0.9294265910523  loss : 0.31600382423926987\n",
      "iterations 7629 accuracy : 0.9294265910523  loss : 0.31599825179293434\n",
      "iterations 7630 accuracy : 0.9294265910523  loss : 0.31599278470942205\n",
      "iterations 7631 accuracy : 0.9294265910523  loss : 0.3159871662584306\n",
      "iterations 7632 accuracy : 0.9294265910523  loss : 0.31598125525627635\n",
      "iterations 7633 accuracy : 0.9294265910523  loss : 0.3159756225243603\n",
      "iterations 7634 accuracy : 0.9294265910523  loss : 0.31597015355901537\n",
      "iterations 7635 accuracy : 0.9294265910523  loss : 0.31596459566866314\n",
      "iterations 7636 accuracy : 0.9294265910523  loss : 0.31595970685903746\n",
      "iterations 7637 accuracy : 0.9294265910523  loss : 0.3159541919186237\n",
      "iterations 7638 accuracy : 0.9294265910523  loss : 0.31594907925307075\n",
      "iterations 7639 accuracy : 0.9294265910523  loss : 0.31594368742297013\n",
      "iterations 7640 accuracy : 0.9294265910523  loss : 0.31593838122070583\n",
      "iterations 7641 accuracy : 0.9294265910523  loss : 0.31593293205887224\n",
      "iterations 7642 accuracy : 0.9294265910523  loss : 0.31592742988370703\n",
      "iterations 7643 accuracy : 0.9294265910523  loss : 0.31592161372417654\n",
      "iterations 7644 accuracy : 0.9294265910523  loss : 0.3159154976782876\n",
      "iterations 7645 accuracy : 0.9294265910523  loss : 0.31590986183840936\n",
      "iterations 7646 accuracy : 0.9294265910523  loss : 0.31590423601717443\n",
      "iterations 7647 accuracy : 0.9294265910523  loss : 0.3158987060404023\n",
      "iterations 7648 accuracy : 0.9294265910523  loss : 0.3158928376469855\n",
      "iterations 7649 accuracy : 0.9294265910523  loss : 0.31588746356689595\n",
      "iterations 7650 accuracy : 0.9294265910523  loss : 0.3158816086574069\n",
      "iterations 7651 accuracy : 0.9294265910523  loss : 0.3158763545227441\n",
      "iterations 7652 accuracy : 0.9294265910523  loss : 0.3158707376932822\n",
      "iterations 7653 accuracy : 0.9294265910523  loss : 0.3158653818398708\n",
      "iterations 7654 accuracy : 0.9294265910523  loss : 0.3158596251119087\n",
      "iterations 7655 accuracy : 0.9294265910523  loss : 0.3158545195108702\n",
      "iterations 7656 accuracy : 0.9294265910523  loss : 0.31584915500374383\n",
      "iterations 7657 accuracy : 0.9294265910523  loss : 0.3158439858086097\n",
      "iterations 7658 accuracy : 0.9294265910523  loss : 0.3158386109565647\n",
      "iterations 7659 accuracy : 0.9294265910523  loss : 0.3158331763910618\n",
      "iterations 7660 accuracy : 0.9294265910523  loss : 0.3158275157400981\n",
      "iterations 7661 accuracy : 0.9294265910523  loss : 0.31582220552953616\n",
      "iterations 7662 accuracy : 0.9294265910523  loss : 0.31581666663395896\n",
      "iterations 7663 accuracy : 0.9294265910523  loss : 0.3158110413680978\n",
      "iterations 7664 accuracy : 0.9294265910523  loss : 0.31580489041506965\n",
      "iterations 7665 accuracy : 0.9294265910523  loss : 0.31579922632256113\n",
      "iterations 7666 accuracy : 0.9294265910523  loss : 0.3157938199061519\n",
      "iterations 7667 accuracy : 0.9294265910523  loss : 0.31578839458538194\n",
      "iterations 7668 accuracy : 0.9294265910523  loss : 0.3157827094383596\n",
      "iterations 7669 accuracy : 0.9294265910523  loss : 0.31577725489541575\n",
      "iterations 7670 accuracy : 0.9294265910523  loss : 0.3157716669357845\n",
      "iterations 7671 accuracy : 0.9294265910523  loss : 0.3157664569886595\n",
      "iterations 7672 accuracy : 0.9294265910523  loss : 0.31576102769054304\n",
      "iterations 7673 accuracy : 0.9294265910523  loss : 0.3157552569372502\n",
      "iterations 7674 accuracy : 0.9294265910523  loss : 0.3157498066418275\n",
      "iterations 7675 accuracy : 0.9294265910523  loss : 0.31574413640227\n",
      "iterations 7676 accuracy : 0.9294265910523  loss : 0.3157386664913546\n",
      "iterations 7677 accuracy : 0.9294265910523  loss : 0.3157331765018913\n",
      "iterations 7678 accuracy : 0.9294265910523  loss : 0.3157273681505532\n",
      "iterations 7679 accuracy : 0.9294265910523  loss : 0.3157220558748509\n",
      "iterations 7680 accuracy : 0.9294265910523  loss : 0.31571696847958614\n",
      "iterations 7681 accuracy : 0.9294265910523  loss : 0.3157116740888142\n",
      "iterations 7682 accuracy : 0.9294265910523  loss : 0.31570608756936974\n",
      "iterations 7683 accuracy : 0.9294265910523  loss : 0.3157006773349116\n",
      "iterations 7684 accuracy : 0.9294265910523  loss : 0.3156949406079913\n",
      "iterations 7685 accuracy : 0.9294265910523  loss : 0.31568985842567626\n",
      "iterations 7686 accuracy : 0.9294265910523  loss : 0.31568431695084576\n",
      "iterations 7687 accuracy : 0.9294265910523  loss : 0.3156783907503707\n",
      "iterations 7688 accuracy : 0.9294265910523  loss : 0.3156725810730315\n",
      "iterations 7689 accuracy : 0.9294265910523  loss : 0.31566672768239173\n",
      "iterations 7690 accuracy : 0.9294265910523  loss : 0.3156610043984185\n",
      "iterations 7691 accuracy : 0.9294265910523  loss : 0.31565546235879827\n",
      "iterations 7692 accuracy : 0.9294265910523  loss : 0.31565013825123883\n",
      "iterations 7693 accuracy : 0.9294265910523  loss : 0.31564435845560357\n",
      "iterations 7694 accuracy : 0.9294265910523  loss : 0.31563882669942767\n",
      "iterations 7695 accuracy : 0.9294265910523  loss : 0.31563330126514777\n",
      "iterations 7696 accuracy : 0.9294265910523  loss : 0.3156272768303251\n",
      "iterations 7697 accuracy : 0.9294265910523  loss : 0.31562159612931184\n",
      "iterations 7698 accuracy : 0.9294265910523  loss : 0.31561573422352224\n",
      "iterations 7699 accuracy : 0.9294265910523  loss : 0.31561047081705434\n",
      "iterations 7700 accuracy : 0.9294265910523  loss : 0.3156053384857616\n",
      "iterations 7701 accuracy : 0.9294265910523  loss : 0.315599691460414\n",
      "iterations 7702 accuracy : 0.9294265910523  loss : 0.3155946165722656\n",
      "iterations 7703 accuracy : 0.9294265910523  loss : 0.3155891887033508\n",
      "iterations 7704 accuracy : 0.9294265910523  loss : 0.3155837258591384\n",
      "iterations 7705 accuracy : 0.9294265910523  loss : 0.3155780864194074\n",
      "iterations 7706 accuracy : 0.9294265910523  loss : 0.31557287581600785\n",
      "iterations 7707 accuracy : 0.9294265910523  loss : 0.3155675272026839\n",
      "iterations 7708 accuracy : 0.9294265910523  loss : 0.3155623361339394\n",
      "iterations 7709 accuracy : 0.9294265910523  loss : 0.3155568441869627\n",
      "iterations 7710 accuracy : 0.9294265910523  loss : 0.31555119840695717\n",
      "iterations 7711 accuracy : 0.9294265910523  loss : 0.31554575245585653\n",
      "iterations 7712 accuracy : 0.9294265910523  loss : 0.31554065257439445\n",
      "iterations 7713 accuracy : 0.9294265910523  loss : 0.31553532858202854\n",
      "iterations 7714 accuracy : 0.9294265910523  loss : 0.3155297013429912\n",
      "iterations 7715 accuracy : 0.9294265910523  loss : 0.31552413687100656\n",
      "iterations 7716 accuracy : 0.9294265910523  loss : 0.3155184585100421\n",
      "iterations 7717 accuracy : 0.9294265910523  loss : 0.31551314299795613\n",
      "iterations 7718 accuracy : 0.9294265910523  loss : 0.3155083542987296\n",
      "iterations 7719 accuracy : 0.9294265910523  loss : 0.3155026412500121\n",
      "iterations 7720 accuracy : 0.9294265910523  loss : 0.3154967088882712\n",
      "iterations 7721 accuracy : 0.9294265910523  loss : 0.3154915199511447\n",
      "iterations 7722 accuracy : 0.9294265910523  loss : 0.3154859309625505\n",
      "iterations 7723 accuracy : 0.9294265910523  loss : 0.3154802324613648\n",
      "iterations 7724 accuracy : 0.9294265910523  loss : 0.3154745263707722\n",
      "iterations 7725 accuracy : 0.9294265910523  loss : 0.3154689869785543\n",
      "iterations 7726 accuracy : 0.9294265910523  loss : 0.31546383321254257\n",
      "iterations 7727 accuracy : 0.9294265910523  loss : 0.3154580662991847\n",
      "iterations 7728 accuracy : 0.9294265910523  loss : 0.3154526618120506\n",
      "iterations 7729 accuracy : 0.9294265910523  loss : 0.3154468997831112\n",
      "iterations 7730 accuracy : 0.9294265910523  loss : 0.31544128964399837\n",
      "iterations 7731 accuracy : 0.9294265910523  loss : 0.31543616726499735\n",
      "iterations 7732 accuracy : 0.9294265910523  loss : 0.31543129952731735\n",
      "iterations 7733 accuracy : 0.9294265910523  loss : 0.31542591491560307\n",
      "iterations 7734 accuracy : 0.9294265910523  loss : 0.3154206160745115\n",
      "iterations 7735 accuracy : 0.9294265910523  loss : 0.31541487976609645\n",
      "iterations 7736 accuracy : 0.9294265910523  loss : 0.3154086472906058\n",
      "iterations 7737 accuracy : 0.9294265910523  loss : 0.3154036188356593\n",
      "iterations 7738 accuracy : 0.9294265910523  loss : 0.3153977791048441\n",
      "iterations 7739 accuracy : 0.9294265910523  loss : 0.3153926714769517\n",
      "iterations 7740 accuracy : 0.9294265910523  loss : 0.3153873545579636\n",
      "iterations 7741 accuracy : 0.9294265910523  loss : 0.31538198033547865\n",
      "iterations 7742 accuracy : 0.9294265910523  loss : 0.315376793416891\n",
      "iterations 7743 accuracy : 0.9294265910523  loss : 0.31537169900209566\n",
      "iterations 7744 accuracy : 0.9294265910523  loss : 0.3153660465907474\n",
      "iterations 7745 accuracy : 0.9294265910523  loss : 0.31536033927850543\n",
      "iterations 7746 accuracy : 0.9294265910523  loss : 0.31535436333378997\n",
      "iterations 7747 accuracy : 0.9294265910523  loss : 0.31534913952282795\n",
      "iterations 7748 accuracy : 0.9294265910523  loss : 0.3153435815260786\n",
      "iterations 7749 accuracy : 0.9294265910523  loss : 0.3153382942425395\n",
      "iterations 7750 accuracy : 0.9294265910523  loss : 0.31533230767458476\n",
      "iterations 7751 accuracy : 0.9294265910523  loss : 0.31532699335091463\n",
      "iterations 7752 accuracy : 0.9294265910523  loss : 0.31532212552065664\n",
      "iterations 7753 accuracy : 0.9294265910523  loss : 0.3153162924985072\n",
      "iterations 7754 accuracy : 0.9294265910523  loss : 0.3153113817099853\n",
      "iterations 7755 accuracy : 0.9294265910523  loss : 0.31530616948790324\n",
      "iterations 7756 accuracy : 0.9294265910523  loss : 0.3153015467880832\n",
      "iterations 7757 accuracy : 0.9294265910523  loss : 0.3152964979452275\n",
      "iterations 7758 accuracy : 0.9294265910523  loss : 0.31529076906837544\n",
      "iterations 7759 accuracy : 0.9294265910523  loss : 0.3152850888666239\n",
      "iterations 7760 accuracy : 0.9294265910523  loss : 0.3152797024476797\n",
      "iterations 7761 accuracy : 0.9294265910523  loss : 0.3152747471378073\n",
      "iterations 7762 accuracy : 0.9294265910523  loss : 0.31526971486088773\n",
      "iterations 7763 accuracy : 0.9294265910523  loss : 0.3152647934963826\n",
      "iterations 7764 accuracy : 0.9294265910523  loss : 0.3152591401251397\n",
      "iterations 7765 accuracy : 0.9294265910523  loss : 0.31525398197270166\n",
      "iterations 7766 accuracy : 0.9294265910523  loss : 0.3152485868328144\n",
      "iterations 7767 accuracy : 0.9294265910523  loss : 0.3152436720650818\n",
      "iterations 7768 accuracy : 0.9294265910523  loss : 0.3152381095116016\n",
      "iterations 7769 accuracy : 0.9294265910523  loss : 0.3152325908820056\n",
      "iterations 7770 accuracy : 0.9294265910523  loss : 0.3152270418866822\n",
      "iterations 7771 accuracy : 0.9294265910523  loss : 0.3152219384853588\n",
      "iterations 7772 accuracy : 0.9294265910523  loss : 0.31521663309025805\n",
      "iterations 7773 accuracy : 0.9294265910523  loss : 0.315211367384494\n",
      "iterations 7774 accuracy : 0.9294265910523  loss : 0.3152054758382316\n",
      "iterations 7775 accuracy : 0.9294265910523  loss : 0.3151997377302469\n",
      "iterations 7776 accuracy : 0.9294265910523  loss : 0.315194278033359\n",
      "iterations 7777 accuracy : 0.9294265910523  loss : 0.3151888687823497\n",
      "iterations 7778 accuracy : 0.9294265910523  loss : 0.315183341509362\n",
      "iterations 7779 accuracy : 0.9294265910523  loss : 0.3151776498242149\n",
      "iterations 7780 accuracy : 0.9296366309598824  loss : 0.31517177727092244\n",
      "iterations 7781 accuracy : 0.9296366309598824  loss : 0.3151665795327759\n",
      "iterations 7782 accuracy : 0.9296366309598824  loss : 0.3151608538500894\n",
      "iterations 7783 accuracy : 0.9294265910523  loss : 0.3151548833236908\n",
      "iterations 7784 accuracy : 0.9294265910523  loss : 0.31514968854405495\n",
      "iterations 7785 accuracy : 0.9294265910523  loss : 0.31514460068068256\n",
      "iterations 7786 accuracy : 0.9294265910523  loss : 0.31513908341602614\n",
      "iterations 7787 accuracy : 0.9294265910523  loss : 0.3151333525350131\n",
      "iterations 7788 accuracy : 0.9294265910523  loss : 0.31512791073791946\n",
      "iterations 7789 accuracy : 0.9294265910523  loss : 0.3151224096791891\n",
      "iterations 7790 accuracy : 0.9294265910523  loss : 0.3151174660655986\n",
      "iterations 7791 accuracy : 0.9294265910523  loss : 0.315112094698068\n",
      "iterations 7792 accuracy : 0.9294265910523  loss : 0.3151072873729233\n",
      "iterations 7793 accuracy : 0.9294265910523  loss : 0.31510163605501046\n",
      "iterations 7794 accuracy : 0.9294265910523  loss : 0.31509572170632577\n",
      "iterations 7795 accuracy : 0.9294265910523  loss : 0.31509045417665754\n",
      "iterations 7796 accuracy : 0.9294265910523  loss : 0.31508494086481537\n",
      "iterations 7797 accuracy : 0.9294265910523  loss : 0.3150798372907943\n",
      "iterations 7798 accuracy : 0.9294265910523  loss : 0.31507465318973027\n",
      "iterations 7799 accuracy : 0.9294265910523  loss : 0.31506950906449815\n",
      "iterations 7800 accuracy : 0.9294265910523  loss : 0.3150643113614506\n",
      "iterations 7801 accuracy : 0.9294265910523  loss : 0.31505863850509463\n",
      "iterations 7802 accuracy : 0.9294265910523  loss : 0.3150533143382932\n",
      "iterations 7803 accuracy : 0.9294265910523  loss : 0.31504747186949117\n",
      "iterations 7804 accuracy : 0.9294265910523  loss : 0.3150425789376053\n",
      "iterations 7805 accuracy : 0.9294265910523  loss : 0.31503731513350364\n",
      "iterations 7806 accuracy : 0.9294265910523  loss : 0.3150317327882873\n",
      "iterations 7807 accuracy : 0.9294265910523  loss : 0.315026197697538\n",
      "iterations 7808 accuracy : 0.9294265910523  loss : 0.31502102782479996\n",
      "iterations 7809 accuracy : 0.9296366309598824  loss : 0.3150154259428592\n",
      "iterations 7810 accuracy : 0.9296366309598824  loss : 0.31500964500347456\n",
      "iterations 7811 accuracy : 0.9296366309598824  loss : 0.31500454014607826\n",
      "iterations 7812 accuracy : 0.9296366309598824  loss : 0.31499881108439154\n",
      "iterations 7813 accuracy : 0.9296366309598824  loss : 0.3149937603436565\n",
      "iterations 7814 accuracy : 0.9296366309598824  loss : 0.3149882424933224\n",
      "iterations 7815 accuracy : 0.9296366309598824  loss : 0.31498293403998295\n",
      "iterations 7816 accuracy : 0.9294265910523  loss : 0.31497803595978546\n",
      "iterations 7817 accuracy : 0.9296366309598824  loss : 0.31497265671152025\n",
      "iterations 7818 accuracy : 0.9296366309598824  loss : 0.31496770255585727\n",
      "iterations 7819 accuracy : 0.9296366309598824  loss : 0.314962345223604\n",
      "iterations 7820 accuracy : 0.9296366309598824  loss : 0.3149572931394964\n",
      "iterations 7821 accuracy : 0.9296366309598824  loss : 0.31495167410485647\n",
      "iterations 7822 accuracy : 0.9294265910523  loss : 0.31494648931178043\n",
      "iterations 7823 accuracy : 0.9296366309598824  loss : 0.31494091522496215\n",
      "iterations 7824 accuracy : 0.9296366309598824  loss : 0.3149355233458251\n",
      "iterations 7825 accuracy : 0.9294265910523  loss : 0.3149305556804468\n",
      "iterations 7826 accuracy : 0.9296366309598824  loss : 0.3149251297259326\n",
      "iterations 7827 accuracy : 0.9294265910523  loss : 0.31491979799711856\n",
      "iterations 7828 accuracy : 0.9294265910523  loss : 0.3149147778078372\n",
      "iterations 7829 accuracy : 0.9294265910523  loss : 0.31490953299529956\n",
      "iterations 7830 accuracy : 0.9294265910523  loss : 0.31490435109609566\n",
      "iterations 7831 accuracy : 0.9294265910523  loss : 0.3148993448710841\n",
      "iterations 7832 accuracy : 0.9294265910523  loss : 0.3148941466046842\n",
      "iterations 7833 accuracy : 0.9294265910523  loss : 0.3148889925835988\n",
      "iterations 7834 accuracy : 0.9294265910523  loss : 0.31488361342914994\n",
      "iterations 7835 accuracy : 0.9294265910523  loss : 0.31487833063418436\n",
      "iterations 7836 accuracy : 0.9294265910523  loss : 0.3148733371219737\n",
      "iterations 7837 accuracy : 0.9294265910523  loss : 0.3148681195075177\n",
      "iterations 7838 accuracy : 0.9296366309598824  loss : 0.31486296945649084\n",
      "iterations 7839 accuracy : 0.9296366309598824  loss : 0.31485800317986706\n",
      "iterations 7840 accuracy : 0.9296366309598824  loss : 0.3148527447925993\n",
      "iterations 7841 accuracy : 0.9294265910523  loss : 0.3148472651977111\n",
      "iterations 7842 accuracy : 0.9296366309598824  loss : 0.31484203468772\n",
      "iterations 7843 accuracy : 0.9296366309598824  loss : 0.31483703704318267\n",
      "iterations 7844 accuracy : 0.9296366309598824  loss : 0.3148318265539581\n",
      "iterations 7845 accuracy : 0.9296366309598824  loss : 0.3148270040763972\n",
      "iterations 7846 accuracy : 0.9296366309598824  loss : 0.3148216837992135\n",
      "iterations 7847 accuracy : 0.9294265910523  loss : 0.3148157146287611\n",
      "iterations 7848 accuracy : 0.9294265910523  loss : 0.31481075753986976\n",
      "iterations 7849 accuracy : 0.9294265910523  loss : 0.31480505933475117\n",
      "iterations 7850 accuracy : 0.9294265910523  loss : 0.3147999974258197\n",
      "iterations 7851 accuracy : 0.9294265910523  loss : 0.314794602324959\n",
      "iterations 7852 accuracy : 0.9294265910523  loss : 0.31478898031112773\n",
      "iterations 7853 accuracy : 0.9294265910523  loss : 0.3147840398539146\n",
      "iterations 7854 accuracy : 0.9296366309598824  loss : 0.3147793121919444\n",
      "iterations 7855 accuracy : 0.9296366309598824  loss : 0.31477422138630495\n",
      "iterations 7856 accuracy : 0.9294265910523  loss : 0.3147689061800401\n",
      "iterations 7857 accuracy : 0.9294265910523  loss : 0.31476335545184675\n",
      "iterations 7858 accuracy : 0.9296366309598824  loss : 0.3147582594142398\n",
      "iterations 7859 accuracy : 0.9296366309598824  loss : 0.314753217067159\n",
      "iterations 7860 accuracy : 0.9296366309598824  loss : 0.3147480629755568\n",
      "iterations 7861 accuracy : 0.9296366309598824  loss : 0.3147430260232613\n",
      "iterations 7862 accuracy : 0.9296366309598824  loss : 0.3147371846398635\n",
      "iterations 7863 accuracy : 0.9296366309598824  loss : 0.31473186611454196\n",
      "iterations 7864 accuracy : 0.9294265910523  loss : 0.3147260327833917\n",
      "iterations 7865 accuracy : 0.9294265910523  loss : 0.31472079949411413\n",
      "iterations 7866 accuracy : 0.9294265910523  loss : 0.3147157837012887\n",
      "iterations 7867 accuracy : 0.9294265910523  loss : 0.31471111803201174\n",
      "iterations 7868 accuracy : 0.9294265910523  loss : 0.31470589492395856\n",
      "iterations 7869 accuracy : 0.9296366309598824  loss : 0.3147008340580844\n",
      "iterations 7870 accuracy : 0.9294265910523  loss : 0.3146953707320013\n",
      "iterations 7871 accuracy : 0.9296366309598824  loss : 0.314690747149639\n",
      "iterations 7872 accuracy : 0.9296366309598824  loss : 0.3146854281332617\n",
      "iterations 7873 accuracy : 0.9296366309598824  loss : 0.3146801674730438\n",
      "iterations 7874 accuracy : 0.9296366309598824  loss : 0.31467532508279406\n",
      "iterations 7875 accuracy : 0.9296366309598824  loss : 0.31467011347444573\n",
      "iterations 7876 accuracy : 0.9296366309598824  loss : 0.31466508475378535\n",
      "iterations 7877 accuracy : 0.9296366309598824  loss : 0.3146604510002948\n",
      "iterations 7878 accuracy : 0.9296366309598824  loss : 0.3146554761575389\n",
      "iterations 7879 accuracy : 0.9296366309598824  loss : 0.31465019343341033\n",
      "iterations 7880 accuracy : 0.9296366309598824  loss : 0.3146450781334808\n",
      "iterations 7881 accuracy : 0.9296366309598824  loss : 0.31464020305241774\n",
      "iterations 7882 accuracy : 0.9296366309598824  loss : 0.3146345702903688\n",
      "iterations 7883 accuracy : 0.9296366309598824  loss : 0.31462911255376325\n",
      "iterations 7884 accuracy : 0.9296366309598824  loss : 0.3146245066830342\n",
      "iterations 7885 accuracy : 0.9296366309598824  loss : 0.3146198444890313\n",
      "iterations 7886 accuracy : 0.9296366309598824  loss : 0.31461517788903764\n",
      "iterations 7887 accuracy : 0.9296366309598824  loss : 0.3146098632180492\n",
      "iterations 7888 accuracy : 0.9296366309598824  loss : 0.31460436657533214\n",
      "iterations 7889 accuracy : 0.9296366309598824  loss : 0.31459860215235985\n",
      "iterations 7890 accuracy : 0.9296366309598824  loss : 0.314593310234476\n",
      "iterations 7891 accuracy : 0.9296366309598824  loss : 0.31458918753722226\n",
      "iterations 7892 accuracy : 0.9296366309598824  loss : 0.3145842426633628\n",
      "iterations 7893 accuracy : 0.9296366309598824  loss : 0.314579035270058\n",
      "iterations 7894 accuracy : 0.9296366309598824  loss : 0.31457392375546306\n",
      "iterations 7895 accuracy : 0.9296366309598824  loss : 0.3145682872511547\n",
      "iterations 7896 accuracy : 0.9296366309598824  loss : 0.3145625913950683\n",
      "iterations 7897 accuracy : 0.9296366309598824  loss : 0.31455731083175037\n",
      "iterations 7898 accuracy : 0.9296366309598824  loss : 0.31455167905560255\n",
      "iterations 7899 accuracy : 0.9296366309598824  loss : 0.314547123554019\n",
      "iterations 7900 accuracy : 0.9296366309598824  loss : 0.31454197453171895\n",
      "iterations 7901 accuracy : 0.9296366309598824  loss : 0.31453614036958655\n",
      "iterations 7902 accuracy : 0.9296366309598824  loss : 0.3145307939811999\n",
      "iterations 7903 accuracy : 0.9296366309598824  loss : 0.3145258835602087\n",
      "iterations 7904 accuracy : 0.9296366309598824  loss : 0.3145206700071798\n",
      "iterations 7905 accuracy : 0.9296366309598824  loss : 0.314515662996449\n",
      "iterations 7906 accuracy : 0.9296366309598824  loss : 0.3145103800784138\n",
      "iterations 7907 accuracy : 0.9296366309598824  loss : 0.31450547218372105\n",
      "iterations 7908 accuracy : 0.9296366309598824  loss : 0.31450047211996873\n",
      "iterations 7909 accuracy : 0.9296366309598824  loss : 0.3144951137017845\n",
      "iterations 7910 accuracy : 0.9296366309598824  loss : 0.3144895337260789\n",
      "iterations 7911 accuracy : 0.9296366309598824  loss : 0.31448421697361884\n",
      "iterations 7912 accuracy : 0.9296366309598824  loss : 0.31447848532490663\n",
      "iterations 7913 accuracy : 0.9296366309598824  loss : 0.3144738188571683\n",
      "iterations 7914 accuracy : 0.9296366309598824  loss : 0.31446910711770387\n",
      "iterations 7915 accuracy : 0.9296366309598824  loss : 0.3144640999442735\n",
      "iterations 7916 accuracy : 0.9296366309598824  loss : 0.3144584512652625\n",
      "iterations 7917 accuracy : 0.9296366309598824  loss : 0.31445312125922725\n",
      "iterations 7918 accuracy : 0.9296366309598824  loss : 0.31444782440617147\n",
      "iterations 7919 accuracy : 0.9294265910523  loss : 0.31444248776380324\n",
      "iterations 7920 accuracy : 0.9294265910523  loss : 0.3144375386936705\n",
      "iterations 7921 accuracy : 0.9294265910523  loss : 0.3144321708283552\n",
      "iterations 7922 accuracy : 0.9294265910523  loss : 0.31442728852308477\n",
      "iterations 7923 accuracy : 0.9294265910523  loss : 0.31442238895853153\n",
      "iterations 7924 accuracy : 0.9296366309598824  loss : 0.31441777933628323\n",
      "iterations 7925 accuracy : 0.9296366309598824  loss : 0.3144126609693248\n",
      "iterations 7926 accuracy : 0.9296366309598824  loss : 0.3144068918116838\n",
      "iterations 7927 accuracy : 0.9296366309598824  loss : 0.3144021806312857\n",
      "iterations 7928 accuracy : 0.9296366309598824  loss : 0.31439681778650375\n",
      "iterations 7929 accuracy : 0.9294265910523  loss : 0.31439123319772655\n",
      "iterations 7930 accuracy : 0.9294265910523  loss : 0.31438584639392586\n",
      "iterations 7931 accuracy : 0.9294265910523  loss : 0.31438073693241003\n",
      "iterations 7932 accuracy : 0.9294265910523  loss : 0.31437557110029374\n",
      "iterations 7933 accuracy : 0.9296366309598824  loss : 0.3143709326522994\n",
      "iterations 7934 accuracy : 0.9296366309598824  loss : 0.3143660910614415\n",
      "iterations 7935 accuracy : 0.9296366309598824  loss : 0.3143612728477323\n",
      "iterations 7936 accuracy : 0.9294265910523  loss : 0.3143553455411547\n",
      "iterations 7937 accuracy : 0.9296366309598824  loss : 0.3143504600393273\n",
      "iterations 7938 accuracy : 0.9296366309598824  loss : 0.31434537387849226\n",
      "iterations 7939 accuracy : 0.9296366309598824  loss : 0.3143404238750254\n",
      "iterations 7940 accuracy : 0.9294265910523  loss : 0.31433467084944\n",
      "iterations 7941 accuracy : 0.9294265910523  loss : 0.31432942892952137\n",
      "iterations 7942 accuracy : 0.9294265910523  loss : 0.31432443985173586\n",
      "iterations 7943 accuracy : 0.9294265910523  loss : 0.31431930985789935\n",
      "iterations 7944 accuracy : 0.9294265910523  loss : 0.3143137301601233\n",
      "iterations 7945 accuracy : 0.9294265910523  loss : 0.31430854636027494\n",
      "iterations 7946 accuracy : 0.9294265910523  loss : 0.31430339493604914\n",
      "iterations 7947 accuracy : 0.9294265910523  loss : 0.31429789919333123\n",
      "iterations 7948 accuracy : 0.9294265910523  loss : 0.31429283840704714\n",
      "iterations 7949 accuracy : 0.9294265910523  loss : 0.314287350967167\n",
      "iterations 7950 accuracy : 0.9294265910523  loss : 0.3142822153496988\n",
      "iterations 7951 accuracy : 0.9294265910523  loss : 0.31427689171615725\n",
      "iterations 7952 accuracy : 0.9294265910523  loss : 0.31427221281579093\n",
      "iterations 7953 accuracy : 0.9294265910523  loss : 0.3142674253000469\n",
      "iterations 7954 accuracy : 0.9294265910523  loss : 0.3142623964427228\n",
      "iterations 7955 accuracy : 0.9294265910523  loss : 0.31425722882723833\n",
      "iterations 7956 accuracy : 0.9296366309598824  loss : 0.3142528020613828\n",
      "iterations 7957 accuracy : 0.9296366309598824  loss : 0.3142473187562346\n",
      "iterations 7958 accuracy : 0.9296366309598824  loss : 0.3142424272198748\n",
      "iterations 7959 accuracy : 0.9296366309598824  loss : 0.3142382479036246\n",
      "iterations 7960 accuracy : 0.9296366309598824  loss : 0.31423297405199\n",
      "iterations 7961 accuracy : 0.9296366309598824  loss : 0.3142271709525932\n",
      "iterations 7962 accuracy : 0.9296366309598824  loss : 0.31422210654189153\n",
      "iterations 7963 accuracy : 0.9296366309598824  loss : 0.3142172675570618\n",
      "iterations 7964 accuracy : 0.9296366309598824  loss : 0.3142125264952975\n",
      "iterations 7965 accuracy : 0.9296366309598824  loss : 0.31420754048666866\n",
      "iterations 7966 accuracy : 0.9296366309598824  loss : 0.31420261790102794\n",
      "iterations 7967 accuracy : 0.9296366309598824  loss : 0.314198034212949\n",
      "iterations 7968 accuracy : 0.9296366309598824  loss : 0.3141923647012859\n",
      "iterations 7969 accuracy : 0.9296366309598824  loss : 0.3141874412760625\n",
      "iterations 7970 accuracy : 0.9296366309598824  loss : 0.3141825371738873\n",
      "iterations 7971 accuracy : 0.9296366309598824  loss : 0.3141780114862417\n",
      "iterations 7972 accuracy : 0.9296366309598824  loss : 0.3141724476284854\n",
      "iterations 7973 accuracy : 0.9296366309598824  loss : 0.31416766667871704\n",
      "iterations 7974 accuracy : 0.9296366309598824  loss : 0.3141623557187979\n",
      "iterations 7975 accuracy : 0.9296366309598824  loss : 0.31415751258015745\n",
      "iterations 7976 accuracy : 0.9296366309598824  loss : 0.3141520982817522\n",
      "iterations 7977 accuracy : 0.9296366309598824  loss : 0.31414777232251156\n",
      "iterations 7978 accuracy : 0.9296366309598824  loss : 0.3141426632629353\n",
      "iterations 7979 accuracy : 0.9296366309598824  loss : 0.31413814995521816\n",
      "iterations 7980 accuracy : 0.9296366309598824  loss : 0.31413333046606584\n",
      "iterations 7981 accuracy : 0.9296366309598824  loss : 0.3141287202795357\n",
      "iterations 7982 accuracy : 0.9296366309598824  loss : 0.3141235755497321\n",
      "iterations 7983 accuracy : 0.9296366309598824  loss : 0.3141184504865524\n",
      "iterations 7984 accuracy : 0.9296366309598824  loss : 0.31411366691065995\n",
      "iterations 7985 accuracy : 0.9296366309598824  loss : 0.31410868723540325\n",
      "iterations 7986 accuracy : 0.9296366309598824  loss : 0.31410365178034044\n",
      "iterations 7987 accuracy : 0.9296366309598824  loss : 0.314098690694203\n",
      "iterations 7988 accuracy : 0.9296366309598824  loss : 0.31409329699899013\n",
      "iterations 7989 accuracy : 0.9296366309598824  loss : 0.3140876052902892\n",
      "iterations 7990 accuracy : 0.9296366309598824  loss : 0.31408183194858186\n",
      "iterations 7991 accuracy : 0.9296366309598824  loss : 0.31407692683420635\n",
      "iterations 7992 accuracy : 0.9296366309598824  loss : 0.31407111214725963\n",
      "iterations 7993 accuracy : 0.9296366309598824  loss : 0.31406646745537464\n",
      "iterations 7994 accuracy : 0.9296366309598824  loss : 0.3140615704383716\n",
      "iterations 7995 accuracy : 0.9296366309598824  loss : 0.3140563873447125\n",
      "iterations 7996 accuracy : 0.9296366309598824  loss : 0.314051432425146\n",
      "iterations 7997 accuracy : 0.9296366309598824  loss : 0.31404706828935425\n",
      "iterations 7998 accuracy : 0.9296366309598824  loss : 0.31404181377735024\n",
      "iterations 7999 accuracy : 0.9296366309598824  loss : 0.3140365927530261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression at 0x1da4c4a2820>"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(iteration=8000, learning_rete=0.01, c=0.01, penalty='l2')\n",
    "clf.fit(x_train, y_train, batch_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9296366309598824, 0.9252728799328296)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_train, y_train), clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(clf.loss)), clf.loss)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(x_test)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtf",
   "language": "python",
   "name": "envtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
